[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "",
    "text": "If you were at all paying attention in pre-college science classes, you have probably seen this equation:\nd = rt or distance = rate*time\nIn English, that says we can know how far something has traveled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance.\nIf you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.\nd/t = r or distance/time = rate\nIn 2012, the South Florida Sun Sentinel found a story in this formula.\nPeople were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.\nBut do police regularly speed on tollways or were there just a few random and fatal exceptions?\nThanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.\nGiven that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time.\nIt took high school algebra to find how fast police officers were driving. And the results were shocking.\nTwenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.\nThe story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.\nAll with simple high school algebra.\n\n\nIt’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform.\n\n\n\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do.\n\n\n\nThis book is the collection of class materials originally written for Matt Waite’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism, with contributions from Sarah Cohen of Arizona State University.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis and Sean Mussenden 2022.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!\n\n\n\n\n\nGoogle Sheets\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nWriting with and about data\nData journalism ethics"
  },
  {
    "objectID": "start-story.html",
    "href": "start-story.html",
    "title": "2  Learn a new way to read",
    "section": "",
    "text": "Getting started in data journalism often feels as if you’ve left the newsroom and entered the land of statistics, computer programming and data science. This chapter will help you start seeing data reporting in a new way, by learning how to study great works of the craft as a writer rather than a reader.\nJelani Cobb tweeted, “an engineer doesn’t look at a bridge the same way pedestrians or drivers do.” They see it as a “language of angles and load bearing structures.” We just see a bridge. While he was referring to long-form writing, reporting with data can also be learned by example – if you spend enough time with the examples.\nAlmost all good writers and reporters try to learn from exemplary work. I know more than one reporter who studies prize-winning journalism to hone their craft. This site will have plenty of examples, but you should stay on the lookout for others."
  },
  {
    "objectID": "start-story.html#read-like-a-reporter",
    "href": "start-story.html#read-like-a-reporter",
    "title": "2  Learn a new way to read",
    "section": "2.1 Read like a reporter",
    "text": "2.1 Read like a reporter\nTry to approach data or empirical reporting as a reporter first, and a consumer second. The goal is to triangulate how the story was discovered, reported and constructed. You’ll want to think about why this story, told this way, at this time, was considered newsworthy enough to publish when another approach on the same topic might not have been.\n\nWhat were the questions?\nIn data journalism, we often start with a tip, or a hypothesis. Sometimes it’s a simple question. Walt Bogdanich of The New York Times is renowned for seeing stories around every corner. Bogdanich has said that the prize-winning story “A Disability Epidemic Among a Railroad’s Retirees” came from a simple question he had when railway workers went on strike over pension benefits – how much were they worth? The story led to an FBI investigation and arrests, along with pension reform at the largest commuter rail in the country.\nThe hypothesis for some stories might be more directed. In 2021, the Howard Center for Investigative Journalism at ASU published “Little victims everywhere”, a set of stories on the lack of justice for survivors of child sexual assault on Native American reservations. That story came after previous reporters for the center analyzed data from the Justice Department showing that the FBI dropped most of the cases it investigated, and the Justice Department then only prosecuted about half of the matters referred to it by investigators. The hypothesis was that they were rarely pursued because federal prosecutors – usually focused on immigration, white collar crime and drugs – weren’t as prepared to pursue violent crime in Indian Country.\nWhen studying a data-driven investigation, try to imagine what the reporters were trying to prove or disprove, and what they used to do it. In journalism, we rely on a mixture of quantitative and qualitative methods. It’s not enough to prove the “numbers” or have the statistical evidence. That is just the beginning of the story. We are supposed to ground-truth them with the stories of actual people and places.\n\n\nGo beyond the numbers\nIt’s easy to focus on the numbers or statistics that make up the key findings, or the reason for the story. Some reporters make the mistake of thinking all of the numbers came from the same place – a rarity in most long-form investigations. Instead, the sources have been woven together and are a mix of original research and research done by others. Try to pay attention to any sourcing done in the piece. Sometimes, it will tell you that the analysis was original. Other times it’s more subtle.\nBut don’t just look at the statistics being reported in the story. In many (most?) investigations, some of the key people, places or time elements come directly from a database.\nWhen I was analyzing Paycheck Protection Program loan data for ProPublica, one fact hit me as I was looking at a handful of sketchy-looking records: a lot of them were from a single county in coastal New Jersey. It turned out to be a pretty good story.\nOften, the place that a reporter visits is determined by examples found in data. In this story on rural development funds, all of the examples came from an analysis of the database. Once the data gave us a good lead, the reporters examined press releases and other easy-to-get sources before calling and visiting the recipients or towns."
  },
  {
    "objectID": "start-story.html#reading-tips",
    "href": "start-story.html#reading-tips",
    "title": "2  Learn a new way to read",
    "section": "2.2 Reading tips",
    "text": "2.2 Reading tips\nYou’ll get better at reading investigations and data-driven work over time, but for now, remember to go beyond the obvious:\n\nWhere might the reporters have found their key examples, and what made them good characters or illustrations of the larger issue? Could they have come from the data?\nWhat do you think came first – a narrative single example that was broadened by data (naively, qualitative method), or a big idea that was illustrated with characters (quantitative method)?\nWhat records were used? Were they public records, leaks, or proprietary data?\nWhat methods did they use? Did they do their own testing, use statistical analysis, or geographic methods? You won’t always know, but look for a methodology section or a description alongside each story.\nHow might you localize or adapt these methods to find your own stories?\nPick out the key findings (usually in the nut graf or in a series of bullets after the opening chapter): are they controversial? How might they have been derived? What might have been the investigative hypothesis? Have they given critics their due and tried to falsify their own work?\nHow effective is the writing and presentation of the story? What makes it compelling journalism rather than a dry study? How might you have done it differently? Is a video story better told in text, or would a text story have made a good documentary? Are the visual elements well integrated? Does the writing draw you in and keep you reading? Think about structure, story length, entry points and graphics all working together.\nAre you convinced? Are there holes or questions that didn’t get addressed?"
  },
  {
    "objectID": "start-story.html#analyze-data-for-story-not-study",
    "href": "start-story.html#analyze-data-for-story-not-study",
    "title": "2  Learn a new way to read",
    "section": "2.3 Analyze data for story, not study",
    "text": "2.3 Analyze data for story, not study\nAs journalists we’ll often be using data, social science methods and even interviewing differently than true experts. We’re seeking stories, not studies. Recognizing news in data is one of the hardest skills for less experienced reporters new to data journalism. This list of potential newsworthy data points is adapted from Paul Bradshaw’s “Data Journalism Heist”.\n\n\n\n\nCompare the claims of powerful people and institutions against facts – the classic investigative approach.\nReport on unexpected highs and lows (of change, or of some other characteristic)\nLook for outliers – individual values that buck a trend seen in the rest\nVerify or bust some myths\nFind signs of distress, happiness or dishonesty or any other emotion.\nUncover new or under-reported long-term trends.\nFind data suggesting your area is the same or different than most others of its kind.\n\nBradshaw also did a recent study of data journalism pieces: “Here are the angles journalists use most often to tell the stories in data”, in Online Journalism Blog. I’m not sure I agree, only because he’s looking mainly at visualizations rather than stories, but they’re worth considering."
  },
  {
    "objectID": "start-story.html#exercises",
    "href": "start-story.html#exercises",
    "title": "2  Learn a new way to read",
    "section": "2.4 Exercises",
    "text": "2.4 Exercises\n\nIf you’re a member of Investigative Reporters and Editors, go to the site and find a recent prize-winning entry (usually text rather than broadcast). Get a copy of the IRE contest entry from the Resources page. Try to match up what the reporters said they did and how they did it with key portions of the story.\nThe next time you find a good data source, try to find a story that references it. If your data is local, you might look for a story that used similar data elsewhere, such as 911 response times or overdose deaths. But many stories use federal datasets that can easily be localized. Look at a description of the dataset and then the story to see how the data might have been used."
  },
  {
    "objectID": "start-math.html",
    "href": "start-math.html",
    "title": "3  Newsroom math",
    "section": "",
    "text": "Statistics are people with the tears washed off\n- Paul Brodeur\nJo Craven McGinty, then of The New York Times, used simple rates and ratios to discover that a 6-story brick New Jersey hospital was the most expensive in the nation. In 2012, Bayonne Medical Center “charged the highest amounts in the country for nearly one-quarter of the most common hospital treatments,” the Times story said.\nTo do this story, McGinty only needed to know the number of the procedures reported to the government and the total amount each hospital charged. Dividing those to find an average price, then ranking the most common procedures, led to this surprising result."
  },
  {
    "objectID": "start-math.html#why-numbers",
    "href": "start-math.html#why-numbers",
    "title": "3  Newsroom math",
    "section": "3.1 Why numbers?",
    "text": "3.1 Why numbers?\nUsing averages, percentages and percent change is the bread and butter of data journalism, leading to stories ranging from home price comparisons to school reports and crime trends. It may have been charming at one time for reporters to announce that they didn’t “do” math, but no longer. Instead, it is now an announcement that the reporter can only do some of the job. You will never be able to tackle complicated, in-depth stories without reviewing basic math.\nThe good news is that most of the math and statistics you need in a newsroom isn’t nearly as difficult as high school algebra. You learned it somewhere around the 4th grade. You then had a decade to forget it before deciding you didn’t like math. But mastering this most basic arithmetic again is a requirement in the modern age.\nIn working with typical newsroom math, you will need to learn how to:\n\nOvercome your fear of numbers\nIntegrate numbers into your reporting\nRoutinely compute averages, differences and rates\nSimplify and select the right numbers for your story\n\nWhile this chapter covers general tips, you can find specific instructions for typical newsroom math in this Appendix A"
  },
  {
    "objectID": "start-math.html#overcoming-your-fear-of-math",
    "href": "start-math.html#overcoming-your-fear-of-math",
    "title": "3  Newsroom math",
    "section": "3.2 Overcoming your fear of math",
    "text": "3.2 Overcoming your fear of math\nWhen we learned to read, we got used to the idea that 26 letters in American English could be assembled into units that we understand without thinking – words, sentences, paragraphs and books. We never got the same comfort level with 10 digits, and neither did our audience.\nThink of your own reaction to seeing a page of words. Now imagine it as a page of numbers.\nInstead, picture the number “five”. It’s easy. It might be fingers or it might be a team on a basketball court. But it’s simple to understand.\nNow picture the number 275 million. It’s hard. Unfortunately, 275 billion isn’t much harder, even though it’s magnitudes larger. (A million seconds goes by in about 11 days but you may not have been alive for a billion seconds – about 36 years.)\nThe easiest way to get used to some numbers is to learn ways to cut them down to size by calculating rates, ratios or percentages. In your analysis, keep an eye out for the simplest accurate way to characterize the numbers you want to use. “Characterize” is the important word here – it’s not usually necessary to be overly precise so long as your story doesn’t hinge on a nuanced reading of small differences. (And is anything that depends on that news? It may not be.)\nHere’s one example of putting huge numbers in perspective. Pay attention to what you really can picture - it’s probably the $21 equivalent.\n\nThe Chicago hedge fund billionaire Kenneth C. Griffin, for example, earns about $68.5 million a month after taxes, according to court filings made by his wife in their divorce. He has given a total of $300,000 to groups backing Republican presidential candidates. That is a huge sum on its face, yet is the equivalent of only $21.17 for a typical American household, according to Congressional Budget Office data on after-tax income.  “Buying Power”, Nicholas Confessore, Sarah Cohen and Karen Yourish, The New York Times, October 2015\n\nOriginally the reporters had written it even more simply, but editors found the facts so unbelievable that they wanted give readers a chance to do the math themselves. That’s reasonable, but here’s an even simpler way to say it: “earned nearly $1 billion after taxes…He has given $300,000 to groups backing candidates, the equivalent of a dinner at Olive Garden for the typical American family , based on Congressional Budget Office income data.” (And yes, the reporter checked the price for an Olive Garden meal at the time for four people.)"
  },
  {
    "objectID": "start-math.html#put-math-in-its-place",
    "href": "start-math.html#put-math-in-its-place",
    "title": "3  Newsroom math",
    "section": "3.3 Put math in its place",
    "text": "3.3 Put math in its place\nFor journalists, numbers – or facts – make up the third leg of a stool supported by human stories or anecdotes, and insightful comment from experts. They serve us in three ways:\n\nAs summaries. Almost by definition, a number counts something, averages something, or otherwise summarizes something. Sometimes, it does a good job, as in the average height of Americans. Sometimes it does a terrible job, as in the average income of Americans. Try to find summaries that accurately characterize the real world.\nAs opinions. Sometimes it’s an opinion derived after years of impartial study. Sometimes it’s an opinion tinged with partisan or selective choices of facts. Use them accordingly.\nAs guesses. Sometimes it’s a good guess, sometimes it’s an off-the-cuff guess. And sometimes it’s a hopeful guess. Even when everything is presumably counted many times, it’s still a (very nearly accurate) guess. Yes, the “audits” of presidential election results in several states in 2021 found a handful of errors – not a meaningful number, but a few just the same.\n\nOnce you find the humanity in your numbers, by cutting them down to size and relegating them to their proper role, you’ll find yourself less fearful. You’ll be able to characterize what you’ve learned rather than numb your readers with every number in your notebook. You may even find that finding facts on your own is fun."
  },
  {
    "objectID": "start-math.html#going-further",
    "href": "start-math.html#going-further",
    "title": "3  Newsroom math",
    "section": "3.4 Going further",
    "text": "3.4 Going further\n\nTipsheets\n\nSteve Doig’s “Math Crib Sheet”\nAppendix A: Common newsroom math, adapted from drafts of the book Numbers in the Newsroom, by Sarah Cohen.\n\n\n\nReading and viewing\n\n“Avoiding Numeric Novocain: Writing Well with Numbers,” by Chip Scanlan, Poynter.com\nT. Christian Miller’s “Writing the data-driven story”\nA viral Twitter thread:\n\nWhat happens in your head when you do 27+48?--- Gene Belcher (@Wparks91) June 25, 2019"
  },
  {
    "objectID": "start-math.html#exercises",
    "href": "start-math.html#exercises",
    "title": "3  Newsroom math",
    "section": "3.5 Exercises",
    "text": "3.5 Exercises\n\nImagine that someone gave you $1 million and you could spend it on anything you want. Write down a list of things that would add up to about that amount. That should be easy. Now, imagine someone gave you $1 billion and you could spend it on whatever you want, but anything left over after a year had to be returned. How would you spend it? (You can give away money, but it can’t be more than 50% of a charity’s annual revenues. So you can’t give 10 $100 million gifts!) See how far you get trying to spend it. A few homes, a few yachts, student loan repayments for all of your friends? You’ve hardly gotten started."
  },
  {
    "objectID": "start-data-def.html",
    "href": "start-data-def.html",
    "title": "4  Defining “Data”",
    "section": "",
    "text": "data /ˈdeɪ.tə/ :\ninformation in an electronic form that can be stored and used by a computer, or information, especially facts or numbers, collected to be examined and >considered and used to help decision-making\n– Cambridge Dictionary – sort of 1"
  },
  {
    "objectID": "start-data-def.html#the-birth-of-a-dataset",
    "href": "start-data-def.html#the-birth-of-a-dataset",
    "title": "4  Defining “Data”",
    "section": "4.1 The birth of a dataset",
    "text": "4.1 The birth of a dataset\nMost journalism uses data collected for one purpose for something entirely different. Understanding its original uses – what matters to the people who collected it, and what doesn’t – will profoundly affect its accuracy or usefulness.\n\nTrace data and administrative records\nIn “The Art of Access”, David Cullier and Charles N. Davis describe a process of tracking down the life and times of a dataset. Their purpose is to make sure they know how to request it from a government agency. The same idea applies to using data that we acquire elsewhere.\nUnderstanding how and why data exists is crucial to understanding what you, as a reporter, might do with it.\nAnything you can systematically search or analyze could be considered one piece of of data. As reporters, we usually deal with data that was created in the process of doing something else – conducting an inspection, delivering a tweet, or scoring a musical. In the sciences, this flotsam and jetsom that is left behind is called “digital trace data” if it was born digitally.\nIn journalism and in the social sciences, many of our data sources were born during some government process – a safety inspection, a traffic ticket, or the filing of a death certificate. These administrative records form the basis of much investigative reporting and they are often the subject of public records and FOIA requests. They were born as part of the government doing its job, without any thought given to how it might be used in another way. In the sciences, those are often called “administrative records”.\nThis trace data might be considered the first part of the definition above – information that can be stored and used.\nHere’s how Chris Bail from Duke University describes it.\n\n\nData collected and curated for analysis\nAnother kind of data is that which is compiled or collected specifically for the purpose of studying something. It might collected in the form of a survey or a poll, or it might be a system of sampling to measure pollution or weather. But it’s there because the information has intrinsic value AS information.\nThe video suggests a hard line between trace data and custom data. In practice, it’s not that clear. Many newsrooms may curate data published in other sources or in administrative records, such as the Washington Post’s police shooting dataset. In other cases, the agencies we are covering get already-compiled data from state and local governments.\nThis type of data might be considered the second type in the definition – tabular information that is used for decision-making."
  },
  {
    "objectID": "start-data-def.html#granular-and-aggregated-data",
    "href": "start-data-def.html#granular-and-aggregated-data",
    "title": "4  Defining “Data”",
    "section": "4.2 Granular and aggregated data",
    "text": "4.2 Granular and aggregated data\nOne of the hardest concepts for a lot of new data journalists is the idea of granularity of your data source. There are a lot of ways to think about this: individual items in a list vs. figures in a table; original records vs. compilations; granular data vs. statistics.\nGenerally, an investigative reporter is interested in getting data that is as close as possible to the most granular information that exists, at least on computer files. Here’s an example, which might give you a little intuition about why it’s so important to think this way:\nWhen someone dies in the US, a standard death certificate is filled out by a series of officials - the attending physician, the institution where they died and even the funeral direcor.\nClick on this link to see a blank version of the standard US death certificate form – notice the detail and the detailed instructions on how it is supposed to be filled out. 2\nA good reporter could imagine many stories coming out of these little boxes. Limiting yourself to just to COVID-19-related stories: You could profile the local doctor who signed the most COVID-19-related death certificates in their city, or examine the number of deaths that had COVID as a contributing, but not underlying or immediate, cause of death. You could compare smoking rates in the city with the number of decedents whose tobacco use likely contributed to their death. Maybe you’d want to know how long patients suffered with the disease before they died. And you could map the deaths to find the block in your town most devastated by the virus.\nEarly in the pandemic, Coulter Jones and Jon Kamp examined the records from one of the few states that makes them public, and concluded that “Coronavirus Deaths were Likely Missed in Michigan, Death Certificates Suggest”\nBut you probably can’t do that. The reason is that, in most states, death certificates are not public records and are treated as secrets. 3. Instead, state and local governments provide limited statistics related to the deaths, usually by county, with no detail. That’s the difference between granular data and aggregate data. Here are some of the typical (not universal) characteristics of each:\n\n\n\n\n\n\n\nGranular\nAggregate\n\n\n\n\nIntended for some purpose other than your work\nIntended to be presented as is to the public\n\n\nMany rows (records), few columns (variables)\nMany columns (variables), few rows (records)\n\n\nRequires a good understanding of the source\nExplanatory notes usually come with the data\n\n\nEasy to cross-reference and compile\nOften impossible to repurpose\n\n\nHas few numeric columns\nMay be almost entirely numerical\n\n\nIs intended for use in a database\nIs intended for use in a spreadsheet\n\n\n\nWe often have to consider the trade-offs. Granular data with the detail we need - especially when it involves personally identifiable information like names and addresses - can take months or years of negotiation over public records requests, even when the law allows it. It’s often much easier to convince an agency to provide summarized or incomplete data. Don’t balk at using it if it works for you. But understand that in the vast majority of cases, it’s been summarized in a way that’s lost information that could be important to your story."
  },
  {
    "objectID": "start-data-def.html#nouns",
    "href": "start-data-def.html#nouns",
    "title": "4  Defining “Data”",
    "section": "4.3 Nouns",
    "text": "4.3 Nouns\nThat brings us to one of the most important things you must find out about any data you begin to analyze: What “noun” does each row in a tabular dataset represent? In statistics, they might be called observations or cases. In data science, they’re usually called records. Either way, every row must represent the same thing – a person, a place, a year, a water sample or a school. And you can’t really do anything with it until you figure out what that is.\nIn 2015, Sarah Cohen did a story at The New York Times called “More Deportation Follow Minor Crimes, Records Show”. The government had claimed it was only removing hardened criminals from the country, but our analysis of the data suggested that many of them were for minor infractions.\nIn writing the piece, they had to work around a problem in our data: the agency refused to provide them anything that would help us distinguish individuals from one another. All the reporters knew was that each row represented one deportation – not one person! Without a column, or field or a variable or an attribute for an individual – say, name and date of birth, or some scrambled version of an their DHS number – they had no way to even estimate how often people were deported multiple times. If you read the story, you’ll see the very careful wording, except when they had reported out and spoken to people on the ground."
  },
  {
    "objectID": "start-data-def.html#further-reading",
    "href": "start-data-def.html#further-reading",
    "title": "4  Defining “Data”",
    "section": "4.4 Further reading",
    "text": "4.4 Further reading\n\n“Basic steps in working with data”, the Data Journalism Handbook, Steve Doig, ASU Professor. He describes in this piece the problem of not knowing exactly how the data was compiled.\n“Counting the Infected” , Rob Gebellof on The Daily, July 8, 2020.\n“Spreadsheet thinking vs. Database thinking”, by Robert Kosara, gets at the idea that looking at individual items is often a “database”, and statistical compilations are often “spreadsheets”.\n“Tidy Data”, in the Journal of Statistical Software (linked here in a pre-print) by Hadley Wickham , is the quintessential article on describing what we think of as “clean” data. For our purposes, much of what he describes as “tidy” comes when we have individual, granular records – not statistical compilations. It’s an academic article, but it has the underlying concepts that we’ll be working with all year."
  },
  {
    "objectID": "start-data-def.html#exercises",
    "href": "start-data-def.html#exercises",
    "title": "4  Defining “Data”",
    "section": "4.5 Exercises",
    "text": "4.5 Exercises\n\nThe next time you get a government statistical report, scour all of the footnotes to find some explanation of where the data came from. You’ll be surprised how often they are compilations of administrative records - the government version of trace data."
  },
  {
    "objectID": "xl-intro.html",
    "href": "xl-intro.html",
    "title": "5  Introduction",
    "section": "",
    "text": "Some people consider using spreadsheets the table stakes for getting into data journalism. It’s relatively easy to see what you’re doing and you can easily share your work with your colleagues. In fact, pieces of the Pulitzer-Prize winning COVID-19 coverage from The New York Times was compiled using an elaborate and highly tuned set of Google spreadsheets with dozens of contributors.\nThis guide uses Google Sheets, although you should be able to do these exercises with Excel on the Mac or Windows. Excel on the Mac is pretty good, bu Excel in Windows is very different – it has much more capability for working with large and more complex data, and provides better tuning for import and other operations. There is a table that compares keystrokes for Apple desktops, laptops and Windows machines for Excel at the bottom of An Excel Refresher"
  },
  {
    "objectID": "xl-intro.html#tutorials",
    "href": "xl-intro.html#tutorials",
    "title": "5  Introduction",
    "section": "5.1 Tutorials",
    "text": "5.1 Tutorials\nSpreadsheets in the form of Google Sheets or Excel are used in almost every workplace in America. This section covers most of what you need in the newsroom, which is a different set of skills than in other businesses.\n\nAn Excel Refresher : Start over with good habits\nSorting and filtering to find stories : The first step of interviewing data\nGrouping with pivot tables: Aggregating, and the super power of spreadsheets\nFormulas in Excel: Percents, sums, and other basic computations used in newsrooms."
  },
  {
    "objectID": "xl-intro.html#practice-exercises",
    "href": "xl-intro.html#practice-exercises",
    "title": "5  Introduction",
    "section": "5.2 Practice exercises",
    "text": "5.2 Practice exercises\n\nPractice with “notice of claims” from Phoenix: Filtering and pivot table practice using claims made against the city of Phoenix 2010-2020."
  },
  {
    "objectID": "xl-refresher.html",
    "href": "xl-refresher.html",
    "title": "6  An Excel Refresher",
    "section": "",
    "text": "Spreadsheets are everywhere, so it’s worth re-learning how to use them well. Reporters usually use spreadsheets in three ways:\n(This guide is done using a Mac. Windows machines will be a little different, mainly because you’ll have more choices in most menus. The Mac CMD key is the same as the Windows CNTL key.)\nSome reporters flinch at typing in 30 or 100 entries into a spreadsheet. You shouldn’t. If you learn to take notes in a structured way, you’ll always be able to find and verify your work. If you try to calculate a sum of 30 numbers on a calculator, you’ll have to type them all in at least twice anyway. Also, getting used to these easy tasks on a spreadsheet keeps your muscles trained for when you need to do more."
  },
  {
    "objectID": "xl-refresher.html#re-learning-sheets-from-the-ground-up",
    "href": "xl-refresher.html#re-learning-sheets-from-the-ground-up",
    "title": "6  An Excel Refresher",
    "section": "6.1 Re-learning Sheets from the ground up",
    "text": "6.1 Re-learning Sheets from the ground up\n\n6.1.1 The spreadsheet grid\n\n\n\nWhen you start up a spreadsheet (try entering sheet.new into your browser!), you’ll see letters across the top and numbers down the side. If you ever played Battleship, you’ll recognize the idea – every little square, or cell, is referenced by the intersection of its column letter and row number:\nB2 is the cell that is currently active. You can tell because it’s outlined in the sheet and it’s shown on the upper left corner.\n\n\n6.1.2 Mouse shapes\n\n\n\n\n\n\n\nThe Copy Tool, or the thin black cross. When you see this, you’ll copy anything that’s selected. This can be good or bad.\n\n\n\nThe Evil Hand. (In Windows, this is the Evil Arrow). If you use this symbol, you will MOVE the selection to a new location. This is very rarely a good idea or something you intend.\n\n\n\n\n\n\n6.1.3 Selecting cells and ranges\nSpreadsheets act only on the cells or regions you have selected. If you begin typing, you’ll start entering information into the currently selected cell.\nTo select: Hold the BFWPS over the cell and clice ONCE – not twice. Check the formula bar to make sure you’ve selected what you think you’ve got. You can also look at the bottom right of your spreadsheet for more information.\nYou’ll often work with ranges of cells in formulas. These are defined by the corners of the area you want to work on – often a column of information. In the example below, the range is A1:B6, with the “:” referring to the word “through”.\nTo select a group of cells and act on them all at once: Hover the BFWPS over one corner, click ONCE and drag to the diagonal corner. Make sure the Evil Hand is nowhere to be seen. The entire area will be shaded in except for the currently selected cell. Look at the upper right corner to see how many rows and columns you selected.\n\n\n\n\n\n\n\n\nTo select a column or row : Hover the cursor over the letter at the top of the column. For a row, hover it over the row number in the margin\n\n\n6.1.4 Reading the screen\n\nThe areas of the spreadsheet have different visual clues, and learning to read them will make your life much easier.\n\n6.1.5 Entering data\nSelect the cell and start typing. The information you type won’t be locked into the cell until you hit the Return / Enter key, or move your selection to another cell. Hit “Escape” to cancel the entry.\nYou can’t do a lot of things while you’re editing, so if you have a lot of greyed out menu items, look at your formula bar to see if you are still editing a cell.\nIf you’re having trouble getting to a menu item or seeing the result of your work, try hitting “Escape” and try again. You may not have actually entered the information into the sheet.\n\n\n6.1.6 Locking in headings\nAs your spreadsheet grows vertically with more rows, you’ll want to be able to see the top all the time. When it grows horizontally with more columns, you’ll probably want to see columns in the left, such as names. This is called “Freezing Panes” – you freeze part of the page so it stays in place when you move around.\nIn Google Sheets, this is done via the View -> Freeze menu:\n\n\n\nfreeze panes\n\n\n\n\n6.1.7 Formatting tricks\n\nUse the buttons or the format dialog box to make numbers easier to read.\nIf a column is filled with a lot of text, select the column and look on the Home ribbon next to the formatting area for “Wrap Text”. This means that when you double-click to widen a column, it will get taller, not wider. This is good when you need to save valuable real estate on the screen."
  },
  {
    "objectID": "xl-refresher.html#getting-started-with-a-dataset",
    "href": "xl-refresher.html#getting-started-with-a-dataset",
    "title": "6  An Excel Refresher",
    "section": "6.2 Getting started with a dataset",
    "text": "6.2 Getting started with a dataset\nSLOW DOWN! Don’t do anything until you understand what you have in front of you and can predict what your next mouse click will do to it.\nMost data we encounter was created by someone else for some purpose other than ours. This means that you can’t assume anything. It may not be complete. It may be inaccurate. It may mean something completely different than it appears at first blush.\n\n6.2.1 First steps\n\nDocument where you got the spreadsheet and how you can get back to the original.\nRead anything you can about what it contains. Look for documentation that comes with the data.\nSave the original into a safe place with its original name and metadata. Work on a copy.\nIf the spreadsheet shows #### instead of words or numbers, widen your columns. If it shows 7E-14 or something like that, format them as numbers, not “General”.\nCheck your corners – look at the top left and bottom right. Is the data all in one area? Are there footnotes or other non-data sections mixed in? We’re going to want to fix that later.\n\n\n\n6.2.2 Interview your data\n\n6.2.2.1 Headings\nThe most fraught part of data reporting is understanding what each column actually means. These often have cryptic, bureaucratic names. You may need to go back to the source of the data to be sure you actually understand them.\nIf your data doesn’t have any headings, that’s going to be your first priority. In effect, you’ll need to build what we call a data dictionary or record layout if one hasn’t been provided. Many reporters create these as a page in a dataset.\n\n\n6.2.2.2 Unit of analysis\nA unit of analysis refers to the items that are listed in the rows of your dataset. Ideally, every row should be at the same unit of analysis – a person, an inspection, or a city, for example. Summaries should be separated by a blank row, or moved to a different sheet. Think of this as the noun you’d use to describe every row.\n\n\n6.2.2.3 Row numbers\nThe data was probably given to you in some sort of natural sort order. Different computer systems sort differently – some are case-sensitive, others are not. It may depend on when and where the data as created! The order of the data may even depend on a column you don’t have. If you don’t do something now, you’ll never be able to get back to the original order, which could have meaning for both the agency and for fact-checking."
  },
  {
    "objectID": "xl-filter-sort.html",
    "href": "xl-filter-sort.html",
    "title": "7  Sorting and filtering to find stories",
    "section": "",
    "text": "When Stephen Neukam - who was sitting in this class a year ago - wanted to find out who was funding candidates for Maryland’s open governor’s seat this year, he downloaded data from the State Board of Elections that listed contributions to the wide array of hopefuls seeking to replace Larry Hogan in Annapolis.\nHe wasn’t sure at first what he was looking for, so he started the way that many reporters do with data: by sorting and filtering. Were there outliers in the list of contributions, and which candidates were getting their money from unusual (non-Maryland) sources?\nNeukam quickly found his story: in the race to be governor, Maryland candidates, and in particular Wes Moore, a first-time Democratic candidate, were raising millions of dollars from out of state donors.\nThe story, “Millions in out-of-state donations help fuel high-profile Maryland Democratic governor candidates” helped explain where candidates were going to fund one of the most contested primaries in recent history (Moore ended up winning)."
  },
  {
    "objectID": "xl-filter-sort.html#sorting-and-filtering-as-a-reporting-tool",
    "href": "xl-filter-sort.html#sorting-and-filtering-as-a-reporting-tool",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.2 Sorting and filtering as a reporting tool",
    "text": "7.2 Sorting and filtering as a reporting tool\nSorting and filtering can:\n\nNarrow your focus to specific items that you want to examine in your story.\nShow you rows containing the highest and lowest values of any column. That can be news or it can be errors or other problems with the data.\nLet you answer quick “how many?” questions, with a count of the rows that match your criteria. (In the next lesson, you’ll see that pivot tables, or group-by queries, are much more powerful for this in most cases.)"
  },
  {
    "objectID": "xl-filter-sort.html#example-data",
    "href": "xl-filter-sort.html#example-data",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.3 Example data",
    "text": "7.3 Example data\n::: {.alert .alert-info } - Data from the State Board of Elections for use in this tutorial - Documentation from the SBOE’s site :::\n\nThe data for this is from the Maryland State Board of Elections’s Campaign Finance Database. There are a couple of caveats:\nIt includes money raised as of Jan. 12, 2022, which covers all of 2021.\nThese are self-reported by campaigns, and subject to amendment in case of errors or omissions.\n\nThe original data download link for Wes Moore’s contributions is https://github.com/stephenneukam/CNS_Annapolis/raw/main/Campaign_finance/Moore_ContributionsList.csv. Download it to your computer and then, in a browser, type sheet.new to create a new Google Sheet. From there, use File -> Import and choose “Upload” and select the file on your computer. Click the “Import Data” button when it appears. Then give your sheet a name, like “Wes Moore Contributions”.\nIt’s a good example set for us because it’s been used as the basis of Neukam’s story and it has at least one of each data type that we plan to deal with in Google Sheets or Excel. And, critically, the first row contains headers, not data. Always have headers, even if you have to add them."
  },
  {
    "objectID": "xl-filter-sort.html#understanding-data-types",
    "href": "xl-filter-sort.html#understanding-data-types",
    "title": "7  Sorting and filtering to find stories",
    "section": "7.4 Understanding data types",
    "text": "7.4 Understanding data types\nWhen you open the spreadsheet, the first thing to notice is its granularity. Unlike Census or budget spreadsheets, this is a list capturing specific characteristics of each contribution. Each column has the same type of data from top to bottom. Those types are:\n\nText. Text or “character” columns can come in long or short form. When they are standardized (the values can contain only one of a small list of values), they’re called “categorical”. If they’re more free-form, they’re might be called “free text”. The computer doesn’t know the difference, but you should. The Post data has examples of both. In spreadsheets, text is left-justified (they move toward the left of the cell and will line up vertically at the beginning)\nNumbers. These are pure numbers with no commas, dollar signs or other embellishments. In Google Sheets these can be formatted to look like numbers in different ways, but underneath they’re just numbers. Adding up a column of numbers that has a word in it or has missing values will just be ignored. It will trip up most other languages. These are right-justified, so the last digit is always lined up vertically.\nLogical: This is a subset of text. It can take one of only two values – yes or no, true or false. There is no “maybe”.\nDate and times: These are actual dates on the calendar, which have magical properties. Underneath, they are a number. In Google Sheets and Excel, that number is the number of days since Jan. 1, 1900.1 They can also have time attached to them, which is a fraction of a day. What this means is that the number 44,536.5 is really Dec. 6, 2021 at noon. In Sheets, you use a format to tell the spreadsheet how you want to see the date or time, just the way you look at dollar values with commas and symbols. (If you get a spreadsheet with a lot of dates of 1/1/1900, it means there is a 0 in that column, which is sometimes a fill-in for “I don’t know.”)\n\nHere’s a picture of a date that is shown in a variety of formats.\n\n\n\ndate formats\n\n\nAll of these are the same, underlying value – the number at the left. Notice that all of these are right-justified.\nThis means that when you see “Friday, December 10”, the computer sees 44540.87431. When you put the dates in order, they won’t be alphabetized with all of the Fridays shown together. Instead, they’ll be arranged by the actual date and time.\nIt also means that you can compute 911 response times even when it crosses midnight, or or compute the someone’s age today given a date of birth. Keeping actual calendar dates in your data will give it much more power than just having the words. (Sheets and Excel use the 1st of the month as a stand-in for an actual date when all you know is the month and year.)\n\n7.4.1 Sorting rows\nSorting means rearranging the rows of a data table into a different order. Some reporters take a conceptual shortcut and call this “sorting columns”. That thinking will only get you into trouble – it lets you forget that you want to keep the rows in tact while changing the order in which you see them. In fact, in other languages it’s called “order by” or “arrange” by one or more columns – a much clearer way to think of it.\nTo sort in Google Sheets, first highlight the entire sheet by clicking on the button above the first row and to the left of the first column. So, between the 1 and the A. Then, look for the sort options under the Data drop-down menu at the top of your screen, and choose “Sort Range” and then “Advanced range sorting options”. Trust me, this is how you want to do it. Check the box that says “Data has header row” and Sort by Contribution Date. In this case, sorting from A to Z gives you a list of the contributions in chronological order.\n\n\nAdding fields to the sort\nAdding more columns to the sort box tells Sheets what to do when the first one is the same or tied. For example, sorting first by date then by amount gives you a list that shows all of the contributions by date in sequence:\n\n\n\n\n7.4.2 Filtering\nFiltering means picking out only some of the rows you want to see based on a criteria you select in a column. Think of it as casting a fishing net – the more filters you add, the fewer fish will be caught.\nTo turn on filters in Google Sheets, go to Data -> Create a filter. It will add small down arrows to each column in the first row, another reason why headers are crucial. You can filter by multiple columns, and each filter you select adds more conditions, narrowing your net.\nTo find contributions of $5,000, use the drop-down menu under Contribution Amount to select it and hit “OK”.\nWhen you do this, notice that the drop-down arrow has turned into a solid green funnel and that any rows that don’t match your filter are hidden.\n\n\nThis method works for small-ish and simple-ish columns. If your column has more than 10,000 different entries, such as names or addresses, only the first 10,000 will be considered. We only caught these for stories when someone did a fact-check using a different method of filtering. If your column has a lot of distinct entries, use option that says “Choose One”, and then use the “Contains” option. Better yet, don’t use filtering for counting things at all.\n\nAdd more filters to narrow down your list of cases even more. For example, if you wanted to see $5,000 contributions from individuals, you would choose “Individual” under Contributor Type:\n\n\nDifferent kinds of filters\nThere are several options under the filter drop-down menu under “Filter by condition”, and you also can type values into a search box to try and filter that way (the latter option is best for text columns). There also is a “Filter by color” option. My opinion: don’t do this.\n ## FAQ\n\n\n\nHow do I turn off all of my filters\nIn the data tab, chose “Remove filter” to remove all of the filters.\n\n\nWhere is the button to filter columns?\nSometimes you don’t want to see all of your columns – there are too many and they’re getting confusing. There is no column filter in Sheets or Excel (You’ll see how to filter, or “Select”, columns from a dataset in R later.)\nInstead, you can hide the columns you don’t want to see. When columns and rows are hidden, they generally won’t copy to a new sheet."
  },
  {
    "objectID": "xl-formulas.html",
    "href": "xl-formulas.html",
    "title": "8  Formulas in Google Sheets",
    "section": "",
    "text": "The quick review of math in Google Sheets uses the City of Phoenix’s budgeted spending for the 2018 fiscal year, compared with previous years.\nYou should get into the habit of creating unique identifiers, checking your corners and looking for documentation before you ever start working with a spreadsheet. These habits were covered in Data journalism in the age of replication and on a Google Sheets refresher."
  },
  {
    "objectID": "xl-formulas.html#formulas-in-spreadsheets",
    "href": "xl-formulas.html#formulas-in-spreadsheets",
    "title": "8  Formulas in Google Sheets",
    "section": "8.1 Formulas in spreadsheets",
    "text": "8.1 Formulas in spreadsheets\nWhether you use Google sheets or Excel, remember that every formula begins with the equals sign (=). Rather than the values you want to work with in the formula, you’ll use references to other cells in the sheet.\nThe easiest formulas are simple arithmetic: adding, subtracting, multiplying and dividing two or more cells. You’ll just use simple operators to do this:\n\n\n\noperator\nsymbol\nexample\n\n\n\n\naddition\n+\n=A2+B2\n\n\nsubtraction\n-\n=A2-B2\n\n\nmultiplication\n*\n=A2*B2\n\n\ndivision\n/\n=A2/B2\n\n\n\nHere’s what a spreadsheet looks like while editing some simple arithmetic:\n\n\n\nformula\n\n\nThe other kind of formula is a function. A function is a command that has a name, and requires arguments – usually the cell addresses or the range of addresses that it will act on. Every programming language has functions built in and many have extensions, or packages or libraries, that add even more as users find things they want to do more efficiently. You begin using a function the same way you begin a formula – with an = sign. Here are three common functions that create summary statistics for the numbers contained in a range of addresses. A range is a set of cells defined by its corner cell address: the top left through the bottom right.\nYou’ll usually use them on a single column at a time.\n\n\n\n\n\n\n\nFormula\nWhat it does\n\n\n\n\n=SUM(start:finish)\nAdds up the numbers between start and finish\n\n\n=AVERAGE(start:finish)\nComputes the mean of the numbers\n\n\n=MEDIAN(start:finish)\nDerives the median of the numbers\n\n\n\n…where “start” means the first cell you want to include, and finish means the last cell. Use the cell address of the first number you want to include , a colon, then the cell address of the last number you want to include. You can also select them while you’re editing the formula.\nHere’s an example of adding up all of the rows in a list by county:\n\n\n\nformula"
  },
  {
    "objectID": "xl-formulas.html#common-spreadsheet-arithmetic",
    "href": "xl-formulas.html#common-spreadsheet-arithmetic",
    "title": "8  Formulas in Google Sheets",
    "section": "8.2 Common spreadsheet arithmetic",
    "text": "8.2 Common spreadsheet arithmetic\nThe budget document shows three years’ of data: The actual spending in the fiscal year that ended in 2016; the spending that was estimated for the end of fiscal year 2017; and the proposed spending for fiscal year 2018. The first page of the document shows these amounts for broad spending categories.\nYou may want to widen the columns and format the numbers before you start:\n\n\n\n\n8.2.1 Check the government’s math with SUM\nOur first job is to make sure the government has provided us data that adds up. To do that, we’ll SUM all of the departments’ spending.\nTo add up the numbers from 2016, enter the following formula in cell C11, just below the number provided by the government:\n  =SUM(C2:C8)\n  and hit the enter key\nCopy that formula to the right. Notice how the formula changes the addresses that it is using as you move to the right – it’s adjusted them to refer to the current column.\n\n\n\nWhat’s wrong? The numbers for the budget 2018 don’t add up. (Hint: look at the page called “notes” for an explanation.)\n\n\n8.2.2 Change in spending\nThe increase or decrease in projected spending from 2017 to 2018 is just the difference between the two values, beginning in cell F3\n  new-old, or  =E2-D2\nWhen you copy it down, note how the references to each row also adjusted. In line 3, it’s E3-D3, and so on. Excel and other spreadsheets assume that, most of the time, you want these kinds of adjustments to be made.\n\n\n\n8.2.3 Percent change\nWe can’t tell the rate of growth for each department until we calculate the percent change from one year to another. Now that we already have the change, the percent change is easy. The formula is:\n  ( new - old ) / old\n\n  .. or just scream \"NOO\"\nThe new-old is already in column F, so all that’s left is to divide again. In grade school, you also had to move the decimal place over two spots, since the concept of percent change is “out of 100”. Excel formats will do that for you.\nRemember, it’s always (new-old)/old , NOT the big one minus the little one. Doing it correctly, the answer could be negative, meaning the value fell.\n\n\n\n\n“% change”\n\n\n\nWhen you’re done, you can format the answer as a percentage to get it into whole numbers.\nUntil you get used to it, there’s no harm in doing these calculations step by step. Excel won’t complain if you have extra columns. You can always hide them.\nIt’s also worth comparing the picture you get by looking at raw numbers vs. percentages. In our case, the budget for public safety is expected to rise by a whopping $102 million, but it’s a smaller percentage increase than other, smaller departments.\n\n\n8.2.4 Parts of a whole: percent of total\nWe’d also like to know what portion of the total spending is eaten up by each department. To do that, we need the percent of total.\nIn our case, let’s use the total that the government gave us. In practice, you’d have to decide what to do if your figures didn’t match those provided by officials. You can’t assume that the total is wrong – you could be missing a category, or there could be a mistake in one of the line items.\nThe formula for percent of total is:\n  category / total\nAgain, Excel will multiply by 100, or move the decimal place over for you once you format.\nBut you have a problem: You either have to type in each row, or you get something like this if you try to copy:\n\n\n\n\nWrong way\n\n\n\nExcel has done its magic, adjusting the location of both the numerator and the denominator when you copied. You don’t have to type in each formula one by one, though. Instead, you’ll use anchors, known in spreadsheets as “absolute references”. Think of a dollar sign as an anchor or stickpin, holding down the location of part of your formula. If you put the stickpin before the letter in the formula, it holds the column in place. If you put it before the number, it holds the row in place. If you put it in both places, it holds the cell in place.\nSo our new formula for the percent of total is:\n\n\n\n\nE2/E$10"
  },
  {
    "objectID": "xl-formulas.html#while-were-at-it-two-kinds-of-averages",
    "href": "xl-formulas.html#while-were-at-it-two-kinds-of-averages",
    "title": "8  Formulas in Google Sheets",
    "section": "8.3 While we’re at it: two kinds of averages",
    "text": "8.3 While we’re at it: two kinds of averages\nAlthough it doesn’t make a lot of sense in this context, we’ll go ahead and calculate the average or mean size of each department, and then calculate the median size.\nSimple average, or mean\nA simple average, also known as the mean, is skewed toward very high or very low values. Its formula is\n    sum of pieces / # of pieces that were summed\nBut in Excel, all we need is the word AVERAGE:\n    =AVERAGE(C2:C9)\nMedian\nIn Excel, you can get the median of a list of numbers by just using the formula, MEDIAN()\n  = MEDIAN(C2:C9)"
  },
  {
    "objectID": "xl-formulas.html#the-final-spreadsheet",
    "href": "xl-formulas.html#the-final-spreadsheet",
    "title": "8  Formulas in Google Sheets",
    "section": "8.4 The final spreadsheet",
    "text": "8.4 The final spreadsheet\n\n\n\n\nfinal worksheet\n\n\n\nDoing simple calclutions like this on data that is provided to you by the government lets you ask better questions when you get an interview, and may even convince officials to talk with you. There’s a big difference between asking them to tell you what the budget numbers are, and asking them to explain specific results!"
  },
  {
    "objectID": "xl-formulas.html#faqs",
    "href": "xl-formulas.html#faqs",
    "title": "8  Formulas in Google Sheets",
    "section": "8.5 FAQs",
    "text": "8.5 FAQs\n\nSheets won’t let me copy my formula\nMake sure your formula is locked in by either hitting “Enter” or “Escape”. This is a common problem if you’re in the habit of double-clicking instead of selecting a cell. There are a lot things you can’t do while Excel thinks you’re still entering information.\n\n\nShould I use average or median?\nIt depends. Averages are easier to explain but can be misleading. Usually, if they’re very different, median will be a better representation of the typical person, city or department. Averages in these cases are more like totals.\n\n\nMy percents are small numbers with decimal points\nUse the format as a % button to move the decimal point over two places and insert the percentage symbol."
  },
  {
    "objectID": "xl-practice-noc.html",
    "href": "xl-practice-noc.html",
    "title": "9  Practice exercise",
    "section": "",
    "text": "This exercise helps you understand the difference between filtering and grouping, or, in Excel terms, filtering and pivot tables. In general, you’ll use filtering to look up the details of cases based on a criteria. These are the stories. You’ll use pivot tables to create aggregated data – count, sum, average, and the like.\nIn the real world, reporters move back and forth between these two techniques. They’ll go back and filter for cases once they find something interesting in the aggregation, and will create sums and counts if they think they see a pattern in the cases.\nThat’s pretty abstract concept – it will become more concrete as you go through the motions of this exercise."
  },
  {
    "objectID": "xl-practice-noc.html#data-source",
    "href": "xl-practice-noc.html#data-source",
    "title": "9  Practice exercise",
    "section": "9.1 Data source",
    "text": "9.1 Data source\nData download\nCity link to Notice of claims form and instructions to claimants\nBackgrounder on the data from Helen Wieffering\nThis dataset includes all “Notice of Claims” against the city of Phoenix between 2010 and 2020. These claims refer to damages that people say they suffered because of a problem in the government ranging from slip-and-fall in a city building to use of force by police. It was obtained by Helen Wieffering just after the end of the 2020 fiscal year, so many of the claims from that year will be unresolved. Although the names of the claimants are public record, they were not included in the data compiled here. Also missing is the court case number of any cases that went to court.\nMake sure to look at the “data_dictionary” sheet for the definitions of each column before you start.\nIn this tutorial, I’m providing the equivalent of a data diary for you – if you follow these instructions, you should come up with the same answers I did. You should keep your own diary alongside your work."
  },
  {
    "objectID": "xl-practice-noc.html#sort-filter",
    "href": "xl-practice-noc.html#sort-filter",
    "title": "9  Practice exercise",
    "section": "9.2 Sort / filter",
    "text": "9.2 Sort / filter\nI almost always go into the Excel preferences, and under “Tables & Filters” turn off the check that says “Use table names in formulas”. Sometimes it sticks, sometimes it doesn’t. Have no idea why.\nArrange (sort) the claims using the ID column which is called “orig_order”.\n\nWhat is the orig_order (id) value of the largest PAID claim in the database (combined personal injury and property) ?\nWhat department and cause were associated with that payment?\nHow long did it take to resolve it?\nDescribe the most recent claim made against the police related to law enforcement activities, regardless of whether it’s been resolved.\nFilter for pd_total of greater than zero, and the type called “Shootings”. Look at the lower left corner and record how many claims against the city for shootings were paid any money?\nFind one other thing that you might want to research further that could make an interesting one-off story. These are distinct events that you’d like to know more about, not trends or patterns. This requires no calculation, just your news judgment."
  },
  {
    "objectID": "xl-practice-noc.html#pivot-table-motivation",
    "href": "xl-practice-noc.html#pivot-table-motivation",
    "title": "9  Practice exercise",
    "section": "9.3 Pivot table motivation",
    "text": "9.3 Pivot table motivation\nHere are some questions you have about each department in the city:\n\nWhat was the highest and lowest payout they made?\nHow many claims were made, and how many were paid out?\nWhat was the total amount paid and the average per claim?\n\n\n9.3.1 Using filters to answer big questions\nHere’s one way to do it, using the filters:\n\nRemove all of your filter.\nFilter by department. We’ll start with Aviation, the first one on the list.\nSelect the entire table, including the headings. Copy it to a new sheet. Don’t be surprised if you miss the headings or some of the columns the first couple of times through and have to do it again.\n\nRename the new sheet “Aviation” by double-clicking on its tab.\nMake sure you’re in the top left corner of the screen (use CTL-HOME key if you’re on a real keyboard, CTL-UP and CTL-Left if you’re not.) Freeze the column headings so you can scroll to the bottom and still see what you’re looking at (under View, Freeze top Row)\nNote the number of rows that were copied and subtract one. Record that as the number of claims made.\nMake sure you start from the top of the sheet, and filter out “Unresolved” claims by un-selecting it. Don’t touch anything before you note the number of records found at the bottom left (it should be 11852 in this case). That’s the number of resolved claims. Now filter on the same column, and uncheck “No payment” and note the number of claims. It should be 5718.\nGo to the bottom of the worksheet. Skip a row, and sum the column. Repeat that, replacing =sum() with =max(), =min() , =average().\nWrite down all of your answers on a piece of paper.\n\nDon’t be surprised if you do this wrong a few times, and you have to do it over. But now you have the answer for the Aviation department.\nTry to imagine what you’d do if you wanted to know the average and minimum payout of just those that were settled ."
  },
  {
    "objectID": "xl-practice-noc.html#rinse-repeat",
    "href": "xl-practice-noc.html#rinse-repeat",
    "title": "9  Practice exercise",
    "section": "9.4 Rinse, repeat",
    "text": "9.4 Rinse, repeat\nNow repeat that entire process with at least two other departments that interest you."
  },
  {
    "objectID": "xl-practice-noc.html#pivot-table",
    "href": "xl-practice-noc.html#pivot-table",
    "title": "9  Practice exercise",
    "section": "9.5 Pivot table",
    "text": "9.5 Pivot table\nAt this point, you should be thoroughly sick of having to do things over and over. The purpose of the pivot table is to let you compute these aggregate, or summary, statistics all at once for each category or set of categories. It can compute average, minimum , maximum and sum of values, and it can count the number of rows for any column. (In Excel they’re always the same. That’s not always true of all programs depending on how they handle missing information.)\n\n9.5.1 Create the pivot table\n1, Create an empty pivot table from the data table.\n\nLook for the “Options” tab toward the top left, and turn off “Generate GetPivotData”. In the same tab, under “Options”, set error values to “N/A”, and set Empty cells to zero (meaning there was nothing in that category.)\nDrag “Department” to the row area\nDrag the type_case to the filter area\nDrag the “department” to the Values area and make sure it says “Count of Department”.\nDrag the pd_total to the Values area, and make sure it says “Sum of pd_total”. If it doesn’t, change the pivot table calculation to “Sum”. Repeat that three times, with “average”, “min” and “max”.\n\nTry playing with the placement of items in the pivot table to get an intuition for what they do. You can drag a column into the “Filter” area so that you can pick out just some of the cases for analysis. It works the same way as the filter in the original sheet.\n\n\n9.5.2 Practice questions\nNow answer the following questions:\n\nWhat department had the most claims made against them from FY 2015-16 to through 2019-20?\nHow much did the city pay in that time because of complaints against Streets. Did any department pay more?\nOn average, how much did the city pay vs. the amount that was requested? Include only those complaints that have been resolved – not those that are still underway or have been reopened."
  },
  {
    "objectID": "ethics.html",
    "href": "ethics.html",
    "title": "10  Ethics in data journalism",
    "section": "",
    "text": "This originally appeared on Open News in March 2013.\nIn 2009, a senior web editor asked me and another developer a question: could our development group build a new news application for Tampabay.com that displayed a gallery of mug shots? Stories about goofy crimes with strange mug shots were popular with readers. The vision, on the part of management, was a website that would display the mugshots collected every day from publicly available websites by two editors—well paid, professional editors with other responsibilities.\nNewsrooms are many things. Alive. Filled with energy. Fueled by stress, coffee and profanity. But they are also idea factories. Day after day, ideas come from everywhere. From reporters on the beat. From editors reading random things. From who knows where. Some of them are brilliant. Some would never work. Most need more people and time than are available. And some are dumber than anyone cares to admit.\nWe thought this idea was nuts. Why would we pay someone, let alone an editor, to fetch mug shots from the Internet? Couldn’t we do that with a scraper?\nIf only this were the most complex question we would face.\nBecause given enough time and enough creativity, scraping a mug shot website is easy. You need to recognize a pattern, parse some HTML and gather the pieces you need. At least that’s how it should work. Police agencies that put mugs online usually buy software from a vendor. Apparently, those vendors enjoy making horrific, non-standard, broken-in-interesting-and-unique-ways HTML. You’ll swear. A lot. But you’ll grind it out. And that’s part of the fun. Scraping isn’t any fun with clean, semantic, valid HTML. And scraping mug shot websites, by that definition, is tons of fun.\nThe complexity comes when you realize the data you are dealing with represent real people’s lives."
  },
  {
    "objectID": "ethics.html#problems",
    "href": "ethics.html#problems",
    "title": "10  Ethics in data journalism",
    "section": "10.1 Problems",
    "text": "10.1 Problems\nThe first problem we faced, long before we actually had data, was that data has a life of its own. Because we were going to put this information in front of a big audience, Google was going to find it. That meant if we used our normal open door policy for the Googlebot, someone’s mug shot was going to be the first record in Google for their name, most likely. It would show up first because most people dont actively cultivate their name on the web for visibility in Google. It would show up first because we know how SEO works and they dont. It would show up first because our site would have more traffic than their site, and so Google would rank us higher.\nAnd that record in Google would exist as long as the URL did. Longer when you consider the cached versions Google keeps.\nThat was a problem because here are the things we could not know:\n\nWas this person wrongly arrested?\nWas this person innocent?\nWere the charges dropped against this person?\nDid this person lie about any of their information?"
  },
  {
    "objectID": "ethics.html#the-googlebot",
    "href": "ethics.html#the-googlebot",
    "title": "10  Ethics in data journalism",
    "section": "10.2 The Googlebot",
    "text": "10.2 The Googlebot\nSo it turned out to be very important to know the Googlebot. It’s your friend … until it isn’t. We went to our bosses and said words that no one had said to them before: we did not want Google to index these pages. In a news organization, the page view is the coin of the realm. It is — unfortunately — how many things are evaluated when the bosses ask if it was successful or not. So, with that in mind, Google is your friend. Google brings you traffic. Indeed, Google is your single largest referrer of traffic at a news organization, so you want to throw the doors open and make friends with the Googlebot.\nBut here we were, saying Google wasn’t our friend and that we needed to keep the Googlebot out. And, thankfully, our bosses listened to our argument. They too didn’t want to be the first result in Google for someone.\nSo, to make sure we were telling the Googlebot no, we used three lines of defense. We told it no in robots.txt and on individual pages as a meta tag, and we put the most interesting bits of data into a simple JavaScript wrapper that made it hard on the bot if the first two things failed.\nThe second solution had ramifications beyond the Googlebot. We decided that we were not trying to make a complete copy of the public record. That existed already. If you wanted to look at the actual public records, the sheriff’s offices in the area had websites and they were the official keeper of the record. We were making browsing those images easy, but we were not the public record.\nThat freedom had two consequences: it meant our scrapers could, at a certain point and given a number of failures, just give up on getting a mug. Data entered by humans will be flawed. There will be mistakes. Because of that, our code would have to try and deal with that. Well, there’s an infinite number of ways people can mess things up, so we decided that since we were not going to be an exact copy of the public record, we could deal with the most common failures and dump the rest. During testing, we were getting well over 98% of mugs without having to spend our lives coding for every possible variation of typo.\nThe second consequence of the decision actually came from the newspapers lawyers. They asked a question that dumbfounded us: How long are you keeping mugs? We never thought about it. Storage was cheap. We just assumed we’d keep them all. But, why should we do that? If we’re not a copy of the public record, we dont have to keep them. And, since we didnt know the result of each case, keeping them was really kind of pointless.\nSo, we asked around: How long does a misdemeanor case take to reach a judgement? The answer we got from various sources was about 60 days. From arrest to adjudication, it took about two months. So, at the 60 day mark, we deleted the data. We had no way of knowing if someone was guilty or innocent, so all of them had to go. We even called the script The Reaper.\nWe’d later learn that the practical impacts of this were nil. People looked at the day’s mugs and moved on. The amount of traffic a mug got after the day of arrest was nearly zero."
  },
  {
    "objectID": "ethics.html#data-lifetimes",
    "href": "ethics.html#data-lifetimes",
    "title": "10  Ethics in data journalism",
    "section": "10.3 Data Lifetimes",
    "text": "10.3 Data Lifetimes\nThe life of your data matters. You have to ask yourself, Is it useful forever? Does it become harmful after a set time? We had to confront the real impact of deleting mugs after 60 days. People share them, potentially lengthening their lifetime long after they’ve fallen off the homepage. Delete them and that URL goes away.\nWe couldn’t stop people from sharing links on social media—and indeed probably didn’t want to stop them from doing it. Heck, we did it while we were building it. We kept IMing URLs to each other. And that’s how we realized we had a problem. All our work to minimize the impact on someone wrongly accused of a crime could be damaged by someone sharing a link on Facebook or Twitter.\nThere’s a difference between frictionless and unobstructed sharing and some reasonable constraints.\nWe couldn’t stop people from posting a mug on Facebook, but we didn’t have to make it easy and we didn’t have to put that mug front and center. So we blocked Facebook from using the mug as the thumbnail image on a shared link. And, after 60 days, the URL to the mug will throw a 404 page not found error. Because it’s gone.\nWe couldn’t block Google from memorializing someone’s arrest, only to let it live on forever on Facebook."
  },
  {
    "objectID": "ethics.html#you-are-a-data-provider",
    "href": "ethics.html#you-are-a-data-provider",
    "title": "10  Ethics in data journalism",
    "section": "10.4 You Are a Data Provider",
    "text": "10.4 You Are a Data Provider\nThe last problem didn’t come until months later. And it came in the middle of the night. Two months after we launched, my phone rang at 1 a.m. This is never a good thing. It was my fellow developer, Jeremy Bowers, now with NPR, calling me from a hotel in Washington DC where he was supposed to appear in a wedding the next day. Amazon, which we were using for image hosting, was alerting him that our bandwidth bills had tripled on that day. And our traffic hadn’t changed.\nWhat was going on?\nAfter some digging, we found out that another developer had scraped our site—because we were so much easier to scrape than the Sheriff’s office sites—and had built a game out of our data called Pick the Perp. There were two problems with this: 1. The game was going viral on Digg (when it was still a thing) and Reddit. It was getting huge traffic. 2. That developer had hotlinked our images. He/she was serving them from our S3 account, which meant we were bearing the costs. And they were going up exponentially by the minute.\nWhat we didn’t realize when we launched, and what we figured out after Pick the Perp, was that we had become data provider, in a sense. We had done the hard work of getting the data out of a website and we put it into neat, semantic, easily digestible HTML. If you were after a stream of mugshots, why go through all the hassle of scraping four different sheriff’s office’s horrible HTML when you could just come get ours easily?\nWhoever built Pick the Perp, at least at the time, chose to use our site. But, in doing so, they also chose to hotlink images—use the URL of our S3 bucket, which cost us money—instead of hosting the images themselves.\nThat was a problem we hadn’t considered. People hotlink images all the time. And, until those images are deleted from our system, they’ll stay hotlinked somewhere.\nAmazon’s S3 has a system where you can attach a key to a file that expires after X period of time. In other words, the URL to your image only lasts 15 minutes, or an hour, or however long you decide, before it breaks. It gives you fine grained control over how long someone can use your image URL.\nSo at 3 a.m., after two hours of pulling our hair out, we figured out how to sync our image keys with our cache refreshes. So every 15 minutes, a url to an image expired and Pick the Perp came crashing down.\nWhile the Pick the Perp example is an easy one—it’s never cool to hotlink an image—it does raise an issue to consider. Because you are thinking carefully about how to build your app the right way doesn’t mean someone else will. And it doesn’t mean they won’t just go take your data from your site. So how could you deal with that? Make the data available as a download? Create an API that uses your same ethical constructs? Terms of service? All have pros and cons and are worth talking about before going forward."
  },
  {
    "objectID": "ethics.html#ethical-data",
    "href": "ethics.html#ethical-data",
    "title": "10  Ethics in data journalism",
    "section": "10.5 Ethical Data",
    "text": "10.5 Ethical Data\nWe live in marvelous times. The web offers you no end of tools to make things on the web, to put data from here on there, to make information freely available. But, we’re an optimistic lot. Developers want to believe that their software is being used only for good. And most people will use it for good. But, there are times where the data you’re working with makes people uncomfortable. Indeed, much of journalism is about making people uncomfortable, publishing things that make people angry, or expose people who don’t want to be exposed.\nWhat I want you to think about, before you write a line of code, is what does it mean to put your data on the internet? What could happen, good and bad? What should you do to be responsible about it?\nBecause it can have consequences.\nOn Dec. 23, the Journal News in New York published a map of every legal gun permit holder in their home circulation county. It was a public record. They put it into Google Fusion Tables and Google dutifully geocoded the addresses. It was a short distance to publication from there.\nWithin days, angry gun owners had besieged the newspaper with complaints, saying the paper had given criminals directions to people’s houses where they’d find valuable guns to steal. They said the paper had violated their privacy. One outraged gun owner assembled a list of the paper’s staff, including their home addresses, telephone numbers, email addresses and other details. The paper hired armed security to stand watch at the paper.\nBy February, the New York state legislature removed handgun permits from the public record, citing the Journal News as the reason.\nThere’s no end of arguments to be had about this, but the simple fact is this: The reason people were angry was because you could click on a dot on the map and see a name and an address. In Fusion Tables, removing that info window would take two clicks.\nBecause you can put data on the web does not mean you should put data on the web. And there’s a difference between a record being “public” and “in front of a large audience.”\nSo before you write the first line of code, ask these questions:\n\nThis data is public, but is it widely available? And does making it widely available and easy to use change anything?\nShould this data be searchable in a search engine?\nDoes this data expose information someone has a reasonable expectation that it would remain at least semi-private?\nDoes this data change over time?\nDoes this data expire?\nWhat is my strategy to update or delete data?\nHow easy should it be to share this data on social media?\nHow should I deal with other people who want this data? API? Bulk download?\n\nYour answers to these questions will guide how you build your app. And hopefully, it’ll guide you to better decisions about how to build an app with ethics in mind."
  },
  {
    "objectID": "publicrecords.html",
    "href": "publicrecords.html",
    "title": "11  Public records",
    "section": "",
    "text": "Public records are the lifeblood of investigative reporting. They carry their own philosophical framework, in a manner of speaking.\nKeeping those things in mind as you navigate public records is helpful."
  },
  {
    "objectID": "publicrecords.html#federal-law",
    "href": "publicrecords.html#federal-law",
    "title": "11  Public records",
    "section": "11.1 Federal law",
    "text": "11.1 Federal law\nYour access to public records and public meetings is a matter of the law. As a journalist, it is your job to know this law better than most lawyers. Which law applies depends on which branch of government you are asking. In addition to documents and other kinds of information, FOIA also provides access to structured datasets of the kind we’ll use in this class.\nThe Federal Government is covered by the Freedom of Information Act, or FOIA. FOIA is not a universal term. Do not use it if you are not talking to a federal agency. FOIA is a beacon of openness to the world. FOIA is deeply flawed and frustrating.\nWhy?\n\nThere is no real timetable with FOIA. Requests can take months, even years.\nAs a journalist, you can ask that your request be expedited.\nGuess what? That requires review. More delays.\nExemptions are broad. National security, personal privacy, often overused.\nDenied? You can appeal. More delays.\n\nThe law was enacted in 1966, but it’s still poorly understood by most federal employees, if not outright flouted by political appointees. Lawsuits are common.\nPost 9/11, the Bush administration rolled back many agency rules. Obama ordered a “presumption of openness” but followed it with some of the most restrictive policies ever seen. The Trump Administration, similar to the Obama administration, claims to be the most transparent administration, but has steadily removed records from open access and broadly denied access to records.\nResult? FOIA is in trouble.\nSPJ is a good resource."
  },
  {
    "objectID": "publicrecords.html#state-law",
    "href": "publicrecords.html#state-law",
    "title": "11  Public records",
    "section": "11.2 State law",
    "text": "11.2 State law\nStates are – generally – more open than the federal government. The distance between the government and the governed is smaller. Some states, like Florida and Texas, are very open. Others, like Virginia and Pennsylvania, are not. Maryland is somewhere in the middle.\nThese laws generally give you license to view – and obtain a copy of – a record held by a state or local government agency.\nWhat is a public record? Generally speaking, public records are information stored on paper or in an electronic format held by a state or local government agency, but each state has its own list of types of records – called “exemptions” – that are not subject to disclosure.\nIf a record has both exempt and non-exempt information mixed in, most states require an agency to disclose it after removing the exempt information, a process called “redaction.” Agencies aren’t required to create a record in order to fill your request.\nIn some states but not all – the public information law (or related case law) explicitly dictates that extracting a slice of a database doesn’t constitute creation of a record. Most states can charge you a reasonable fee for time spent retrieving or copying records, though many have provisions to waive those fees for journalists. Every state law operates on a different timeline. Some only require agencies respond in a “reasonable” time, but others spell out exactly how fast an agency must respond to you, and how fast they must turn over the record.\nThe Reporters Committee For Freedom of the Press has a good resource for learning the law in your state.\nPlease and thank you will get you more records than any lawyer or well-written request. Be nice. Be polite. And be persistent. Following up regularly to check on status of a request lets an agency know they can’t ignore you (and some will try). Hunting for records is like any other kind of reporting – you have to do research. You have to ask questions. Ask them: What records do you keep? For how long?\nWhen requesting data, you are going to scare the press office and you are going to confuse the agency lawyer. Request to have their data person on the phone.\nA good source of info? Records retention schedules, often required by law or administrative rule at an agency. Here’s an example from Maryland’s Circuit Courts."
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "12  Using GitHub",
    "section": "",
    "text": "GitHub is a platform for managing and storing files, data and code built atop Git, a popular open source version control software. GitHub accounts are free and it’s easy to get started. The one prerequisite is that you have Git installed on your local computer. There are installers for Mac, Windows and Linux."
  },
  {
    "objectID": "github.html#how-it-works",
    "href": "github.html#how-it-works",
    "title": "12  Using GitHub",
    "section": "12.1 How It Works",
    "text": "12.1 How It Works\nVersion control is based on the ideas that you want to keep track of changes you make to a collection of files and that multiple people can work together without getting in each other’s way or having to do things in a set order. For individual users, it’s great for making sure that you always have your work.\nGitHub users work in what are known as repositories on their local computers and also push changes to a remote repository located on GitHub. That remote repository is key: if you lose your computer, you can fetch a version of your files from GitHub. If you want to work with someone else on the same files, you can each have a local copy, push changes to GitHub and then pull each others’ changes back to your local computers.\nSo, like Microsoft Word’s track changes but with a remote backup and multiple editors."
  },
  {
    "objectID": "github.html#getting-started",
    "href": "github.html#getting-started",
    "title": "12  Using GitHub",
    "section": "12.2 Getting Started",
    "text": "12.2 Getting Started\nAfter installing Git and signing up for a GitHub account, download and install GitHub Desktop. It will have you sign into your GitHub account and then you’ll have access to any existing repositories. If you don’t have any, that’s fine! You can make one locally.\nGitHub has good documentation for working in the Desktop app, and while the emphasis in this book will be on using GitHub for version control, it also supports recording issues (read: problems or questions) with your files, contributing to projects that aren’t yours and more."
  },
  {
    "objectID": "github.html#advanced-use",
    "href": "github.html#advanced-use",
    "title": "12  Using GitHub",
    "section": "12.3 Advanced Use",
    "text": "12.3 Advanced Use\nAlthough our focus is on the GitHub Desktop app, you can use Git and GitHub from your computer’s command line interface, and GitHub has a purpose-built command line client, too. GitHub can also serve as a publishing platform for many types of files, and entire websites are hosted on GitHub Pages."
  },
  {
    "objectID": "r-basics.html",
    "href": "r-basics.html",
    "title": "13  R Basics",
    "section": "",
    "text": "R is a programming language, one specifically geared toward data analysis.\nLike all programming languages, it has certain built-in functions.\nThere are many ways you can write and execute R code. The first, and most basic, is the console, shown here as part of a software tool called RStudio (Desktop Open Source Edition) that we’ll be using all semester.\nThink of the console like talking directly to the R language engine that’s busy working inside your computer. You use it send R commands, making sure to use the only language it understands, which is R. The R language engine processes those commands, and sends information back to you.\nUsing the console is direct, but it has some drawbacks and some quirks we’ll get into later. Let’s examine a basic example of how the console works.\nIf you load up R Studio, type 2+2 into the console and hit enter it will spit out the number 4, as displayed below.\nIt’s not very complex, and you knew the answer before hand, but you get the idea. With R, we can compute things.\nWe can also store things for later use under a specific name. In programming languages, these are called variables. We can assign things to variables using this left-facing arrow: <-. The <- is a called an assignment operator.\nIf you load up R studio and type this code in the console…\n…and then type this code, it will spit out the number 4, as show below.\nWe can have as many variables as we can name. We can even reuse them (but be careful you know you’re doing that or you’ll introduce errors).\nIf you load up R studio and type this code in the console…\n…and then type this, it will split out the number 6, as shown below.\nWe can store anything in a variable. A whole table. A list of numbers. A single word. A whole book. All the books of the 18th century. Variables are really powerful. We’ll explore them at length.\nA quick note about the console: After this brief introduction, we won’t spend much time in R Studio actually writing code directly into the console. Instead, we’ll write code in fancied-up text files – interchangably called R Markdown or R Notebooks – as will be explained in the next chapter. But that code we write in those text files will still execute in the console, so it’s good to know how it works."
  },
  {
    "objectID": "r-basics.html#about-libraries",
    "href": "r-basics.html#about-libraries",
    "title": "13  R Basics",
    "section": "13.1 About libraries",
    "text": "13.1 About libraries\nThe real strength of any programming language is the external libraries (often called “packages”) that power it. The base language can do a lot, but it’s the external libraries that solve many specific problems – even making the base language easier to use.\nWith R, there are hundreds of free, useful libraries that make it easier to do data journalism, created by a community of thousands of R users in multiple fields who contribute to open-source coding projects.\nFor this class, we’ll make use of several external libraries.\nMost of them are part of a collection of libraries bundled into one “metapackage” called the Tidyverse that streamlines tasks like:\n\nLoading data into R. (We’ll use the readr Tidyverse library)\nCleaning and reshaping the data before analysis. (We’ll use the the tidyr and dplyr Tidyverse libraries)\nData analysis. (We’ll use the dplyr Tidyverse library)\nData visualization (We’ll use the ggplot2 Tidyverse library)\n\nTo install packages, we use the function install.packages().\nYou only need to install a library once, the first time you set up a new computer to do data journalism work. You never need to install it again, unless you want to update to a newer version of the package.\nTo install all of the Tidyverse libraries at once, the function is install.packages('tidyverse'). You can type it directly in the console.\nTo use the R Markdown files mentioned earlier, we also need to install a Tidyverse-related library that doesn’t load as part of the core Tidyverse package. The package is called, conveniently, rmarkdown. The code to install that is install.packages('rmarkdown')"
  },
  {
    "objectID": "replication.html",
    "href": "replication.html",
    "title": "14  Data journalism in the age of replication",
    "section": "",
    "text": "A single word in a single job ad for Buzzfeed News posted in 2017 offered an indication of a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThe word you’re seeing more and more of? Reproducible. And it started in earnest in 2017 when data journalism crossed a major threshold in American journalism: It got it’s own section in the Associated Press Stylebook.\n“Data journalism has become a staple of reporting across beats and platforms,” the Data Journalism section of the Stylebook opens. “The ability to analyze quantitative information and present conclusions in an engaging and accurate way is no longer the domain of specialists alone.”\nThe AP’s Data Journalism section discusses how to request data and in what format, guidelines for scraping data from websites with automation, the ethics of using leaked or hacked data and other topics long part of data journalism conference talks.\nBut the third page of the section contains perhaps the most profound commandment: “As a general rule, all assertions in a story based on data analysis should be reproducible. The methodology description in the story or accompanying materials should provide a road map to replicate the analysis.”\nReproducible research – replication – is a cornerstone of scientific inquiry. Researchers across a range of academic disciplines use methods to find new knowledge and publish it in peer reviewed journals. And, when it works, other researchers take that knowledge and try it with their own samples in their own locations. Replication studies exist to take something from an “interesting finding” to a “theory” and beyond.\nIt doesn’t always work.\nReplication studies aren’t funded at nearly the level as new research. And, to the alarm of many, scores of studies can’t be replicated by others. Researchers across disciplines are finding that when their original studies are replicated, flaws are found, or the effects found aren’t as strong as the original. Because of this, academics across a number of disciplines have written about a replication crisis in their respective fields, particularly psychology, social science and medical research.\nIn Chapter 1 of the New Precision Journalism, Phil Meyer wrote that “we journalists would be wrong less often if we adapted to our own use some of the research tools of the social scientists.”\nMeyer would go on to write about how computers pouring over datasets too large to crunch by hand had changed social science from a discipline with “a few data and a lot of interpretation” into a much more meaningful and powerful area of study. If journalists could become comfortable with data and some basic statistics, they too could harness this power.\n“It used to be said that journalism is history in a hurry,” Meyer wrote. “The argument of this book is that to cope with the acceleration of social change in today’s world, journalism must become social science in a hurry.”\nHe wrote that in 1971. It might as well have been yesterday.\nJournalism doesn’t have a history of replication, but the concerns about credibility are substantially greater. Trust in media is at an all time low and shows no signs of improving. While the politics of the day have quite a bit to do with this mistrust of media, being more transparent about what journalists do can’t hurt.\nThe AP’s commandment that “Thou must replicate your findings” could, if taken seriously by the news business, have substantial impacts on how data journalism gets done in newsrooms and how data journalism gets taught, both at professional conferences and universities.\nHow? Two ways.\nIf the AP’s replication rules are to be followed, journalism needs to become much more serious about the tools and techniques used to do data journalism. The days of “point and click” tools to do “quick and dirty” analysis that get published are dying. The days of formal methods using documented steps are here."
  },
  {
    "objectID": "replication.html#the-stylebook",
    "href": "replication.html#the-stylebook",
    "title": "14  Data journalism in the age of replication",
    "section": "14.1 The stylebook",
    "text": "14.1 The stylebook\nTroy Thibodeaux, the editor of the AP’s data journalism team, said the stylebook entry started when the data team found themselves answering the same questions over and over. With a grant from the Knight Foundation, the team began to document their own standards and turn that into a stylebook section.\nFrom the beginning, they had a fairly clear idea of what they wanted to do – think through a project and ask what the frequently asked questions are that came up. It was not going to be a soup-to-nuts guide to how to do a data project.\nWhen the section came out, eyebrows went up on the replication parts, surprising Thibodeaux.\n“From our perspective, this is a core value for us,” he said. “Just for our own benefit, we need to be able to have someone give us a second set of eyes. We benefit from that every day. We catch things for each other.”\nThibodeaux said the AP data team has two audiences when it comes to replication – they have the readers of the work, and members of the collective who may want to do their own work with the data.\n“This is something that’s essential to the way we work,” he said. “And it’s important in terms of transparency and credibility going forward. We thought it would be kind of unexceptionable.”"
  },
  {
    "objectID": "replication.html#replication",
    "href": "replication.html#replication",
    "title": "14  Data journalism in the age of replication",
    "section": "14.2 Replication",
    "text": "14.2 Replication\nMeyer, now 86, said he’s delighted to see replication up for discussion now, but warned that we shouldn’t take it too far.\n“Making the analysis replicable was something I worried about from the very beginning,” he wrote in an email. So much so that in 1967, after publishing stories from his landmark survey after the Detroit riots, he shipped the data and backup materials about it to a social science data repository at the University of North Carolina.\nAnd, in doing so, he opened the door to others replicating his results. One scholar attempted to find fault with Meyer’s analysis by slicing the data ever thinner until the differences weren’t significant – gaming the analysis to criticize the stories.\nMeyer believes replication is vitally important, but doesn’t believe it should take on the trappings of science replication, where newsrooms take their own samples or re-survey a community. That would be prohibitively expensive.\nBut journalists should be sharing their data and analysis steps. And it doesn’t need to be complicated, he said.\n“Replication is a theoretical standard, not a requirement that every investigator duplicate his or her own work for every project,” he said. “Giving enough information in the report to enable another investigator to follow in your footsteps is enough. Just telling enough to make replication possible will build confidence.”\nBut as simple as that sounds, it’s not so simple. Ask social scientists.\nAndrew Gelman, a professor of statistics and political science and director of the Applied Statistics Center at Columbia University, wrote in the journal CHANCE that difficulties with replication in empirical research are pervasive.\n“When an outsider requests data from a published paper, the authors will typically not post or send their data files and code, but instead will point to their sources, so replicators have to figure out exactly what to do from there,” Gelman wrote. “End-to-end replicability is not the norm, even among scholars who actively advocate for the principles of open science.”\nSo goes science, so goes journalism.\nUntil a recent set of exceptions, journalists rarely shared data. The “nerd box” – a sidebar story that explains how a news organization did what they did – is a term that first appeared on NICAR-L, a email listserv of data journalists, in the 1990s.\nIt was a form born in print.\nAs newsrooms adapted to the internet, some news organizations began linking to their data sources if they were online. Often, the data used in stories were obtained through records requests. Sometimes, reporters created the data themselves.\nJournalism, more explicitly than science, is a competitive business. There have been arguments that nerd boxes and downloadable links give too much away to competitors.\nEnter the AP Stylebook.\nThe AP Stylebook argues explicitly for both internal and external replication. Externally, they argue that the “methodology description in the story or accompanying materials should provide a road map to replicate the analysis”, meaning someone else could do the replication post publication.\nInternally, the AP Stylebook says: “If at all possible, an editor or another reporter should attempt to reproduce the results of the analysis and confirm all findings before publication.”\nThere are two problems here.\nFirst is that journalism, unlike science, has no history of replication. There is no “scientific method” for stories. There is no standard “research methods” class taught at every journalism school, at least not where it comes to writing stories. And, beyond that, journalism school isn’t a requirement to get into the news business. In other words, journalism lacks the standards other disciplines have.\nThe second problem is, in many ways, worse: Except for the largest newsrooms, most news organizations lack editors who could replicate the analysis. Many don’t have a second person who would know what to do.\nNot having a second set of eyes in a newsroom is a problem, Thibodeaux acknowledges. Having a data journalism team “is an incredible luxury” at the AP, he said, and their rule is nothing goes on the wire without a second set of eyes.\nThibodeaux, for his part, wants to see fewer “lone nerds in the corner” – it’s too much pressure. That person gets too much credibility from people who don’t understand what they do, and they get too much blame when a mistake is made.\nSo what would replication look like in a newsroom? What does this mean for how newsrooms do data journalism on deadline?"
  },
  {
    "objectID": "replication.html#goodbye-excel",
    "href": "replication.html#goodbye-excel",
    "title": "14  Data journalism in the age of replication",
    "section": "14.3 Goodbye Excel?",
    "text": "14.3 Goodbye Excel?\nFor decades, Excel has been the gateway drug for data journalists, the Swiss Army knife of data tools, the “One Tool You Can’t Live Without.” Investigative Reporters and Editors, an organization that trains investigative journalists, have built large amounts of their curricula around Excel. Of the journalism schools that teach data journalism, most of them begin and end with spreadsheets.\nThe Stylebook says at a minimum, today’s data journalists should keep a log that details:\n\nThe source of the data, making sure to work on a copy of the data and not the original file.\nData dictionaries or any other supporting documentation of the data.\n“Description of all steps required to transform the data and perform the analysis.”\n\nThe trouble with Excel (or Google Sheets) is, unless you are keeping meticulous notes on what steps you are taking, there’s no way to keep track. Many data journalists will copy and paste the values of a formula over the formula itself to prevent Excel from fouling up cell references when moving data around – a practical step that also cuts off another path to being able to replicate the results.\nAn increasing number of data journalists are switching to tools like analysis notebooks, which use languages like Python and R, to document their work. The notebooks, generally speaking, allow a data journalist to mix code and explanation in the same document.\nCombined with online sharing tools like GitHub, analysis notebooks seem to solve the problem of replication. But the number using them is small compared to those using spreadsheets. Recent examples of news organizations using analysis notebooks include the Los Angeles Times, the New York Times, FiveThirtyEight, and Buzzfeed.\nPeter Aldous, a data journalist at Buzzfeed recently published a story about how the online news site used machine learning to find airplanes being used to spy on people in American cities. Published with the story is the code Aldous used to build his case.\n“I think of it this way: As a journalist, I don’t like to simply trust what people tell me. Sometimes sources lie. Sometimes they’re just mistaken. So I like to verify what I’m told,” he wrote in an email. “By the same token, why should someone reading one of my articles believe my conclusions, if I don’t provide the evidence that explains how I reached them?”\nThe methodology document, associated code and source data took Aldous a few hours to create. The story, from the initial data work through the reporting required to make sense of it all, took a year. Aldous said there wasn’t a discussion about if the methodology would be published because it was assumed – “it’s written into our DNA at BuzzFeed News.”\n“My background is in science journalism, and before that (way back in the 1980s) in science,” Aldous said. “In science, there’s been a shift from descriptive methods sections to publishing data and analysis code for reproducible research. And I think we’re seeing a similar shift in data journalism. Simply saying what you’ve done is not as powerful as providing the means for others to repeat and build on your work.”\nThibodeaux said that what Buzzfeed and others do with analysis notebooks and code repositories that include their data is “lovely.”\n“That to me is the shining city on the hill,” Thibodeaux said. “We’re not going to get there, and I don’t think we have to for every story and every use case, and I don’t think it’s necessarily practical for every person working with data to get to that point.”\nThere’s a wide spectrum of approaches that still gets journalists to the essence of what the stylebook is trying to do, Thibodeaux said. There are many tools, many strategies, and the AP isn’t going to advocate for any single one of them, he said. They’re just arguing for transparency and replicability, even if that means doing more work.\n“There’s a certain burden that comes with transparency,” he said. “And I think we have to accept that burden.”\nThe question, Thibodeaux said, is what is sufficient? What’s enough transparency? What does someone need for replicability?\n“Maybe we do have to set a higher standard – the more critical the analysis is to the story, and the more complex that analysis is, that’s going to push the bar on what is a sufficient methodology statement,” he said. “And it could end up being a whole code repo in order to just say, this isn’t black magic, here’s how we got it if you’re so interested.”"
  },
  {
    "objectID": "replication.html#receptivity-is-high",
    "href": "replication.html#receptivity-is-high",
    "title": "14  Data journalism in the age of replication",
    "section": "14.4 “Receptivity … is high”",
    "text": "14.4 “Receptivity … is high”\nThough written almost half a century ago, Meyer foresaw how data journalism was going to arrive in the newsroom.\n“For the new methods to gain currency in journalism, two things must happen,” he wrote. “Editors must feel the need strongly enough to develop the in-house capacity for systematic research … The second need, of course, is for the editors to be able to find the talent to fill this need.”\nMeyer optimistically wrote that journalism schools were prepared to provide that talent – they were not then, and only small handful are now – but students were unlikely to be drawn to these new skills if they didn’t see a chance to use those skills in their careers.\nIt’s taken 45 years, but we are now at this point.\n“The potential for receptivity, especially among the younger generation of newspaper managers, is high,” Meyer wrote."
  },
  {
    "objectID": "replication.html#replication-in-notebooks",
    "href": "replication.html#replication-in-notebooks",
    "title": "14  Data journalism in the age of replication",
    "section": "14.5 Replication in notebooks",
    "text": "14.5 Replication in notebooks\nFor our purposes in this book, replication requires two things from you, the student: What and why. What is this piece of code doing, and why are you doing that here and now? What lead you to this place? That you can copy and paste code from this book or the internet is not impressive. What is necessary for learning is that you know what a piece of code is doing a thing and why you want to do that thing here.\nHow will we replicate? We’ll make use of special text files – R Markdown, also known as R Notebooks – that combine contextual text; the code we use to load, clean, analyze and visualize data; and the output of that code that allowed us to draw certain conclusions to use in stories.\nIn an R Notebook, there are two blocks: A block that uses markdown, which has no special notation, and a code block. The code blocks can run mulitple languages inside R Studio. There’s R, of course, but we could also run Python, a general purpose scripting language; and SQL, or Structured Query Language, the language of databases.\nFor the rest of the class, we’re going to be working in notebooks.\nIn notebooks, you will both run your code and explain each step, much as I am doing here in this online book. This entire book was produced with R markdown files.\nTo start a notebook in R Studio, you click on the green plus in the top left corner and go down to R Notebook.\n\nIn our first lab, we’ll go through the process of editing a markdown notebook."
  },
  {
    "objectID": "aggregates.html",
    "href": "aggregates.html",
    "title": "15  Aggregates",
    "section": "",
    "text": "R is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\n\nlibrary(readr)\n\nTo load them both, you need to do this:\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\n\nlibrary(tidyverse)\n\nYou can keep doing that for as many libraries as you need."
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "15  Aggregates",
    "section": "15.2 Importing data",
    "text": "15.2 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “rds” file, which is a format for storing data with R. Later in the course, we’ll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people. But reading in CSVs is less foolproof than reading in rds files, so for now we’ll work with rds.\nThe rds file we’re going to read in contains individual campaign contributions from Maryland donors via WinRed, an online fundraising platform used by conservatives. We’ll be working with a slice of the data from earlier this year.\nSo step 1 is to import the data. The code to import the data looks like this:\nmaryland_winred_contributions <- read_rds(\"maryland_winred.rds\")\nLet’s unpack that.\nThe first part – maryland_winred_contributions – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one maryland_winred_contributions. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe <- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called maryland_winred_contributions and stuffing all this data into it.\nread_rds() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\nmaryland_winred_contributions <- read_rds(\"maryland_winred.rds\")\nInside of the read_rds() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (maryland_winred.rds). If you put your data in a folder called “data” that sits next to your data notebook, your function would instead look like this:\n\nmaryland_winred_contributions <- read_rds(\"data/maryland_winred.rds\")\n\nIn this data set, each row represents an individual contribution to a federal political committee, typically a candidate’s campaign account.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns(called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(maryland_winred_contributions), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(maryland_winred_contributions)\n\nRows: 54,247\nColumns: 24\n$ linenumber       <chr> \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA11AI\", \"SA…\n$ fec_committee_id <chr> \"C00694323\", \"C00694323\", \"C00694323\", \"C00694323\", \"…\n$ tran_id          <chr> \"A9EF36684A71C4FF097A\", \"A64BB5021801F42B386B\", \"A583…\n$ flag_orgind      <chr> \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND\", \"IND…\n$ org_name         <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ last_name        <chr> \"Hansher\", \"Leishman\", \"Bode\", \"Gallagher\", \"Bode\", \"…\n$ first_name       <chr> \"Beth\", \"Alexander\", \"John\", \"Daniel\", \"Denise\", \"Joe…\n$ middle_name      <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ prefix           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ suffix           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ address_one      <chr> \"7205 Arrowood Rd\", \"1021 Gadsden Ave\", \"29389 Catalp…\n$ address_two      <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ city             <chr> \"Bethesda\", \"Silver Spring\", \"Easton\", \"Towson\", \"Eas…\n$ state            <chr> \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\", \"MD\",…\n$ zip              <dbl> 20817, 20905, 21601, 21204, 21601, 20815, 21502, 2081…\n$ prigen           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ date             <date> 2022-06-01, 2022-05-25, 2022-06-16, 2022-04-05, 2022…\n$ amount           <dbl> 5800.00, 5800.00, 5800.00, 5800.00, 5800.00, 5800.00,…\n$ aggregate_amount <dbl> 5800.00, 22300.00, 11600.00, 17900.00, 17650.00, 1450…\n$ employer         <chr> \"INFORMATION REQUESTED\", \"RIVER\", \"CORN REFINERS ASSO…\n$ occupation       <chr> \"HOMEMAKER\", \"CEO\", \"ASSOCIATION EXECUTIVE\", \"CLO\", \"…\n$ memo_code        <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ memo_text        <chr> \"Earmarked for BLAKE MASTERS FOR SENATE (C00784165)\",…\n$ cycle            <dbl> 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022, 2022,…\n\n\nIf we type head(maryland_winred_contributions), it will print out the columns and the first six rows of data.\n\nhead(maryland_winred_contributions)\n\n# A tibble: 6 × 24\n  linenu…¹ fec_c…² tran_id flag_…³ org_n…⁴ last_…⁵ first…⁶ middl…⁷ prefix suffix\n  <chr>    <chr>   <chr>   <chr>   <lgl>   <chr>   <chr>   <lgl>   <lgl>  <lgl> \n1 SA11AI   C00694… A9EF36… IND     NA      Hansher Beth    NA      NA     NA    \n2 SA11AI   C00694… A64BB5… IND     NA      Leishm… Alexan… NA      NA     NA    \n3 SA11AI   C00694… A5835B… IND     NA      Bode    John    NA      NA     NA    \n4 SA11AI   C00694… AE3AF0… IND     NA      Gallag… Daniel  NA      NA     NA    \n5 SA11AI   C00694… AC5569… IND     NA      Bode    Denise  NA      NA     NA    \n6 SA11AI   C00694… A2B1EB… IND     NA      Kaplan  Joel    NA      NA     NA    \n# … with 14 more variables: address_one <chr>, address_two <chr>, city <chr>,\n#   state <chr>, zip <dbl>, prigen <lgl>, date <date>, amount <dbl>,\n#   aggregate_amount <dbl>, employer <chr>, occupation <chr>, memo_code <lgl>,\n#   memo_text <chr>, cycle <dbl>, and abbreviated variable names ¹​linenumber,\n#   ²​fec_committee_id, ³​flag_orgind, ⁴​org_name, ⁵​last_name, ⁶​first_name,\n#   ⁷​middle_name\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively."
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "15  Aggregates",
    "section": "15.3 Group by and count",
    "text": "15.3 Group by and count\nSo what if we wanted to know how many contributions went to each recipient?\nTo do that by hand, we’d have to take each of the 54,247 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each recipient – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: %>%. The best way to read that operator, in my opinion, is to interpret that as “and then do this.”\nWe’re going to establish a pattern that will come up again and again throughout this book: data %>% function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data %>% group_by(COLUMN NAME) %>% summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with recipient information is called “memo_text”\nHere’s the code to count the number of contributions to each recipient:\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n()\n  )\n\n# A tibble: 519 × 2\n   memo_text                                           count_contribs\n   <chr>                                                        <int>\n 1 Earmarked for ABRAHAM LINCOLN PAC (C00631051)                    3\n 2 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890)              2\n 3 Earmarked for AL FOR MONTANA (C00783381)                         1\n 4 Earmarked for ALAMO PAC (C00387464)                              1\n 5 Earmarked for ALAN SIMS FOR CONGRESS (C00784975)                 2\n 6 Earmarked for ALASKANS FOR DAN SULLIVAN (C00570994)              5\n 7 Earmarked for ALASKANS FOR NICK BEGICH (C00792341)               1\n 8 Earmarked for ALEK FOR OREGON (C00715854)                      121\n 9 Earmarked for ALEX FOR CONGRESS (C00771279)                      3\n10 Earmarked for ALLCORN FOR COLORADO (C00798983)                   2\n# … with 509 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – maryland_winred_contributions – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the recipients, signified by the field name memo_text, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of loans for each county grouping. The line of code count_contribs = n(), says create a new field, called count_contribs and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of contributions in this dataset.\nWhen we run that, we get a list of recipients with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – %>% – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the county with the most loans, we need to sort it in descending order. That looks like this:\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n()\n  ) %>%\n  arrange(desc(count_contribs))\n\n# A tibble: 519 × 2\n   memo_text                                                          count_co…¹\n   <chr>                                                                   <int>\n 1 Earmarked for SAVE AMERICA JOINT FUNDRAISING COMMITTEE (C00770941)       7809\n 2 Earmarked for NRSC (C00027466)                                           6497\n 3 Earmarked for REPUBLICAN NATIONAL COMMITTEE (C00003418)                  4268\n 4 Earmarked for NRCC (C00075820)                                           3945\n 5 Earmarked for TEAM SCALISE (C00750521)                                   2477\n 6 Earmarked for KEVIN MCCARTHY FOR CONGRESS (C00420935)                    1461\n 7 Earmarked for MARCO RUBIO FOR SENATE (C00620518)                         1326\n 8 Earmarked for RON JOHNSON FOR SENATE, INC. (C00482984)                    974\n 9 Earmarked for JOHN KENNEDY FOR US (C00608398)                             955\n10 Earmarked for TIM SCOTT FOR SENATE (C00540302)                            915\n# … with 509 more rows, and abbreviated variable name ¹​count_contribs\n\n\nThe Save America Joint Fundraising Committee has 7,809 contributions, more than any other recipient.\nWe can, if we want, group by more than one thing.\nThe WinRed data contains a column detailing the date of the contribution: “date”.\nWe can group by “memo_text” and “date” to see how many contributions occurred on every date to every recipient. We’ll sort by recipient and then date\n\nmaryland_winred_contributions %>%\n  group_by(memo_text, date) %>%\n  summarise(\n    count_contribs = n()\n  ) %>%\n  arrange(memo_text, date)\n\n`summarise()` has grouped output by 'memo_text'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 9,194 × 3\n# Groups:   memo_text [519]\n   memo_text                                           date       count_contribs\n   <chr>                                               <date>              <int>\n 1 Earmarked for ABRAHAM LINCOLN PAC (C00631051)       2022-06-27              1\n 2 Earmarked for ABRAHAM LINCOLN PAC (C00631051)       2022-06-28              1\n 3 Earmarked for ABRAHAM LINCOLN PAC (C00631051)       2022-06-30              1\n 4 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890) 2022-04-01              1\n 5 Earmarked for ADRIAN SMITH FOR CONGRESS (C00412890) 2022-06-27              1\n 6 Earmarked for AL FOR MONTANA (C00783381)            2022-06-04              1\n 7 Earmarked for ALAMO PAC (C00387464)                 2022-05-16              1\n 8 Earmarked for ALAN SIMS FOR CONGRESS (C00784975)    2022-04-23              1\n 9 Earmarked for ALAN SIMS FOR CONGRESS (C00784975)    2022-05-23              1\n10 Earmarked for ALASKANS FOR DAN SULLIVAN (C00570994) 2022-04-22              1\n# … with 9,184 more rows"
  },
  {
    "objectID": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "href": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "title": "15  Aggregates",
    "section": "15.4 Other summarization methods: summing, mean, median, min and max",
    "text": "15.4 Other summarization methods: summing, mean, median, min and max\nIn the last example, we grouped like records together and counted them, but there’s so much more we can to summarize each group.\nLet’s say we wanted to know the total dollar amount of contributions to each recipient? For that, we could use the sum() function to add up all of the loan values in the column “amount”. We put the column we want to total – “amount” – inside the sum() function sum(amount). Note that we can simply add a new summarize function here, keeping our count_contribs field in our output table.\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount)\n  ) %>%\n  arrange(desc(total_amount))\n\n# A tibble: 519 × 3\n   memo_text                                                     count…¹ total…²\n   <chr>                                                           <int>   <dbl>\n 1 Earmarked for SAVE AMERICA JOINT FUNDRAISING COMMITTEE (C007…    7809 220155.\n 2 Earmarked for NRSC (C00027466)                                   6497 149673.\n 3 Earmarked for NRCC (C00075820)                                   3945 118658.\n 4 Earmarked for REPUBLICAN NATIONAL COMMITTEE (C00003418)          4268 111955.\n 5 Earmarked for NICOLEE AMBROSE FOR CONGRESS (C00812891)            150  67980.\n 6 Earmarked for TEAM SCALISE (C00750521)                           2477  51709.\n 7 Earmarked for PARROTT FOR CONGRESS (C00691931)                    346  45243.\n 8 Earmarked for MARCO RUBIO FOR SENATE (C00620518)                 1326  43279.\n 9 Earmarked for KEVIN MCCARTHY FOR CONGRESS (C00420935)            1461  29612.\n10 Earmarked for RON JOHNSON FOR SENATE, INC. (C00482984)            974  29611.\n# … with 509 more rows, and abbreviated variable names ¹​count_contribs,\n#   ²​total_amount\n\n\nWe can also calculate the average amount for each recipient – the mean – and the amount that sits at the midpoint of our data – the median.\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount),\n    mean_amount = mean(amount),\n    median_amount = median(amount)\n  ) %>%\n  arrange(desc(count_contribs))\n\n# A tibble: 519 × 5\n   memo_text                                     count…¹ total…² mean_…³ media…⁴\n   <chr>                                           <int>   <dbl>   <dbl>   <dbl>\n 1 Earmarked for SAVE AMERICA JOINT FUNDRAISING…    7809 220155.    28.2    20  \n 2 Earmarked for NRSC (C00027466)                   6497 149673.    23.0    12.5\n 3 Earmarked for REPUBLICAN NATIONAL COMMITTEE …    4268 111955.    26.2    18  \n 4 Earmarked for NRCC (C00075820)                   3945 118658.    30.1    25  \n 5 Earmarked for TEAM SCALISE (C00750521)           2477  51709.    20.9    10  \n 6 Earmarked for KEVIN MCCARTHY FOR CONGRESS (C…    1461  29612.    20.3    10  \n 7 Earmarked for MARCO RUBIO FOR SENATE (C00620…    1326  43279.    32.6    20.2\n 8 Earmarked for RON JOHNSON FOR SENATE, INC. (…     974  29611.    30.4    25  \n 9 Earmarked for JOHN KENNEDY FOR US (C00608398)     955  26433.    27.7    15  \n10 Earmarked for TIM SCOTT FOR SENATE (C0054030…     915  29095.    31.8    10  \n# … with 509 more rows, and abbreviated variable names ¹​count_contribs,\n#   ²​total_amount, ³​mean_amount, ⁴​median_amount\n\n\nWe see something interesting here. The mean contribution amount is higher than the median amount in most cases, but the difference isn’t huge. In some cases the mean gets skewed by larger amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest amounts for each recipient? For that, we can use the min() and max() functions.\n\nmaryland_winred_contributions %>%\n  group_by(memo_text) %>%\n  summarise(\n    count_contribs = n(),\n    total_amount = sum(amount),\n    mean_amount = mean(amount),\n    median_amount = median(amount),\n    min_amount = min(amount),\n    max_amount = max(amount)\n  ) %>%\n  arrange(desc(max_amount))\n\n# A tibble: 519 × 7\n   memo_text                     count…¹ total…² mean_…³ media…⁴ min_a…⁵ max_a…⁶\n   <chr>                           <int>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 Earmarked for ALASKANS FOR D…       5   7304   1461.      500    1       5800\n 2 Earmarked for AMAL FOR CONGR…      30  10442.   348.       25    1       5800\n 3 Earmarked for BLAKE MASTERS …      24   6780    282.       25   10       5800\n 4 Earmarked for FRIENDS OF MIK…     358  17711.    49.5      10    0.5     5800\n 5 Earmarked for FRIENDS OF TOD…      75  14833.   198.        5    0.2     5800\n 6 Earmarked for GALLUCH FOR CO…       1   5800   5800      5800 5800       5800\n 7 Earmarked for KELLY FOR ALAS…      54   7900    146.       25    5       5800\n 8 Earmarked for LAWLER FOR CON…       1   5800   5800      5800 5800       5800\n 9 Earmarked for LAXALT FOR SEN…     334  26920.    80.6      25    0.77    5800\n10 Earmarked for MCHENRY FOR CO…       3   6800   2267.      500  500       5800\n# … with 509 more rows, and abbreviated variable names ¹​count_contribs,\n#   ²​total_amount, ³​mean_amount, ⁴​median_amount, ⁵​min_amount, ⁶​max_amount\n\n\nFrom this, we can see that some committees focus on small-dollar donors while others ask for (and get) larger amounts. This pattern isn’t random: campaigns make choices about how they will raise money.\nIt would be interesting to see what the largest donation was. To do that, we could simply take our original data set and sort it from highest to lowest on the amount.\n\nmaryland_winred_contributions %>%\n  arrange(desc(amount))\n\n# A tibble: 54,247 × 24\n   linen…¹ fec_c…² tran_id flag_…³ org_n…⁴ last_…⁵ first…⁶ middl…⁷ prefix suffix\n   <chr>   <chr>   <chr>   <chr>   <lgl>   <chr>   <chr>   <lgl>   <lgl>  <lgl> \n 1 SA11AI  C00694… A9EF36… IND     NA      Hansher Beth    NA      NA     NA    \n 2 SA11AI  C00694… A64BB5… IND     NA      Leishm… Alexan… NA      NA     NA    \n 3 SA11AI  C00694… A5835B… IND     NA      Bode    John    NA      NA     NA    \n 4 SA11AI  C00694… AE3AF0… IND     NA      Gallag… Daniel  NA      NA     NA    \n 5 SA11AI  C00694… AC5569… IND     NA      Bode    Denise  NA      NA     NA    \n 6 SA11AI  C00694… A2B1EB… IND     NA      Kaplan  Joel    NA      NA     NA    \n 7 SA11AI  C00694… A2D9CC… IND     NA      Freas   Dustin  NA      NA     NA    \n 8 SA11AI  C00694… A8F4D9… IND     NA      Monahan Tom     NA      NA     NA    \n 9 SA11AI  C00694… A3E262… IND     NA      powers  rick    NA      NA     NA    \n10 SA11AI  C00694… AD5034… IND     NA      JonesL… Brien   NA      NA     NA    \n# … with 54,237 more rows, 14 more variables: address_one <chr>,\n#   address_two <chr>, city <chr>, state <chr>, zip <dbl>, prigen <lgl>,\n#   date <date>, amount <dbl>, aggregate_amount <dbl>, employer <chr>,\n#   occupation <chr>, memo_code <lgl>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​org_name, ⁵​last_name, ⁶​first_name, ⁷​middle_name\n\n\nLots of $5,800 contributions, which probably seems like a weirdly specific number. And it is! That’s the maximum contribution an individual can give for both a primary and a general election."
  },
  {
    "objectID": "mutating.html",
    "href": "mutating.html",
    "title": "16  Mutating data",
    "section": "",
    "text": "Often the data you have will prompt questions that it doesn’t immediately answer. The PPP loan applications include a total amount column but also columns for different categories the money would be used for: payroll, utilities, rent, etc. Amounts are great, but comparing absolute numbers to each other is only useful if you have a very small number. We need percentages!\nTo do that in R, we can use dplyr and mutate to calculate new metrics in a new field using existing fields of data. That’s the essence of mutate - using the data you have to answer a new question.\nSo first we’ll import the tidyverse so we can read in our data and begin to work with it.\nNow we’ll import a dataset of county-level election results from Maryland’s 2018 primary that is in the data folder in this chapter’s pre-lab directory. We’ll use this to explore ways to create new information from existing data.\nFirst, let’s add a column called percent_election_day for the percentage of votes that were cast on election day for each candidate result in a county. The code to calculate a percentage is pretty simple. Remember, with summarize, we used n() to count things. With mutate, we use very similar syntax to calculate a new value – a new column of data – using other values in our dataset.\nIf we look at what we got when we imported the data, you’ll see there’s election_day as the numerator, and we’ll use votes as the denominator. We can simply things by only selecting a few columns.\nNow we’ve got our percent_election_day column. But what do you see right away? Do those numbers look like we expect them to? No. They’re a decimal expressed as a percentage. So let’s fix that by multiplying by 100.\nNow, does this ordering do anything for us? No. Let’s fix that with arrange.\nSo now we have results ordered by percent_election_day with the highest percentage first. To see the lowest percentage first, we can reverse that arrange function.\nOnly a handful of candidates got less than 50 percent of their total votes in a county on election day. What a difference a pandemic makes."
  },
  {
    "objectID": "mutating.html#another-use-of-mutate",
    "href": "mutating.html#another-use-of-mutate",
    "title": "16  Mutating data",
    "section": "16.1 Another use of mutate",
    "text": "16.1 Another use of mutate\nMutate is also useful for standardizing data - for example, making different spellings of, say, cities into a single one.\nLet’s load some campaign contribution data - in this case Maryland donors to Republican committees via WinRed’s online platform earlier this year - and take a look at the city column in our data.\n\nmaryland_cities <- read_csv(\"data/winred_md_cities.csv\")\n\nRows: 408 Columns: 3\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): city\ndbl (2): count, sum\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nYou’ll notice that there’s a mix of styles: “Baltimore” and “BALTIMORE” for example. R will think those are two different cities, and that will mean that any aggregates we create based on city won’t be accurate.\nSo how can we fix that? Mutate - it’s not just for math! And a function called str_to_upper that will convert a character column into all uppercase. Now we can say exactly how many donations came from Baltimore (I mean, of course, BALTIMORE).\n\nstandardized_maryland_cities <- maryland_cities %>%\n  mutate(\n    upper_city = str_to_upper(city)\n)\n\nThere are lots of potential uses for standardization - addresses, zip codes, anything that can be misspelled or abbreviated."
  },
  {
    "objectID": "mutating.html#a-more-powerful-use",
    "href": "mutating.html#a-more-powerful-use",
    "title": "16  Mutating data",
    "section": "16.2 A more powerful use",
    "text": "16.2 A more powerful use\nMutate is even more useful when combined with some additional functions. Let’s focus on individual contributions from Maryland donors via WinRed; we’d like to group their donations by amount into one of four categories:\n\nUnder $100\n$101-$499\n$500-$2,900\nMore than $2,901\n\nMutate can make that happen by creating a new column and putting in a category value based on the amount of each record. First, let’s load the individual contributions:\n\nmaryland_winred <- read_csv(\"data/winred_md_q2_2022.csv\")\n\nRows: 54247 Columns: 24\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): linenumber, fec_committee_id, tran_id, flag_orgind, last_name, fi...\ndbl   (4): zip, amount, aggregate_amount, cycle\nlgl   (6): org_name, middle_name, prefix, suffix, prigen, memo_code\ndate  (1): date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(maryland_winred)\n\n# A tibble: 6 × 24\n  linenu…¹ fec_c…² tran_id flag_…³ org_n…⁴ last_…⁵ first…⁶ middl…⁷ prefix suffix\n  <chr>    <chr>   <chr>   <chr>   <lgl>   <chr>   <chr>   <lgl>   <lgl>  <lgl> \n1 SA11AI   C00694… A9EF36… IND     NA      Hansher Beth    NA      NA     NA    \n2 SA11AI   C00694… A64BB5… IND     NA      Leishm… Alexan… NA      NA     NA    \n3 SA11AI   C00694… A5835B… IND     NA      Bode    John    NA      NA     NA    \n4 SA11AI   C00694… AE3AF0… IND     NA      Gallag… Daniel  NA      NA     NA    \n5 SA11AI   C00694… AC5569… IND     NA      Bode    Denise  NA      NA     NA    \n6 SA11AI   C00694… A2B1EB… IND     NA      Kaplan  Joel    NA      NA     NA    \n# … with 14 more variables: address_one <chr>, address_two <chr>, city <chr>,\n#   state <chr>, zip <dbl>, prigen <lgl>, date <date>, amount <dbl>,\n#   aggregate_amount <dbl>, employer <chr>, occupation <chr>, memo_code <lgl>,\n#   memo_text <chr>, cycle <dbl>, and abbreviated variable names ¹​linenumber,\n#   ²​fec_committee_id, ³​flag_orgind, ⁴​org_name, ⁵​last_name, ⁶​first_name,\n#   ⁷​middle_name\n\n\nNow that we’ve gotten a look, we can use case_when to give our new category column a value using some standard numeric logic:\n\nmaryland_winred_categories <- maryland_winred %>%\n  mutate(\n    amount_category = case_when(\n        amount < 100 ~ \"Less than $100\",\n        amount >= 100 & amount <= 500 ~ \"Between $100 and $500\",\n        amount > 500 & amount <= 2900 ~ \"Between $501 and $2900\",\n        amount > 2900 ~ \"More than $2,900\"\n      )\n  )\n\nWe can then use our new amount_category column in group_by statements to make summarizing easier:\n\nmaryland_winred_categories %>%\n  group_by(amount_category) %>%\n  summarize(total_amount = sum(amount)) %>%\n  arrange(desc(total_amount))\n\n# A tibble: 4 × 2\n  amount_category        total_amount\n  <chr>                         <dbl>\n1 Less than $100              868922.\n2 Between $100 and $500       454152.\n3 Between $501 and $2900      433948.\n4 More than $2,900            132467.\n\n\nThe largest category - by far - in dollar amount is the sub-$100 category, which makes sense for an online fundraising platform. Big little money.\nMutate is there to make your data more useful and to make it easier for you to ask more and better questions of it."
  },
  {
    "objectID": "workingwithdates.html",
    "href": "workingwithdates.html",
    "title": "17  Working with dates",
    "section": "",
    "text": "One of the most frustrating things in data is working with dates. Everyone has a different opinion on how to record them, and every software package on the planet has to sort it out. Dealing with it can be a little … confusing. And every dataset has something new to throw at you. So consider this an introduction.\nFirst, there’s the right way to display dates in data. Most of the rest of the world knows how to do this, but Americans aren’t taught it. The correct way to display dates is the following format: YYYY-MM-DD, or 2022-09-15. Any date that looks different should be converted into that format when you’re using R.\nLuckily, this problem is so common that the Tidyverse has an entire library for dealing with it: lubridate.\nWe’re going to do this two ways. First I’m going to show you how to use base R to solve a tricky problem. And then we’ll use a library called lubridate to solve a more common and less tricky problem. And then we’ll use a new library to solve most of the common problems before they start. If it’s not already installed, just run install.packages('lubridate')"
  },
  {
    "objectID": "workingwithdates.html#making-dates-dates-again",
    "href": "workingwithdates.html#making-dates-dates-again",
    "title": "17  Working with dates",
    "section": "17.1 Making dates dates again",
    "text": "17.1 Making dates dates again\nFirst, we’ll import tidyverse like we always do and our newly-installed lubridate.\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nLet’s start with a dataset of campaign expenses from Maryland political committees:\n\nmaryland_expenses <- read_csv(\"data/maryland_expenses.csv\")\n\nRows: 5190 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (12): expenditure_date, payee_name, address, payee_type, committee_name,...\ndbl  (1): amount\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 13\n  expen…¹ payee…² address payee…³ amount commi…⁴ expen…⁵ expen…⁶ expen…⁷ expen…⁸\n  <chr>   <chr>   <chr>   <chr>    <dbl> <chr>   <chr>   <chr>   <chr>   <chr>  \n1 3/1/22  Bank o… 10 Lig… Busine…   30.0 Gordon… Other … Bank C… <NA>    EFT    \n2 4/1/22  Bank o… 10 Lig… Busine…   30.0 Gordon… Other … Bank C… <NA>    EFT    \n3 5/1/22  Bank o… 10 Lig… Busine…   30.0 Gordon… Other … Bank C… <NA>    EFT    \n4 6/30/22 Meta    1 Hack… Busine…   26.0 Hazel … Media   Online… <NA>    Debit …\n5 6/13/22 Antiet… 1000 W… Busine… 9506   Barr  … Media   TV      <NA>    Check  \n6 6/10/22 Meta    1 Hack… Busine…   35.4 Ali  S… Media   Online… <NA>    Debit …\n# … with 3 more variables: vendor <chr>, fundtype <chr>, comments <chr>, and\n#   abbreviated variable names ¹​expenditure_date, ²​payee_name, ³​payee_type,\n#   ⁴​committee_name, ⁵​expense_category, ⁶​expense_purpose, ⁷​expense_toward,\n#   ⁸​expense_method\n\n\nTake a look at that first column, expenditure_date. It looks like a date, but see the <chr right below the column name? That means R thinks it’s actually a character column. What we need to do is make it into an actual date column, which lubridate is very good at doing. It has a variety of functions that match the format of the data you have. In this case, the current format is m/d/y, and the lubridate function is called mdy that we can use with mutate:\n\nmaryland_expenses <- maryland_expenses %>% mutate(expenditure_date=mdy(expenditure_date))\n\nhead(maryland_expenses)\n\n# A tibble: 6 × 13\n  expenditure_d…¹ payee…² address payee…³ amount commi…⁴ expen…⁵ expen…⁶ expen…⁷\n  <date>          <chr>   <chr>   <chr>    <dbl> <chr>   <chr>   <chr>   <chr>  \n1 2022-03-01      Bank o… 10 Lig… Busine…   30.0 Gordon… Other … Bank C… <NA>   \n2 2022-04-01      Bank o… 10 Lig… Busine…   30.0 Gordon… Other … Bank C… <NA>   \n3 2022-05-01      Bank o… 10 Lig… Busine…   30.0 Gordon… Other … Bank C… <NA>   \n4 2022-06-30      Meta    1 Hack… Busine…   26.0 Hazel … Media   Online… <NA>   \n5 2022-06-13      Antiet… 1000 W… Busine… 9506   Barr  … Media   TV      <NA>   \n6 2022-06-10      Meta    1 Hack… Busine…   35.4 Ali  S… Media   Online… <NA>   \n# … with 4 more variables: expense_method <chr>, vendor <chr>, fundtype <chr>,\n#   comments <chr>, and abbreviated variable names ¹​expenditure_date,\n#   ²​payee_name, ³​payee_type, ⁴​committee_name, ⁵​expense_category,\n#   ⁶​expense_purpose, ⁷​expense_toward\n\n\nNow look at the expenditure_date column: R says it’s a date column and it looks like we want it to: YYYY-MM-DD. Accept no substitutes.\nLubridate has functions for basically any type of character date format: mdy, ymd, even datetimes like ymd_hms.\nThat’s less code and less weirdness, so that’s good.\nBut to get clean data, I’ve installed a library and created a new field so I can now start to work with my dates. That seems like a lot, but don’t think your data will always be perfect and you won’t have to do these things.\nStill, there’s got to be a better way. And there is.\nFortunately, readr anticipates some date formatting and can automatically handle many of these issues (indeed it uses lubridate under the hood). When you are importing a CSV file, be sure to use read_csv, not read.csv.\nBut you’re not done with lubridate yet. It has some interesting pieces parts we’ll use elsewhere.\nFor example, in spreadsheets you can extract portions of dates - a month, day or year - with formulas. You can do the same in R with lubridate. Let’s say we wanted to add up the total amount spent in each month in our Maryland expenses data.\nWe could use formatting to create a Month field but that would group all the Aprils ever together. We could create a year and a month together, but that would give us an invalid date object and that would create problems later. Lubridate has something called a floor date that we can use.\nSo to follow along here, we’re going to use mutate to create a month field, group by to lump them together, summarize to count them up and arrange to order them. We’re just chaining things together.\n\nmaryland_expenses %>%\n  mutate(month = floor_date(expenditure_date, \"month\")) %>%\n  group_by(month) %>%\n  summarise(total_amount = sum(amount)) %>%\n  arrange(desc(total_amount))\n\n# A tibble: 12 × 2\n   month      total_amount\n   <date>            <dbl>\n 1 2022-06-01    11854494.\n 2 2022-07-01     1807956.\n 3 2022-05-01       94152.\n 4 2022-01-01       18288.\n 5 2022-03-01       18191.\n 6 2022-04-01       11292.\n 7 2022-02-01        5033.\n 8 2020-07-01        1500 \n 9 2021-12-01         236.\n10 2020-02-01          60 \n11 2021-03-01          20 \n12 NA                  NA \n\n\nSo the month of June 2022 had the most expenditures by far in this data. We’ll be learning more about the calendar of campaigns and how it impacts campaign finance data."
  },
  {
    "objectID": "filters.html",
    "href": "filters.html",
    "title": "18  Filters and selections",
    "section": "",
    "text": "More often than not, we have more data than we want. Sometimes we need to be rid of that data. In dplyr, there’s two ways to go about this: filtering and selecting.\nFiltering creates a subset of the data based on criteria. All records where the amount is greater than 150,000. All records that match “College Park”. Something like that. Filtering works with rows – when we filter, we get fewer rows back than we start with.\nSelecting simply returns only the fields named. So if you only want to see city and amount, you select those fields. When you look at your data again, you’ll have two columns. If you try to use one of your columns that you had before you used select, you’ll get an error. Selecting works with columns. You will have the same number of records when you are done, but fewer columns of data to work with.\nNow we’ll import a dataset of county-level election results from Maryland’s 2018 primary that is in the data folder in this chapter’s pre-lab directory. It has results from all across the state, so one place to begin is by looking at individual jurisdictions - Maryland has 23 counties and one independent city, Baltimore. Let’s start by loading tidyverse and reading in the Maryland data:\nThe data we want to filter on is in jurisdiction. So we’re going to use filter and something called a comparison operator. We need to filter all records equal to “Prince George’s”. The comparison operators in R, like most programming languages, are == for equal to, != for not equal to, > for greater than, >= for greater than or equal to and so on.\nBe careful: = is not == and = is not “equal to”. = is an assignment operator in most languages – how things get named.\nAnd just like that, we have just Prince George’s results, which we can verify looking at the head, the first six rows.\nWe also have more data than we might want. For example, we may only want to work with the office, district, candidate name, party and votes.\nTo simplify our dataset, we can use select.\nAnd now we only have five columns of data for whatever analysis we might want to do."
  },
  {
    "objectID": "filters.html#combining-filters",
    "href": "filters.html#combining-filters",
    "title": "18  Filters and selections",
    "section": "18.1 Combining filters",
    "text": "18.1 Combining filters\nSo let’s say we wanted to see all the candidates for governor and the number of votes each received in Prince George’s County. We can do this a number of ways. The first is we can chain together a whole lot of filters.\n\nprince_georges_governor <- primary_18 %>% filter(jurisdiction == \"Prince George's\") %>% filter(office == \"Governor / Lt. Governor\")\n\nnrow(prince_georges_governor)\n\n[1] 10\n\n\nThat gives us 10 applicants. But that’s repetitive, no? We can do better using boolean operators – AND and OR. In this case, AND is & and OR is |.\nThe difference? With AND, all three things must be true to be included. With OR, any of those three things can be true and it will be included. A Prince George’s corporation will get included because it applied for more than $150k. One of the conditions is true.\nHere’s the difference.\n\nand_prince_georges <- primary_18 %>% filter(jurisdiction == \"Prince George's\" & office == \"Governor / Lt. Governor\")\n\nnrow(and_prince_georges)\n\n[1] 10\n\n\nSo AND gives us the same answer we got before. What does OR give us?\n\nor_prince_georges <- primary_18 %>% filter(jurisdiction == \"Prince George's\" | office == \"Governor / Lt. Governor\")\n\nnrow(or_prince_georges)\n\n[1] 355\n\n\nSo there’s 355 rows that are EITHER in Prince George’s OR are governor results. OR is additive; AND is restrictive.\nA general tip about using filter: it’s easier to work your way towards the filter syntax you need rather than try and write it once and trust the result. Each time you modify your filter, check the results to see if they make sense. This adds a little time to your process but you’ll thank yourself for doing it because it helps avoid mistakes."
  },
  {
    "objectID": "data-cleaning-part-i.html",
    "href": "data-cleaning-part-i.html",
    "title": "19  Data Cleaning Part I: Data smells",
    "section": "",
    "text": "Any time you are given a dataset from anyone, you should immediately be suspicious. Is this data what I think it is? Does it include what I expect? Is there anything I need to know about it? Will it produce the information I expect?\nOne of the first things you should do is give it the smell test.\nFailure to give data the smell test can lead you to miss stories and get your butt kicked on a competitive story.\nWith data smells, we’re trying to find common mistakes in data. For more on data smells, read the GitHub wiki post that started it all. Some common data smells are:\nNot all of these data smells are detectable in code. You may have to ask people about the data. You may have to compare it to another dataset yourself. Does the agency that uses the data produce reports from the data? Does your analysis match those reports? That will expose wrongly derived data, or wrong units, or mistakes you made with inclusion or exclusion.\nBut with several of these data smells, we can do them first, before we do anything else.\nWe’re going to examine three here as they apply to some precinct-level election results data: wrong type, missing data and gaps in data."
  },
  {
    "objectID": "data-cleaning-part-i.html#wrong-type",
    "href": "data-cleaning-part-i.html#wrong-type",
    "title": "19  Data Cleaning Part I: Data smells",
    "section": "19.1 Wrong Type",
    "text": "19.1 Wrong Type\nFirst, let’s look at Wrong Type Of Data.\nWe can sniff that out by looking at the output of readr.\nLet’s load the tidyverse.\n\n# Remove scientific notation\noptions(scipen=999)\n# Load the tidyverse\nlibrary(tidyverse)\n\nThen let’s load some precinct-level election results data from Texas for the 2020 general election.\nThis time, we’re going to load the data in a CSV format, which stands for comma separated values and is essentially a fancy structured text file. Each column in the csv is separated – “delimited” – by a comma from the next column.\nWe’re also going to introduce a new argument to our function that reads in the data, read_csv(), called “guess_max”. As R reads in the csv file, it will attempt to make some calls on what “data type” to assign to each field: number, character, date, and so on. The “guess_max” argument says: look at the values in the whatever number of rows we specify before deciding which data type to assign. In this case, we’ll pick 10.\n\n# Load the data\ntexas_precinct_20 <- read_csv(\"data/tx_precinct_2020.csv\", guess_max=10)\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 476915 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): county, precinct, office, candidate, party\ndbl (6): district, votes, absentee, election_day, early_voting, mail\nlgl (2): provisional, limited\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nPay attention to the red warning that signals “one or more parsing issues.” It advises us to run the problems() function to see what went wrong. Let’s do that.\n\nproblems(texas_precinct_20)\n\n# A tibble: 1,640 × 5\n     row   col expected           actual file \n   <int> <int> <chr>              <chr>  <chr>\n 1 28450    12 1/0/T/F/TRUE/FALSE 20     \"\"   \n 2 28450    13 1/0/T/F/TRUE/FALSE 21     \"\"   \n 3 28451    12 1/0/T/F/TRUE/FALSE 15     \"\"   \n 4 28451    13 1/0/T/F/TRUE/FALSE 3      \"\"   \n 5 28465    12 1/0/T/F/TRUE/FALSE 20     \"\"   \n 6 28465    13 1/0/T/F/TRUE/FALSE 20     \"\"   \n 7 28466    12 1/0/T/F/TRUE/FALSE 15     \"\"   \n 8 28466    13 1/0/T/F/TRUE/FALSE 3      \"\"   \n 9 28472    12 1/0/T/F/TRUE/FALSE 22     \"\"   \n10 28472    13 1/0/T/F/TRUE/FALSE 17     \"\"   \n# … with 1,630 more rows\n\n\nIt produces a table of all the parsing problems. It has 1,640 rows, which means we have that many problems. In almost every case here, the readr library has guessed that a given column was of a “logical” data type – True or False. It did it based on very limited information – only 1,000 rows. So, when it hit a value that looked like a date, or a character string, it didn’t know what to do. So it just didn’t read in that value correctly.\nThe easy way to fix this is to set the guess_max argument higher. It will take a little longer to load, but we’ll use every single row in the data set to guess the column type – 476,915\n\ntexas_precinct_20 <- read_csv(\"data/tx_precinct_2020.csv\", guess_max=476915)\n\nRows: 476915 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (6): county, precinct, office, candidate, party, election_day\ndbl (5): district, absentee, mail, provisional, limited\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nThis time, we got no parsing failures. And if we examine the data types readr assigned to each column using glimpse(), they generally make sense.\n\nglimpse(texas_precinct_20)\n\nRows: 476,915\nColumns: 13\n$ county       <chr> \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton…\n$ precinct     <chr> \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Bo…\n$ office       <chr> \"Registered Voters\", \"Ballots Cast\", \"President\", \"Presid…\n$ district     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ candidate    <chr> NA, NA, \"Donald J Trump\", \"Joseph R Biden\", \"Jo Jorgensen…\n$ party        <chr> NA, NA, \"REP\", \"DEM\", \"LBT\", \"GRE\", NA, NA, NA, NA, NA, N…\n$ votes        <dbl> 1003, 665, 557, 92, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 553,…\n$ absentee     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ election_day <chr> NA, \"141\", \"124\", \"11\", \"2\", \"2\", \"0\", \"0\", \"0\", \"0\", \"0\"…\n$ early_voting <dbl> NA, 524, 433, 81, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 430, 8…\n$ mail         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ provisional  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ limited      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nThings that should be characters – like county, precinct, candidate – are characters (chr). Things that should be numbers (dbl) – like votes – are numbers.\nThere are some minor problems. The election_day column is a good example. It read in as a number (chr), even though there clearly are numbers in it judging from our initial inspection. Here’s why: the original file has a single value in that column that is “5+”.\n\ntexas_precinct_20 %>% filter(election_day == \"5+\")\n\n# A tibble: 1 × 13\n  county   precinct office   distr…¹ candi…² party votes absen…³ elect…⁴ early…⁵\n  <chr>    <chr>    <chr>      <dbl> <chr>   <chr> <dbl>   <dbl> <chr>     <dbl>\n1 Anderson 11       Railroa…      NA Chryst… DEM     141      NA 5+          136\n# … with 3 more variables: mail <dbl>, provisional <dbl>, limited <dbl>, and\n#   abbreviated variable names ¹​district, ²​candidate, ³​absentee, ⁴​election_day,\n#   ⁵​early_voting\n\n\nBecause this is just one result that’s weird, we can fix it by comparing the other votes Castaneda received in Anderson to the county totals for her. The difference should be what that “5+” value should be. I’ve done those calculations and it turns out that 49 is the actual likely value.\nWe can fix that pretty easily, by changing that value to “49” using case_when and then using mutate to make the entire column numeric.\n\ntexas_precinct_20 <- texas_precinct_20 %>%\n  mutate(election_day = case_when(\n    election_day == '5+' ~ '49',\n    TRUE ~ election_day\n  ))\n\ntexas_precinct_20 <- texas_precinct_20 %>% mutate(election_day = as.numeric(election_day))\n\nWhen we glimpse() the dataframe again, it’s been changed\n\nglimpse(texas_precinct_20)\n\nRows: 476,915\nColumns: 13\n$ county       <chr> \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton\", \"Newton…\n$ precinct     <chr> \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Box 1\", \"Bo…\n$ office       <chr> \"Registered Voters\", \"Ballots Cast\", \"President\", \"Presid…\n$ district     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ candidate    <chr> NA, NA, \"Donald J Trump\", \"Joseph R Biden\", \"Jo Jorgensen…\n$ party        <chr> NA, NA, \"REP\", \"DEM\", \"LBT\", \"GRE\", NA, NA, NA, NA, NA, N…\n$ votes        <dbl> 1003, 665, 557, 92, 6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 553,…\n$ absentee     <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ election_day <dbl> NA, 141, 124, 11, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 123, 1…\n$ early_voting <dbl> NA, 524, 433, 81, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 430, 8…\n$ mail         <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ provisional  <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ limited      <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n\nNow we’ve got numbers in the election_day column that we can add."
  },
  {
    "objectID": "data-cleaning-part-i.html#missing-data",
    "href": "data-cleaning-part-i.html#missing-data",
    "title": "19  Data Cleaning Part I: Data smells",
    "section": "19.2 Missing Data",
    "text": "19.2 Missing Data\nThe second smell we can find in code is missing data.\nWe can do that by grouping and counting columns. In addition to identifying the presence of NA values, this method will also give us a sense of the distribution of values in those columns.\nLet’s start with the “mail” column, which represents the number of votes a candidate received in a precinct from ballots cast by mail. The following code groups by the mail column, counts the number in each group, and then sorts from highest to lowest. There are 402,345 NA values in this column. This is most of our rows, so that should give us some pause. Either counties didn’t report votes by mail as a separate category or called it something else. This will impact how we can describe the data.\n\ntexas_precinct_20 %>%\n  group_by(mail) %>%\n  summarise(\n    count=n()\n  ) %>%\n  arrange(desc(count))\n\n# A tibble: 30 × 2\n    mail  count\n   <dbl>  <int>\n 1    NA 402345\n 2     0  71589\n 3     1   1708\n 4     2    646\n 5     3    277\n 6     4    125\n 7     5     79\n 8     6     35\n 9     7     21\n10     8     21\n# … with 20 more rows\n\n\nNow let’s try the “provisional” column, which represents the number of accepted provisional votes cast. In this case, there are 135,073 NA values. The rest have different dollar amounts.\n\ntexas_precinct_20 %>%\n  group_by(provisional) %>%\n  summarise(\n    count=n()\n  ) %>%\n  arrange(desc(count))\n\n# A tibble: 27 × 2\n   provisional  count\n         <dbl>  <int>\n 1          NA 473381\n 2           0   2836\n 3           1    210\n 4           2    105\n 5           3     69\n 6           5     53\n 7           4     44\n 8           6     44\n 9          10     29\n10           7     27\n# … with 17 more rows\n\n\nThe number of NA values - 473,381 - is even higher, which should give us confidence that most Texas counties did not report provisional votes at the precinct level. Like with mail votes, this helps define the constraints we have to work under with this data."
  },
  {
    "objectID": "data-cleaning-part-i.html#gaps-in-data",
    "href": "data-cleaning-part-i.html#gaps-in-data",
    "title": "19  Data Cleaning Part I: Data smells",
    "section": "19.3 Gaps in data",
    "text": "19.3 Gaps in data\nLet’s now look at gaps in data. It’s been my experience that gaps in data often have to do with time, but there are other potential gaps, too. To illustrate those, we’re going to introduce some voter registration data from Yadkin County, North Carolina. Let’s load it and take a look:\n\nyadkin_voters <- read_csv(\"data/yadkin_voters.csv\")\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 28456 Columns: 67\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (37): county_desc, voter_reg_num, ncid, last_name, first_name, middle_n...\ndbl   (9): county_id, zip_code, full_phone_number, birth_year, age_at_year_e...\nlgl  (20): mail_addr3, mail_addr4, ward_abbrv, ward_desc, county_commiss_abb...\ndate  (1): registr_dt\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nEach row represents a current or previously registered voter in the county, along with information about that person and the political jurisdictions they reside in. When we talk about gaps, often they indicate the administrative boundaries. Here’s an example: let’s find the most recent registr_dt in this dataset:\n\nyadkin_voters %>% arrange(desc(registr_dt))\n\n# A tibble: 28,456 × 67\n   count…¹ count…² voter…³ ncid  last_…⁴ first…⁵ middl…⁶ name_…⁷ statu…⁸ voter…⁹\n     <dbl> <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n 1      99 YADKIN  000000… ER48… FINCAN… JOY     LYNN    <NA>    A       ACTIVE \n 2      99 YADKIN  000000… ER48… LUCK    ERIN    EILEEN  <NA>    A       ACTIVE \n 3      99 YADKIN  000000… ER37… OSBORNE EMILY   DANIEL… <NA>    A       ACTIVE \n 4      99 YADKIN  000000… AC80… TEMPLE  BRENDA  PUGH    <NA>    A       ACTIVE \n 5      99 YADKIN  000000… EN44… EDWARDS ROCKY   ALLEN   <NA>    A       ACTIVE \n 6      99 YADKIN  000000… BN36… HAWKINS COREY   ALAN    <NA>    A       ACTIVE \n 7      99 YADKIN  000000… BN27… LING    ANDREW  STUART  <NA>    A       ACTIVE \n 8      99 YADKIN  000000… ER48… SPICER  CHARLES STANLEY JR      A       ACTIVE \n 9      99 YADKIN  000000… CG17… WHITAK… SABRINA JOLEIG… <NA>    A       ACTIVE \n10      99 YADKIN  000000… ER48… GLEI    SALLY   COWIN   <NA>    A       ACTIVE \n# … with 28,446 more rows, 57 more variables: reason_cd <chr>,\n#   voter_status_reason_desc <chr>, res_street_address <chr>,\n#   res_city_desc <chr>, state_cd <chr>, zip_code <dbl>, mail_addr1 <chr>,\n#   mail_addr2 <chr>, mail_addr3 <lgl>, mail_addr4 <lgl>, mail_city <chr>,\n#   mail_state <chr>, mail_zipcode <chr>, full_phone_number <dbl>,\n#   confidential_ind <chr>, registr_dt <date>, race_code <chr>,\n#   ethnic_code <chr>, party_cd <chr>, gender_code <chr>, birth_year <dbl>, …\n\n\nIt’s July 14, 2022. That means that this dataset doesn’t have any records newer than that, so if we were describing it we’d need to include that information.\nWhat about the most recent birth_year?\n\nyadkin_voters %>% arrange(desc(birth_year))\n\n# A tibble: 28,456 × 67\n   count…¹ count…² voter…³ ncid  last_…⁴ first…⁵ middl…⁶ name_…⁷ statu…⁸ voter…⁹\n     <dbl> <chr>   <chr>   <chr> <chr>   <chr>   <chr>   <chr>   <chr>   <chr>  \n 1      99 YADKIN  000000… ER47… ADAMS   BLAIN   NOAH    <NA>    A       ACTIVE \n 2      99 YADKIN  000000… ER47… ALEXAN… PERRY   NATHAN… <NA>    A       ACTIVE \n 3      99 YADKIN  000000… ER48… ALVARE… KAREN   ESMERA… <NA>    A       ACTIVE \n 4      99 YADKIN  000000… ER48… ALVARE… PABLO   <NA>    <NA>    A       ACTIVE \n 5      99 YADKIN  000000… ER47… ARCADI… JAZMIN  <NA>    <NA>    A       ACTIVE \n 6      99 YADKIN  000000… ER47… ARELLA… ALEX    <NA>    <NA>    A       ACTIVE \n 7      99 YADKIN  000000… ER47… ARZATE… ELIDETH <NA>    <NA>    A       ACTIVE \n 8      99 YADKIN  000000… ER47… ARZATE… JESSICA <NA>    <NA>    A       ACTIVE \n 9      99 YADKIN  000000… ER47… BALL    ALEXIS  NYKOL   <NA>    A       ACTIVE \n10      99 YADKIN  000000… ER47… BAUTIS… ALEXAN… ROSE    <NA>    A       ACTIVE \n# … with 28,446 more rows, 57 more variables: reason_cd <chr>,\n#   voter_status_reason_desc <chr>, res_street_address <chr>,\n#   res_city_desc <chr>, state_cd <chr>, zip_code <dbl>, mail_addr1 <chr>,\n#   mail_addr2 <chr>, mail_addr3 <lgl>, mail_addr4 <lgl>, mail_city <chr>,\n#   mail_state <chr>, mail_zipcode <chr>, full_phone_number <dbl>,\n#   confidential_ind <chr>, registr_dt <date>, race_code <chr>,\n#   ethnic_code <chr>, party_cd <chr>, gender_code <chr>, birth_year <dbl>, …\n\n\nLots of 2004 records in there, which makes sense, since those folks are just becoming eligible to vote in North Carolina, where the minimum age is 18. In other words, we shouldn’t see records in here where the “birth_year” is greater than 2004. If we do, we should ask some questions.\nIt’s good to be aware of all gaps in data, but they don’t always represent a problem."
  },
  {
    "objectID": "data-cleaning-part-i.html#suspicious-outliers",
    "href": "data-cleaning-part-i.html#suspicious-outliers",
    "title": "19  Data Cleaning Part I: Data smells",
    "section": "19.4 Suspicious Outliers",
    "text": "19.4 Suspicious Outliers\nAny time you are going to focus on a column for analysis, you should check for suspicious values. Are there any unusually large values or unusually small values? Are there any values that should not exist in the data?\nFinally, let’s first look at “registr_dt” again, so we can see if there’s any missing months, or huge differences in the number of registrations by month. If we’re going to work with dates, we should have lubridate handy for floor_date.\n\nlibrary(lubridate)\n\n\nAttaching package: 'lubridate'\n\n\nThe following objects are masked from 'package:base':\n\n    date, intersect, setdiff, union\n\n\nThe floor_date function will allow us to group by month, instead of a single day.\n\nyadkin_voters %>%\n  mutate(registration_month = floor_date(registr_dt, \"month\")) %>%\n  group_by(registration_month) %>%\n   summarise(\n    count=n()\n  ) %>%\n  arrange(registration_month)\n\n# A tibble: 634 × 2\n   registration_month count\n   <date>             <int>\n 1 1900-01-01            12\n 2 1900-05-01             1\n 3 1933-02-01             1\n 4 1949-04-01             1\n 5 1949-12-01             1\n 6 1951-04-01             1\n 7 1955-06-01             1\n 8 1956-05-01             1\n 9 1956-10-01             1\n10 1960-04-01             1\n# … with 624 more rows\n\n\nSo, uh, if this data is accurate, then we have 13 registered voters who are more than 120 years old in Yadkin County. What’s the most likely explanation for this? Some data systems have placeholder values when certain information isn’t known or available. The next oldest registration month is from 1933, which seems plausible."
  },
  {
    "objectID": "data-cleaning-part-ii.html",
    "href": "data-cleaning-part-ii.html",
    "title": "20  Data Cleaning Part II: Janitor",
    "section": "",
    "text": "The bane of every data analyst’s existence is data cleaning.\nEvery developer, every data system, every agency, the all have opinions about how data gets collected. Some decisions make sense from the outside. Some decisions are based entirely on internal politics: who is creating the data, how they are creating it, why they are creating it. Is it automated? Is it manual? Are data normalized? Are there free form fields where users can just type into or does the system restrict them to choices?\nYour journalistic questions – what you want the data to tell you – is almost never part of that equation.\nSo cleaning data is the process of fixing issues in your data so you can answer the questions you want to answer. Data cleaning is a critical step that you can’t skip past. A standard metric is that 80 percent of the time working with data will be spent cleaning and verifying data, and 20 percent the more exciting parts like analysis and visualization.\nThe tidyverse has a lot of built-in tools for data cleaning. We’re also going to make use of a new library, called janitor that has a bunch of great functions for cleaning data. Let’s load those now.\nNow let’s load a tiny slice of our Maryland WinRed contributions. To make the cleaning demonstration in this chapter easier, this dataset only has 14 rows, all from Conowingo, Maryland.\nLet’s glimpse it to get a sense of it, to examine the column data types and possible values.\nAnd let’s examine the full data set.\nThere are a number of issues with this data set that might get in the way of asking questions and receiving accurate answers. They are:\nLet’s get cleaning. Our goal will be to build up one block of code that does all the necessary cleaning in order to answer this question: what is the total amount of contributions from Conowingo, MD in ZIP code 21918?"
  },
  {
    "objectID": "data-cleaning-part-ii.html#cleaning-headers",
    "href": "data-cleaning-part-ii.html#cleaning-headers",
    "title": "20  Data Cleaning Part II: Janitor",
    "section": "20.1 Cleaning headers",
    "text": "20.1 Cleaning headers\nOne of the first places we can start with cleaning data is cleaning the column names (or headers).\nEvery system has their own way of recording headers, and every developer has their own thoughts of what a good idea is within it. R is most happy when headers are lower case, without special characters.\nIf column headers start with a number, or have a space in between two words, you have to set them off with backticks when using them in a function. Generally speaking, we want one word (or words separated by an underscore), all lowercase, that don’t start with numbers.\nThe janitor library makes fixing headers trivially simple with the function clean_names()\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names()\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 14 × 21\n   x1_li…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n14 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <chr>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​x1_linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nThis function changed LAST_NAME to last_name. It put an underscore in address_one to get rid of the space. And it changed 1_linenumber to x1_linenumber. That last one was an improvement – it no longer starts with a number – but it’s still kind of clunky.\nWe can use a tidyverse function rename() to fix that. Let’s just call it linenumber. NOTE: when using rename(), the new name comes first.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber)\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 14 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n14 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <chr>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one"
  },
  {
    "objectID": "data-cleaning-part-ii.html#changing-data-types",
    "href": "data-cleaning-part-ii.html#changing-data-types",
    "title": "20  Data Cleaning Part II: Janitor",
    "section": "20.2 Changing data types",
    "text": "20.2 Changing data types\nRight now, the amount column is stored as a character. Do you see the little <chr> under the amount column in the table above? If we wanted to do math to it, we’d get an error, like so.\n\n# cleaning function\ntotal_conowingo <- cleaned_conowingo %>%\n  summarise(total_amount = sum(amount))\n\nError in `summarise()`:\n! Problem while computing `total_amount = sum(amount)`.\nCaused by error in `sum()`:\n! invalid 'type' (character) of argument\n\n# display the cleaned dataset\ntotal_conowingo\n\nError in eval(expr, envir, enclos): object 'total_conowingo' not found\n\n\nWe got an “invalid ‘type’ (character)” error. So let’s fix that using the mutate() function in concert with as.numeric(). We’ll reuse the same column name, so it overwrites it.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 14 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n14 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nNotice that the amount has been converted to a <dbl>, which is short for double, a number format. When we attempt to add up all of the amounts to create a total, this time it works fine.\n\n# cleaning function\ntotal_conowingo <- cleaned_conowingo %>%\n  summarise(total_amount = sum(amount))\n\n# display the cleaned dataset\ntotal_conowingo\n\n# A tibble: 1 × 1\n  total_amount\n         <dbl>\n1          226"
  },
  {
    "objectID": "data-cleaning-part-ii.html#duplicates",
    "href": "data-cleaning-part-ii.html#duplicates",
    "title": "20  Data Cleaning Part II: Janitor",
    "section": "20.3 Duplicates",
    "text": "20.3 Duplicates\nOne of the most difficult problems to fix in data is duplicate records in the data. They can creep in with bad joins, bad data entry practices, mistakes – all kinds of reasons. A duplicated record isn’t always there because of an error, but you need to know if it’s there before making that determination.\nSo the question is, do we have any records repeated?\nHere we’ll use a function called get_dupes from the janitor library to check for fully repeated records in our cleaned data set.\n\ncleaned_conowingo %>%\n  get_dupes()\n\nNo variable names specified - using all columns.\n\n\n# A tibble: 2 × 22\n  linenu…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n  <chr>    <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n1 SA11AI   C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n2 SA11AI   C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n# … with 12 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>,\n#   dupe_count <int>, and abbreviated variable names ¹​linenumber,\n#   ²​fec_committee_id, ³​flag_orgind, ⁴​last_name, ⁵​first_name, ⁶​middle_name,\n#   ⁷​address_one\n\n\nIn this case, a contribution by Derrick Hamilton in our table is fully duplicated. Every field is identical in each.\nWe can fix this by adding the function distinct() to our cleaning script. This will keep only one copy of each unique record in our table\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount)) %>%\n  distinct()\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one"
  },
  {
    "objectID": "data-cleaning-part-ii.html#cleaning-strings",
    "href": "data-cleaning-part-ii.html#cleaning-strings",
    "title": "20  Data Cleaning Part II: Janitor",
    "section": "20.4 Cleaning strings",
    "text": "20.4 Cleaning strings\nThe rest of the problems with this data set all have to do with inconsistent format of values in a few of the columns. To fix these problems, we’re going to make use of mutate() and a new function, case_when() in concert with “string functions” – special functions that allow us to clean up columns stored as character strings. The tidyverse package stringr has lots of useful string functions, more than we’ll learn in this chapter.\nLet’s start by cleaning up the zip field. Remember, three of rows had a five-digit ZIP code, while two had a nine-digit ZIP code, separated by a hyphen.\nWe’re going to write code that tells R to keep the first five digits on the left, and get rid of anything after that by using mutate() in concert with str_sub(), from the stringr package.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount)) %>%\n  distinct() %>%\n  mutate(zip = str_sub(zip, start=1L, end=5L))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nLet’s break down this line of code. It says: take the value in each zip column and extract the first character on the left (1L) through the fifth character on the left (5L), and then use that five-digit zip to overwrite the zip column.\nWe’ll use a different set of functions to standardize how we standardize the different flavors of the word “Conowingo” in the city column. Let’s start by changing every value to title case – first letter uppercase, subsequent letters lowercase – using the str_to_title() function from stringr.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount)) %>%\n  distinct() %>%\n  mutate(zip = str_sub(zip, start=1L, end=5L)) %>%\n  mutate(city = str_to_title(city))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nThat was enough to standardize two values (CONOWINGO and conowingo). The only ones that remain are the two clear misspellings (Conowing and Conowingoo). To fix those, we’re going to do some manual editing. And for that, we’re going to use case_when(), a function that let’s us say if a value meets a certain condition, then change it, and if it doesn’t, don’t change it.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount)) %>%\n  distinct() %>%\n  mutate(zip = str_sub(zip, start=1L, end=5L)) %>%\n  mutate(city = str_to_title(city)) %>%\n  mutate(city = case_when(\n    city == \"Conowing\" ~ \"Conowingo\",\n    TRUE ~ city\n  ))\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nThis is a little complex, so let’s break it down.\nWhat the code above says, in English, is this: Look at all the values in the city column. If the value is “Conowing”, then (that’s what the “~” means, then) replace it with the word “Conowingo”. If it’s anything other than that (that’s what “TRUE” means, otherwise), then keep the existing value in that column.\nWe could fix “Conowingoo” by adding another line inside that function, that looks identical: city == \"Conowingoo\" ~ \"Conowingo\". Like so.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount)) %>%\n  distinct() %>%\n  mutate(zip = str_sub(zip, start=1L, end=5L)) %>%\n  mutate(city = str_to_title(city)) %>%\n  mutate(city = case_when(\n    city == \"Conowing\" ~ \"Conowingo\",\n    city == \"Conowingoo\" ~ \"Conowingo\",\n    TRUE ~ city\n  ))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nInstead of specifying the exact value, we can also solve the problem by using something more generalizable, using a function called str_detect(), which allows us to search parts of words.\nThe second line of our case_when() function below now says, in English: look in the city column. If you find that one of the values starts with “Conowing” (the “^” symbol means “starts with”), then (the tilde ~ means then) change it to “Conowingo”.\n\n# cleaning function\ncleaned_conowingo <- conowingo %>%\n  clean_names() %>%\n  rename(linenumber = x1_linenumber) %>%\n  mutate(amount = as.numeric(amount)) %>%\n  distinct() %>%\n  mutate(zip = str_sub(zip, start=1L, end=5L)) %>%\n  mutate(city = str_to_title(city)) %>%\n  mutate(city = case_when(\n    str_detect(city,\"^Conowing\") ~ \"Conowingo\",\n    TRUE ~ city\n  ))\n\n\n# display the cleaned dataset\ncleaned_conowingo\n\n# A tibble: 13 × 21\n   linen…¹ fec_c…² tran_id flag_…³ last_…⁴ first…⁵ middl…⁶ prefix suffix addre…⁷\n   <chr>   <chr>   <chr>   <chr>   <chr>   <chr>   <lgl>   <lgl>  <lgl>  <chr>  \n 1 SA11AI  C00694… AC8C71… IND     Pabis   Sherry  NA      NA     NA     144 Bi…\n 2 SA11AI  C00694… A61211… IND     Garvey  Brian   NA      NA     NA     116 Fi…\n 3 SA11AI  C00694… A9C895… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 4 SA11AI  C00694… ABAA82… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 5 SA11AI  C00694… A45519… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 6 SA11AI  C00694… A33987… IND     Huddle… Deborah NA      NA     NA     118 Me…\n 7 SA11AI  C00694… AE7C6D… IND     Hamilt… Derrick NA      NA     NA     553 Be…\n 8 SA11AI  C00694… A07D7A… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n 9 SA11AI  C00694… A29033… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n10 SA11AI  C00694… AD6C11… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n11 SA11AI  C00694… A99BBC… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n12 SA11AI  C00694… A142B2… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n13 SA11AI  C00694… AF8E4C… IND     Buonau… Wm      NA      NA     NA     84 Cle…\n# … with 11 more variables: address_two <lgl>, city <chr>, state <chr>,\n#   zip <chr>, date <date>, amount <dbl>, aggregate_amount <dbl>,\n#   employer <chr>, occupation <chr>, memo_text <chr>, cycle <dbl>, and\n#   abbreviated variable names ¹​linenumber, ²​fec_committee_id, ³​flag_orgind,\n#   ⁴​last_name, ⁵​first_name, ⁶​middle_name, ⁷​address_one\n\n\nBy using str_detect(city,“^Conowing”), we pick up any values that start with “Conowing”, so it would change all four of these. If we used city == “Conowing”, it would only pick up one.\nLastly, there’s the issue with inconsistent spelling of Merry Knoll Lane in the street address column. Do we need to clean this?\nRemember the motivating question that’s driving us to do this cleaning: what is the total amount of contributions from Conowingo, MD in ZIP code 21918?\nWe don’t need the street_address field to answer that question. So we’re not going to bother cleaning it.\nThat’s a good approach for the future. A good rule of thumb is that you should only spend time cleaning fields that are critical to the specific analysis you want to do."
  },
  {
    "objectID": "open-refine.html",
    "href": "open-refine.html",
    "title": "21  Data Cleaning Part III: Open Refine",
    "section": "",
    "text": "Gather ’round kids and let me tell you a tale about one of your authors. In college, this author (Matt Waite) got involved in a project where he mapped crime in Lincoln, Nebraska, looking specifically in the neighborhoods surrounding campus. This was in the mid 1990s. Computers were under powered. Tools were pretty primitive. I was given a database of nearly 50,000 calls for service.\nAnd then I learned that addresses were not stored in a standard way. However the officer wrote it down, that’s how it was recorded.\nWhat did that mean?\nIt meant the Lincoln Police Department came up with dozens of ways to say a single place. And since the mapping software needed the addressed to be in a specific form, I had to fix them. For example, I will go to my grave knowing that Lincoln High School’s street address is 2229 J Street. Police officers wrote down LHS, L.H.S., Lincoln HS, Lincoln H.S., LHS (J Street), 2229 J, 2229 J ST, St., Street and on and on and on. That one was relatively easy. The local convenience store chain, with 8 locations around the city, was harder. I had to use the patrol district to locate them.\nIt took me four months to clean up more than 30,000 unique addresses and map them.\nI tell you this because if I had Open Refine, it would have taken me a week, not four months.\nEvery time I talk about Open Refine, I remember this, and I get mad.\nFortunately (unfortunately?) several columns in the PPP loan data we’re working with are flawed in exactly the same way. There are dozens of variations on just “Baltimore”.\nWe’re going to explore two ways into Open Refine: Through R, and through Open Refine itself."
  },
  {
    "objectID": "open-refine.html#refinr-open-refine-in-r",
    "href": "open-refine.html#refinr-open-refine-in-r",
    "title": "21  Data Cleaning Part III: Open Refine",
    "section": "21.1 Refinr, Open Refine in R",
    "text": "21.1 Refinr, Open Refine in R\nWhat is Open Refine?\nOpen Refine is a software program that has tools – algorithms – that find small differences in text and helps you fix them quickly. How Open Refine finds those small differences is through something called clustering. The algorithms behind clustering are not exclusive to Open Refine, so they can be used elsewhere.\nEnter refinr, a package that contains the same clustering algorithms as Open Refine but all within R. Go ahead and install it if you haven’t already by opening the console and running install.packages(\"refinr\"). Then we can load libraries as we do.\n\nlibrary(tidyverse)\nlibrary(refinr)\nlibrary(janitor)\n\nLet’s load some campaign expenditure data focused on food-related expenses in Washington, D.C. Essentially, where campaigns spend their money on D.C. restaurants.\nNow let’s try and group and count the number of expenditures by recipient. To make it a bit more manageable, let’s use another string function from stringr and filter for recipients that start with the uppercase “W” or lowercase “w” using the function str_detect() with a regular expression.\nThe filter function in the codeblock below says: look in the city column, and pluck out any value that starts with (the “^” symbol means “starts with”) a lowercase “w” OR (the vertical “|”, called a pipe, means OR) an uppercase “W”.\n\ndc_food %>%\n  group_by(recipient_name) %>%\n  summarise(\n    count=n()\n  ) %>%\n  filter(str_detect(recipient_name, '^w|^W')) %>%\n  arrange(recipient_name)\n\n# A tibble: 50 × 2\n   recipient_name      count\n   <chr>               <int>\n 1 W HOTELS                3\n 2 W. MILLAR & CO          1\n 3 W. MILLAR & CO.         5\n 4 WA METRO                1\n 5 WAGSHAL'S DELI          1\n 6 WALMART SUPERCENTER    13\n 7 WALTER'S SPORTS BAR     1\n 8 WALTERS                 1\n 9 WALTERS SPORT           1\n10 WALTERS SPORTS          1\n# … with 40 more rows\n\n\nThere are several problems in this data that will prevent proper grouping and summarizing. We’ve learned several functions to do this manually.\nBy using the Open Refine package for R, refinr, our hope is that it can identify and standardize the data with a little more ease.\nThe first merging technique that’s part of the refinr package we’ll try is the key_collision_merge.\nThe key collision merge function takes each string and extracts the key parts of it. It then puts every key in a bin based on the keys matching.\nOne rule you should follow when using this is: do not overwrite your original fields. Always work on a copy. If you overwrite your original field, how will you know if it did the right thing? How can you compare it to your original data? To follow this, I’m going to mutate a new field called clean_city and put the results of key collision merge there.\n\ncleaned_dc_food <- dc_food %>%\n  mutate(recipient_clean=key_collision_merge(recipient_name)) %>%\n  select(recipient_name, recipient_clean, everything())\n\ncleaned_dc_food\n\n# A tibble: 13,108 × 50\n   recipient_n…¹ recip…² commi…³ commi…⁴ repor…⁵ repor…⁶ image…⁷ line_…⁸ trans…⁹\n   <chr>         <chr>   <chr>   <chr>     <dbl> <chr>     <dbl> <chr>   <chr>  \n 1 ST. REGIS HO… ST. RE… C00658… MARK G…    2021 YE      2.02e17 17      SB17.I…\n 2 HARKER, GRAY  HARKER… C00536… JOBS, …    2021 YE      2.02e17 21B     SB21B.…\n 3 REPUBLICAN N… REPUBL… C00055… NY REP…    2021 M2      2.02e17 21B     B35A42…\n 4 JOE'S SEAFOO… JOE'S … C00551… OORAH!…    2021 M4      2.02e17 21B     SB21B.…\n 5 NATIONAL DEM… NATION… C00391… JIM CO…    2021 Q1      2.02e17 17      BA7DD5…\n 6 NATIONAL DEM… NATION… C00454… MARCIA…    2021 Q1      2.02e17 17      SB17.2…\n 7 CAFE MILANO   CAFE M… C00391… JIM CO…    2021 Q1      2.02e17 17      B68639…\n 8 CAPITOL HILL… CAPITO… C00383… CONAWA…    2021 Q1      2.02e17 17      BA078C…\n 9 CAPITOL HILL… CAPITO… C00725… CLIFF …    2021 Q1      2.02e17 17      17883  \n10 CAPITOL HILL… CAPITO… C00383… CONAWA…    2021 Q1      2.02e17 17      B51CE8…\n# … with 13,098 more rows, 41 more variables: file_number <dbl>,\n#   entity_type <chr>, entity_type_desc <chr>,\n#   unused_recipient_committee_id <chr>, recipient_committee_id <chr>,\n#   recipient_state <chr>, beneficiary_committee_name <chr>,\n#   national_committee_nonfederal_account <lgl>, disbursement_type <lgl>,\n#   disbursement_type_description <lgl>, disbursement_description <chr>,\n#   memo_code <chr>, memo_code_full <lgl>, disbursement_date <chr>, …\n\n\nTo examine changes refinr made, let’s examine the changes it made to cities that start with the letter “W”.\n\ncleaned_dc_food %>%\n  group_by(recipient_name, recipient_clean) %>%\n  summarise(\n    count=n()\n  ) %>%\n  filter(str_detect(recipient_clean, \"^w|^W\")) %>%\n  arrange(recipient_clean)\n\n`summarise()` has grouped output by 'recipient_name'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 52 × 3\n# Groups:   recipient_name [52]\n   recipient_name      recipient_clean     count\n   <chr>               <chr>               <int>\n 1 W HOTELS            W HOTELS                3\n 2 W. MILLAR & CO      W. MILLAR & CO.         1\n 3 W. MILLAR & CO.     W. MILLAR & CO.         5\n 4 WA METRO            WA METRO                1\n 5 WAGSHAL'S DELI      WAGSHAL'S DELI          1\n 6 WALMART SUPERCENTER WALMART SUPERCENTER    13\n 7 WALTERS             WALTERS                 1\n 8 WALTERS SPORT       WALTERS SPORT           1\n 9 WALTERS SPORTS      WALTERS SPORTS          1\n10 WALTER'S SPORTS BAR WALTERS SPORTS BAR      1\n# … with 42 more rows\n\n\nYou can see several changes on the second page of results, including that refinr made “PIZZA, WE THE” into “WE THE PIZZA” which is pretty smart. Other potential changes, grouping together “WASHINGTON NATIONALS” and “WASHINGTON NATIONALS BASEBALL CLUB”, didn’t happen. Key collision will do well with different cases, but all of our records are upper case.\nThere’s another merging algorithim that’s part of refinr that works a bit differently, called n_gram_merge(). Let’s try applying that one.\n\ncleaned_dc_food <- dc_food %>%\n  mutate(recipient_clean=n_gram_merge(recipient_name)) %>%\n  select(recipient_name, recipient_clean, everything())\n\ncleaned_dc_food\n\n# A tibble: 13,108 × 50\n   recipient_n…¹ recip…² commi…³ commi…⁴ repor…⁵ repor…⁶ image…⁷ line_…⁸ trans…⁹\n   <chr>         <chr>   <chr>   <chr>     <dbl> <chr>     <dbl> <chr>   <chr>  \n 1 ST. REGIS HO… ST. RE… C00658… MARK G…    2021 YE      2.02e17 17      SB17.I…\n 2 HARKER, GRAY  HARKER… C00536… JOBS, …    2021 YE      2.02e17 21B     SB21B.…\n 3 REPUBLICAN N… REPUBL… C00055… NY REP…    2021 M2      2.02e17 21B     B35A42…\n 4 JOE'S SEAFOO… JOE'S … C00551… OORAH!…    2021 M4      2.02e17 21B     SB21B.…\n 5 NATIONAL DEM… NATION… C00391… JIM CO…    2021 Q1      2.02e17 17      BA7DD5…\n 6 NATIONAL DEM… NATION… C00454… MARCIA…    2021 Q1      2.02e17 17      SB17.2…\n 7 CAFE MILANO   CAFE M… C00391… JIM CO…    2021 Q1      2.02e17 17      B68639…\n 8 CAPITOL HILL… CAPITO… C00383… CONAWA…    2021 Q1      2.02e17 17      BA078C…\n 9 CAPITOL HILL… CAPITO… C00725… CLIFF …    2021 Q1      2.02e17 17      17883  \n10 CAPITOL HILL… CAPITO… C00383… CONAWA…    2021 Q1      2.02e17 17      B51CE8…\n# … with 13,098 more rows, 41 more variables: file_number <dbl>,\n#   entity_type <chr>, entity_type_desc <chr>,\n#   unused_recipient_committee_id <chr>, recipient_committee_id <chr>,\n#   recipient_state <chr>, beneficiary_committee_name <chr>,\n#   national_committee_nonfederal_account <lgl>, disbursement_type <lgl>,\n#   disbursement_type_description <lgl>, disbursement_description <chr>,\n#   memo_code <chr>, memo_code_full <lgl>, disbursement_date <chr>, …\n\n\nTo examine changes refinr made with this algorithm, let’s again look at cities that start with the letter “A”. Examining Aberdeen and Annapolis, we see there wasn’t a substantial change from the previous method.\n\ncleaned_dc_food %>%\n  group_by(recipient_name, recipient_clean) %>%\n  summarise(\n    count=n()\n  ) %>%\n  filter(str_detect(recipient_clean, \"^w|^W\")) %>%\n  arrange(recipient_clean)\n\n`summarise()` has grouped output by 'recipient_name'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 51 × 3\n# Groups:   recipient_name [51]\n   recipient_name      recipient_clean     count\n   <chr>               <chr>               <int>\n 1 W HOTELS            W HOTELS                3\n 2 W. MILLAR & CO      W. MILLAR & CO.         1\n 3 W. MILLAR & CO.     W. MILLAR & CO.         5\n 4 WA METRO            WA METRO                1\n 5 WAGSHAL'S DELI      WAGSHAL'S DELI          1\n 6 WALMART SUPERCENTER WALMART SUPERCENTER    13\n 7 WALTERS             WALTERS                 1\n 8 WALTERS SPORT       WALTERS SPORT           1\n 9 WALTERS SPORTS      WALTERS SPORT           1\n10 WALTER'S SPORTS BAR WALTERS SPORTS BAR      1\n# … with 41 more rows\n\n\nThis method also made some good changes, but not in every case. No single method will be perfect and often a combination is necessary.\nThat’s how you use the Open Refine r package, refinr.\nNow let’s upload the data to the interactive version of OpenRefine, which really shines at this task."
  },
  {
    "objectID": "open-refine.html#manually-cleaning-data-with-open-refine",
    "href": "open-refine.html#manually-cleaning-data-with-open-refine",
    "title": "21  Data Cleaning Part III: Open Refine",
    "section": "21.2 Manually cleaning data with Open Refine",
    "text": "21.2 Manually cleaning data with Open Refine\nOpen Refine is free software. You should download and install it; the most recent version is 3.6.0. Refinr is great for quick things on smaller datasets that you can check to make sure it’s not up to any mischief.\nFor bigger datasets, Open Refine is the way to go. And it has a lot more tools than refinr does (by design).\nAfter you install it, run it. (If you are on a Mac it might tell you that it can’t run the program. Go to System Preferences -> Security & Privacy -> General and click “Open Anyway”.) Open Refine works in the browser, and the app spins up a small web server visible only on your computer to interact with it. A browser will pop up automatically.\nYou first have to import your data into a project. Click the choose files button and upload a csv of the DC food-related expenditures.\n\n\n\n\n\nAfter your data is loaded into the app, you’ll get a screen to look over what the data looks like. On the top right corner, you’ll see a button to create the project. Click that.\n\n\n\n\n\nOpen Refine has many, many tools. We’re going to use one piece of it, as a tool for data cleaning. To learn how to use it, we’re going to clean the “recipient_name” field.\nFirst, let’s make a copy of the original recipient_name column so that we can preserve the original data while cleaning the new one.\nClick the dropdown arrow next to the recipient_name column, choose “edit column” > “Add column based on this column”:\n\n\n\n\n\nOn the window that pops up, type “recipient_name_orig” in the “new column name” field. Then hit the OK button.\n\n\n\n\n\nNow, let’s get to work cleaning the recipient_name column.\nNext to the recipient_name field name, click the down arrow, then facet, then text facet.\n\n\n\n\n\nAfter that, a new box will appear on the left. It tells us how many unique recipient_names there are: 1,655. And, there’s a button on the right of the box that says Cluster.\n\n\n\n\n\nClick the cluster button. A new window will pop up, a tool to help us identify things that need to be cleaned, and quickly clean them.\n\n\n\n\n\nThe default “method” used is a clustering algorithim called “key collision”, using the fingerprint function. This is the same method we used with the refinr package above.\nAt the top, you’ll see which method was used, and how many clusters that algorithm identified. There are several different methods, each of which work slightly differently and produce different results.\n\n\n\n\n\nThen, below that, you can see what those clusters are. Right away, we can see how useful this program is. It identified 99 rows that have some variation on “Hawk N Dove” in the recipient_name field. It proposed changing them all to “HAWK N DOVE”.\nUsing human judgement, you can say if you agree with the cluster. If you do, click the “merge” checkbox. When it merges, the new result will be what it says in New Cell Value. Most often, that’s the row with the most common result. You also can manually edit the “New Cell Value” if you want it to be something else:\nNow begins the fun part: You have to look at all 88 clusters found and decide if they are indeed valid. The key collision method is very good, and very conservative. You’ll find that most of them are usually valid.\nBe careful! If you merge two things that aren’t supposed to be together, it will change your data in a way that could lead to inaccurate results.\nWhen you’re done, click Merge Selected and Re-Cluster.\nIf any new clusters come up, evaluate them. Repeat until either no clusters come up or the clusters that do come up are ones you reject.\nNow. Try a new method, maybe the “nearest neighbor levenshtein” method. Notice that it finds even more clusters - 167 - using a slightly different approach.\nRinse and repeat.\nYou’ll keep doing this, and if the dataset is reasonably clean, you’ll find the end.\nWhen you’re finished cleaning, click “Merge Selected & Close”.\nThen, export the data as a csv so you can load it back into R.\n\n\n\n\n\nA question for all data analysts – if the dataset is bad enough, can it ever be cleaned?\nThere’s no good answer. You have to find it yourself."
  },
  {
    "objectID": "pdfs.html",
    "href": "pdfs.html",
    "title": "22  Cleaning Data Part IV: PDFs",
    "section": "",
    "text": "The next circle of Hell on the Dante’s Inferno of Data Journalism is the PDF. Governments everywhere love the PDF and publish all kinds of records in a PDF. The problem is a PDF isn’t a data format – it’s a middle finger, saying I’ve Got Your Accountability Right Here, Pal.\nIt’s so ridiculous that there’s a constellation of tools that do nothing more than try to harvest tables out of PDFs. There are online services like CometDocs where you can upload your PDF and point and click your way into an Excel file. There are mobile device apps that take a picture of a table and convert it into a spreadsheet. But one of the best is a tool called Tabula. It was build by journalists for journalists.\nThere is a version of Tabula that will run inside of R – a library called Tabulizer – but the truth is I’m having the hardest time installing it on my machine, which leads me to believe that trying to install it across a classroom of various machines would be disastrous. The standalone version works just fine, and it provides a useful way for you to see what’s actually going on.\nUnfortunately, harvesting tables from PDFs with Tabula is an exercise in getting your hopes up, only to have them dashed. We’ll start with an example. First, let’s load the tidyverse and janitor."
  },
  {
    "objectID": "pdfs.html#easy-does-it",
    "href": "pdfs.html#easy-does-it",
    "title": "22  Cleaning Data Part IV: PDFs",
    "section": "22.1 Easy does it",
    "text": "22.1 Easy does it\nTabula works best when tables in PDFs are clearly defined and have nicely-formatted information. Here’s a perfect example: active voters by county in Maryland.\nDownload and install Tabula. Tabula works much the same way as Open Refine does – it works in the browser by spinning up a small webserver in your computer.\nWhen Tabula opens, you click browse to find the PDF on your computer somewhere, and then click import. After it imports, click autodetect tables. You’ll see red boxes appear around what Tabula believes are the tables. You’ll see it does a pretty good job at this.\n\n\n\n\n\nNow you can hit the green “Preview & Export Extracted Data” button on the top right. You should see something very like this:\n\n\n\n\n\nYou can now export that extracted table to a CSV file using the “Export” button. And then we can read it into R:\n\nvoters_by_county <- read_csv(\"data/tabula-Eligible Active Voters by County - PG20.csv\")\n\nRows: 25 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): County\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nvoters_by_county\n\n# A tibble: 25 × 10\n   County              DEM    REP   BAR   GRN   LIB   WCP   OTH    UNA  TOTAL\n   <chr>             <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>\n 1 Allegany          12820  22530    20    74   204    19   395   7674  43736\n 2 Anne Arundel     174494 135457   105   564  1922   197  2715  90162 405616\n 3 Baltimore City   311610  30163   130   802   951   290  3289  52450 399685\n 4 Baltimore County 313870 142534   134   898  2227   297  5872 100576 566408\n 5 Calvert           24587  28181    20    89   332    30   567  14178  67984\n 6 Caroline           6629  10039     2    33    86    17   163   4208  21177\n 7 Carroll           33662  63967    27   155   670    38  1072  25770 125361\n 8 Cecil             21601  30880     8   103   341    49   727  15110  68819\n 9 Charles           72416  24711    19   112   349    83   763  19849 118302\n10 Dorchester         9848   8730     4    19    78    12   148   3348  22187\n# … with 15 more rows\n\n\nBoom - we’re good to go."
  },
  {
    "objectID": "pdfs.html#when-it-looks-good-but-needs-a-little-fixing",
    "href": "pdfs.html#when-it-looks-good-but-needs-a-little-fixing",
    "title": "22  Cleaning Data Part IV: PDFs",
    "section": "22.2 When it looks good, but needs a little fixing",
    "text": "22.2 When it looks good, but needs a little fixing\nHere’s a slightly more involved PDF. Here’s a PDF of 2020 general election results from Fayette County, Pa..\n\n\n\n\n\nLooks like a spreadsheet, right? Save that PDF file to your computer in a place where you’ll remember it (like a Downloads folder).\nNow let’s repeat the steps we did to import the PDF into Tabula and autodetect the tables. It should look like this:\n\n\n\n\n\nThis is pretty good, but we don’t want the “Times Cast” boxes at the top of each contest, because they have a slightly different layout. Go through the pages (there are 9 of them) and click the “x” to the right of those to clear them.\nNow you can hit the green “Preview & Export Extracted Data” button on the top right. Using the “Lattice” method, you should see something very like this:\n\n\n\n\n\nYou can now export that extracted table to a CSV file using the “Export” button. And then we can read it into R and clean up the column names:\n\nfayette_2020 <- read_csv(\"data/tabula-Fayette PA 2020 General Summary.csv\") %>% clean_names()\n\nNew names:\n• `` -> `...8`\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 284 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Candidate, Party, Election Day, Absentee, Mail-In, Provisional, Total\nlgl (1): ...8\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfayette_2020\n\n# A tibble: 284 × 8\n   candidate                   party elect…¹ absen…² mail_in provi…³ total x8   \n   <chr>                       <chr> <chr>   <chr>   <chr>   <chr>   <chr> <lgl>\n 1 \"Joseph R. Biden V.P. Kama… DEM   9,337   1,364   9,181   574     20,4… NA   \n 2 \"Donald J. Trump V.P. Mich… REP   34,965  1,224   4,056   998     41,2… NA   \n 3 \"Jo Jorgensen V.P Jeremy\\r… LIB   328     34      94      12      468   NA   \n 4 \"Write-in\"                  <NA>  51      4       32      4       91    NA   \n 5  <NA>                       <NA>  Electi… Absent… Mail-In Provis… Total NA   \n 6 \"Brian Carroll\"             WRIT… 0       0       0       0       0     NA   \n 7 \"Andrew Tang\"               WRIT… 1       0       0       0       1     NA   \n 8 \"Kayne West\"                WRIT… 1       0       0       0       1     NA   \n 9 \"Benjamin Sota\"             WRIT… 1       0       0       0       1     NA   \n10 \"Patrick Roy\"               WRIT… 1       0       0       0       1     NA   \n# … with 274 more rows, and abbreviated variable names ¹​election_day,\n#   ²​absentee, ³​provisional"
  },
  {
    "objectID": "pdfs.html#cleaning-up-the-data-in-r",
    "href": "pdfs.html#cleaning-up-the-data-in-r",
    "title": "22  Cleaning Data Part IV: PDFs",
    "section": "22.3 Cleaning up the data in R",
    "text": "22.3 Cleaning up the data in R\nThe good news is that we have data we don’t have to retype. The bad news is, it’s hardly in importable shape. We have a few things to fix. All the columns with the number of votes in them have commas, which causes R to think they are  columns, not numbers. Let’s fix that by re-importing it and calling mutate so that those columns are numeric.\n\nfayette_2020 <- read_csv(\"data/tabula-Fayette PA 2020 General Summary.csv\") %>% clean_names()\n\nNew names:\n• `` -> `...8`\n\n\nWarning: One or more parsing issues, see `problems()` for details\n\n\nRows: 284 Columns: 8\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (7): Candidate, Party, Election Day, Absentee, Mail-In, Provisional, Total\nlgl (1): ...8\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nfayette_2020 <- fayette_2020 %>% mutate(election_day=as.numeric(parse_number(election_day)), absentee=as.numeric(parse_number(absentee)), mail_in=as.numeric(parse_number(mail_in)), provisional=as.numeric(parse_number(provisional)),total=as.numeric(parse_number(total)))\n\nWarning: 20 parsing failures.\nrow col expected       actual\n  5  -- a number Election Day\n 25  -- a number Election Day\n 73  -- a number Election Day\n 74  -- a number Election Day\n 80  -- a number Election Day\n... ... ........ ............\nSee problems(...) for more details.\n\n\nWarning: 20 parsing failures.\nrow col expected   actual\n  5  -- a number Absentee\n 25  -- a number Absentee\n 73  -- a number Absentee\n 74  -- a number Absentee\n 80  -- a number Absentee\n... ... ........ ........\nSee problems(...) for more details.\n\n\nWarning: 20 parsing failures.\nrow col expected  actual\n  5  -- a number Mail-In\n 25  -- a number Mail-In\n 73  -- a number Mail-In\n 74  -- a number Mail-In\n 80  -- a number Mail-In\n... ... ........ .......\nSee problems(...) for more details.\n\n\nWarning: 20 parsing failures.\nrow col expected      actual\n  5  -- a number Provisional\n 25  -- a number Provisional\n 73  -- a number Provisional\n 74  -- a number Provisional\n 80  -- a number Provisional\n... ... ........ ...........\nSee problems(...) for more details.\n\n\nWarning: 20 parsing failures.\nrow col expected actual\n  5  -- a number  Total\n 25  -- a number  Total\n 73  -- a number  Total\n 74  -- a number  Total\n 80  -- a number  Total\n... ... ........ ......\nSee problems(...) for more details.\n\nfayette_2020\n\n# A tibble: 284 × 8\n   candidate                   party elect…¹ absen…² mail_in provi…³ total x8   \n   <chr>                       <chr>   <dbl>   <dbl>   <dbl>   <dbl> <dbl> <lgl>\n 1 \"Joseph R. Biden V.P. Kama… DEM      9337    1364    9181     574 20456 NA   \n 2 \"Donald J. Trump V.P. Mich… REP     34965    1224    4056     998 41243 NA   \n 3 \"Jo Jorgensen V.P Jeremy\\r… LIB       328      34      94      12   468 NA   \n 4 \"Write-in\"                  <NA>       51       4      32       4    91 NA   \n 5  <NA>                       <NA>       NA      NA      NA      NA    NA NA   \n 6 \"Brian Carroll\"             WRIT…       0       0       0       0     0 NA   \n 7 \"Andrew Tang\"               WRIT…       1       0       0       0     1 NA   \n 8 \"Kayne West\"                WRIT…       1       0       0       0     1 NA   \n 9 \"Benjamin Sota\"             WRIT…       1       0       0       0     1 NA   \n10 \"Patrick Roy\"               WRIT…       1       0       0       0     1 NA   \n# … with 274 more rows, and abbreviated variable names ¹​election_day,\n#   ²​absentee, ³​provisional\n\n\nOk, now we have numbers. Next we’ll get rid of the rows where candidate is NA or the value of candidate is literally “Candidate” and also drop the x8 blank column. To do the former, we’ll use the inverse of the is.na function by placing an exclamation point before it (you can read that filter as “where candidate is NOT NA”) and do the same to exclude “Candidate” from the matching values. For the latter, we’ll use select and the minus sign to drop that column\n\nfayette_2020 <- fayette_2020 %>% filter(!is.na(candidate)) %>% filter(candidate != \"Candidate\") %>% select(-x8)\n\nfayette_2020\n\n# A tibble: 264 × 7\n   candidate                         party elect…¹ absen…² mail_in provi…³ total\n   <chr>                             <chr>   <dbl>   <dbl>   <dbl>   <dbl> <dbl>\n 1 \"Joseph R. Biden V.P. Kamala\\rD.… DEM      9337    1364    9181     574 20456\n 2 \"Donald J. Trump V.P. Michael\\rR… REP     34965    1224    4056     998 41243\n 3 \"Jo Jorgensen V.P Jeremy\\rSpike … LIB       328      34      94      12   468\n 4 \"Write-in\"                        <NA>       51       4      32       4    91\n 5 \"Brian Carroll\"                   WRIT…       0       0       0       0     0\n 6 \"Andrew Tang\"                     WRIT…       1       0       0       0     1\n 7 \"Kayne West\"                      WRIT…       1       0       0       0     1\n 8 \"Benjamin Sota\"                   WRIT…       1       0       0       0     1\n 9 \"Patrick Roy\"                     WRIT…       1       0       0       0     1\n10 \"R19 Boddie\"                      WRIT…       0       0       0       0     0\n# … with 254 more rows, and abbreviated variable names ¹​election_day,\n#   ²​absentee, ³​provisional\n\n\nThis still isn’t perfect - we don’t have the offices these folks are running for - but all things considered, that was pretty easy. Many - most? - electronic PDFs aren’t so easy to parse. Sometimes you’ll need to open the exported CSV file and clean things up before importing into R. Other times you’ll be able to do that cleaning in R itself.\nHere’s the sad truth: THIS IS PRETTY GOOD. It sure beats typing it out. And since many government processes don’t change all that much, you can save the code to process subsequent versions of PDFs."
  },
  {
    "objectID": "merging.html",
    "href": "merging.html",
    "title": "23  Combining and joining",
    "section": "",
    "text": "Often, as data journalists, we’re looking at data across time or at data stored in multiple tables. And to do that, we need to often need to merge that data together.\nDepending on what we have, we may just need to stack data on top of each other to make new data. If we have 2019 data and 2018 data and we want that to be one file, we stack them. If we have a dataset of cows in counties and a dataset of populations in county, we’re going to join those two together on the county – the common element.\nLet’s explore."
  },
  {
    "objectID": "merging.html#combining-data-stacking",
    "href": "merging.html#combining-data-stacking",
    "title": "23  Combining and joining",
    "section": "23.1 Combining data (stacking)",
    "text": "23.1 Combining data (stacking)\nLet’s say that we have Maryland county voter registration data from three different elections in three different files. They have the same record layout and the same number of counties (plus Baltimore City). We can combine them into a single dataframe.\nLet’s do what we need to import them properly. I’ve merged it all into one step for each of the three datasets.\n\nlibrary(tidyverse)\n\n\ncounty_voters_2016 <- read_csv(\"data/county_voters_2016.csv\")\n\nRows: 25 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, Total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ncounty_voters_2018 <- read_csv(\"data/county_voters_2018.csv\")\n\nRows: 25 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, Total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\ncounty_voters_2020 <- read_csv(\"data/county_voters_2020.csv\")\n\nRows: 25 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (8): YEAR, DEM, REP, GRN, LIB, OTH, UNA, Total\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAll three of these datasets have the same number of columns, all with the same names, so if we want to merge them together to compare them over time, we need to stack them together. The verb here, in R, is bind_rows. You tell the function what you want to combine and it does it, assuming that you’ve got column names in common containing identically formatted data.\nSince we have three dataframes, we’re going to need to pass them as a list, meaning they’ll be enclosed inside the list function.\n\ncounty_voters_combined <- bind_rows(list(county_voters_2016, county_voters_2018, county_voters_2020))\n\nAnd boom, like that, we have 75 rows of data together instead of three dataframes. There are plenty of uses for bind_rows: any regularly updated data that comes in the same format like crime reports or award recipients or player game statistics. Or election results."
  },
  {
    "objectID": "merging.html#joining-data",
    "href": "merging.html#joining-data",
    "title": "23  Combining and joining",
    "section": "23.2 Joining data",
    "text": "23.2 Joining data\nMore complicated is when you have two separate tables that are connected by a common element or elements. But there’s a verb for that, too: join.\nLet’s start by reading in some Maryland 2020 county population data:\n\nmaryland_population <- read_csv('data/maryland_population_2020.csv')\n\nRows: 24 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): COUNTY\ndbl (1): POP2020\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of the columns we have is called county, which is what we have in our county_voters_2020 dataframe.\nTo put the Maryland population data and voter registration data together, we need to use something called a join. There are different kinds of joins. It’s better if you think of two tables sitting next to each other. A left_join takes all the records from the left table and only the records that match in the right one. A right_join does the same thing. An inner_join takes only the records where they are equal. There’s one other join – a full_join which returns all rows of both, regardless of if there’s a match – but I’ve never once had a use for a full join.\nIn the best-case scenario, the two tables we want to join share a common column. In this case, both of our tables have a column called county that has the same characteristics: values in both look identical, including how they distinguish Baltimore City from Baltimore County. This is important, because joins work on exact matches.\nWe can do this join multiple ways and get a similar result. We can put the population file on the left and the registration data on the right and use a left join to get them all together. And we use by= to join by the correct column. I’m going to count the rows at the end. The reason I’m doing this is important: Rule 1 in joining data is having an idea of what you are expecting to get. So with a left join with population on the left, I have 24 rows, so I expect to get 24 rows when I’m done.\n\nmaryland_population %>% left_join(county_voters_2020, by=\"COUNTY\") %>% nrow()\n\n[1] 24\n\n\nRemove the nrow and run it again for yourself. By default, dplyr will do a “natural” join, where it’ll match all the matching columns in both tables. So if we take out the by, it’ll use all the common columns between the tables. That may not be right in every instance but let’s try it. If it works, we should get 24 rows.\n\nmaryland_population %>% left_join(county_voters_2020)\n\nJoining, by = \"COUNTY\"\n\n\n# A tibble: 24 × 10\n   COUNTY           POP2020  YEAR    DEM    REP   GRN   LIB   OTH    UNA  Total\n   <chr>              <dbl> <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>\n 1 Allegany           68106  2020  12820  22530    74   204   434   7674  43736\n 2 Anne Arundel      588261  2020 174494 135457   564  1922  3017  90162 405616\n 3 Baltimore City    585708  2020 311610  30163   802   951  3709  52450 399685\n 4 Baltimore County  854535  2020 313870 142534   898  2227  6303 100576 566408\n 5 Calvert            92783  2020  24587  28181    89   332   617  14178  67984\n 6 Caroline           33293  2020   6629  10039    33    86   182   4208  21177\n 7 Carroll           172891  2020  33662  63967   155   670  1137  25770 125361\n 8 Cecil             103725  2020  21601  30880   103   341   784  15110  68819\n 9 Charles           166617  2020  72416  24711   112   349   865  19849 118302\n10 Dorchester         32531  2020   9848   8730    19    78   164   3348  22187\n# … with 14 more rows\n\n\nSince we only have one column in common between the two tables, the join only used that column. And we got the same answer. If we had more columns in common, you could see in your results columns with .X after them - that’s a sign of duplicative columns between two tables, and you may decide you don’t need both moving forward.\nLet’s save our joined data to a new dataframe, but this time let’s remove the select function so we don’t limit the columns to just three.\n\nmaryland_population_with_voters <- maryland_population %>% left_join(county_voters_2020)\n\nJoining, by = \"COUNTY\"\n\n\nNow, with our joined data, we can answer questions in a more useful way. But joins can do even more than just bring data together; they can include additional data to enable you to ask more sophisticated questions. Right now we have registered voters and total population. But we can do more.\nLet’s try adding more Maryland demographic data to the mix. Using a file describing the 18-and-over population (from which eligible voters come) from the state’s data catalog, we can read it into R:\n\nmaryland_demographics <- read_csv('data/maryland_demographics.csv')\n\nRows: 24 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): NAME\ndbl (10): GEOCODE, pop_18_over, pop_one_race, pop_white, pop_black, pop_nati...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nAgain, we can use a left_join to make our demographic data available. This time we’ll need to specify the two fields to join because they do not have identical names. We’ll use COUNTY from our population data and NAME from the demographic data, and the order matters - the first column is from the dataframe you name first.\n\nmaryland_population_with_voters_and_demographics <- maryland_population_with_voters %>% left_join(maryland_demographics, by=c(\"COUNTY\"=\"NAME\"))\n\nNow we’ve got population data and demographic data by county. That means we can draw from both datasets in asking our questions. For example, we could see the counties with the highest 18+ Black population as a percentage of all population 18 and over and also the percentage of Democrats in that county.\nWe can get this by using mutate and arrange:\n\nmaryland_population_with_voters_and_demographics %>%\n  mutate(pct_black_18_plus = (pop_black/pop_18_over)*100, pct_dems = (DEM/Total)*100) %>%\n  arrange(desc(pct_black_18_plus)) %>%\n  select(COUNTY, pct_black_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_black_18_plus pct_dems\n   <chr>                        <dbl>    <dbl>\n 1 Prince George's               60.9     78.3\n 2 Baltimore City                56.3     78.0\n 3 Charles                       48.2     61.2\n 4 Somerset                      39.0     41.8\n 5 Baltimore County              28.8     55.4\n 6 Dorchester                    26.2     44.4\n 7 Wicomico                      25.6     42.3\n 8 Howard                        18.7     52.4\n 9 Montgomery                    18.1     61.0\n10 Anne Arundel                  17.4     43.0\n# … with 14 more rows\n\n\nIf you know Maryland political demographics, this result isn’t too surprising, but Somerset County - the state’s 2nd smallest in terms of population - stands out for its Black population, which is a greater percentage than Baltimore County and Montgomery County.\nLet’s change that to look at Asian population:\n\nmaryland_population_with_voters_and_demographics %>%\n  mutate(pct_asian_18_plus = (pop_asian/pop_18_over)*100, pct_dems = (DEM/Total)*100) %>%\n  arrange(desc(pct_asian_18_plus)) %>%\n  select(COUNTY, pct_asian_18_plus, pct_dems)\n\n# A tibble: 24 × 3\n   COUNTY           pct_asian_18_plus pct_dems\n   <chr>                        <dbl>    <dbl>\n 1 Howard                       19.4      52.4\n 2 Montgomery                   16.0      61.0\n 3 Baltimore County              6.34     55.4\n 4 Frederick                     4.88     38.9\n 5 Prince George's               4.68     78.3\n 6 Anne Arundel                  4.52     43.0\n 7 Baltimore City                4.17     78.0\n 8 Charles                       3.55     61.2\n 9 Harford                       3.15     35.4\n10 St. Mary's                    3.13     35.7\n# … with 14 more rows\n\n\nHere, Howard and Montgomery County stand out in terms of the percentage of Asian population 18 and over. The jurisdictions with the highest percentage of Democrats - Prince George’s and Baltimore City - have small Asian populations.\nSometimes joins look like they should work but don’t. Often this is due to the two columns you’re joining on having different data types: joining a  column to a  column, for example. Let’s walk through an example of that using some demographic data by zip code.\n\nmaryland_zcta <- read_csv('data/maryland_zcta.csv')\n\nRows: 468 Columns: 40\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (5): FIRST_CLAS, FIRST_MTFC, FIRST_FUNC, REPORT_2_P, REPORT_9_P\ndbl (35): OBJECTID_1, ZCTA5CE10, FIRST_STAT, FIRST_GEOI, ZCTA5N, STATE, AREA...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(maryland_zcta)\n\nRows: 468\nColumns: 40\n$ OBJECTID_1 <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ ZCTA5CE10  <dbl> 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ FIRST_STAT <dbl> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ FIRST_GEOI <dbl> 2420601, 2420602, 2420603, 2420606, 2420607, 2420608, 24206…\n$ FIRST_CLAS <chr> \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\", \"B5\",…\n$ FIRST_MTFC <chr> \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G6350\", \"G635…\n$ FIRST_FUNC <chr> \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\", \"S\",…\n$ ZCTA5N     <dbl> 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206…\n$ STATE      <dbl> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,…\n$ AREALAND   <dbl> 115635266, 35830723, 44239637, 7501011, 54357590, 45583064,…\n$ AREAWATR   <dbl> 387684, 352762, 219356, 1248760, 448221, 5330329, 6602735, …\n$ POP100     <dbl> 24156, 24955, 28967, 431, 9802, 919, 1120, 1078, 261, 11860…\n$ HU100      <dbl> 8722, 9736, 10317, 230, 3504, 426, 554, 413, 142, 4424, 204…\n$ NHW        <dbl> 9785, 8466, 9625, 377, 2165, 438, 1009, 798, 245, 4044, 352…\n$ NHB        <dbl> 11146, 13054, 15025, 45, 6321, 453, 82, 215, 12, 6786, 32, …\n$ NHAI       <dbl> 155, 116, 98, 1, 33, 5, 2, 5, 0, 106, 2, 32, 3, 4, 38, 8, 1…\n$ NHA        <dbl> 880, 731, 1446, 4, 560, 2, 1, 10, 0, 186, 3, 165, 5, 1, 402…\n$ NHNH       <dbl> 11, 15, 24, 0, 3, 0, 1, 0, 0, 4, 1, 2, 0, 0, 4, 1, 0, 3, 1,…\n$ NHO        <dbl> 48, 58, 65, 0, 6, 0, 0, 0, 0, 8, 0, 1, 0, 3, 5, 8, 0, 5, 10…\n$ NHT        <dbl> 849, 999, 1091, 0, 234, 9, 15, 33, 1, 321, 13, 213, 14, 4, …\n$ HISP       <dbl> 1282, 1516, 1593, 4, 480, 12, 10, 17, 3, 405, 2, 244, 9, 7,…\n$ PNHW       <dbl> 40.5, 33.9, 33.2, 87.5, 22.1, 47.7, 90.1, 74.0, 93.9, 34.1,…\n$ PNHB       <dbl> 46.1, 52.3, 51.9, 10.4, 64.5, 49.3, 7.3, 19.9, 4.6, 57.2, 7…\n$ PNHAI      <dbl> 0.6, 0.5, 0.3, 0.2, 0.3, 0.5, 0.2, 0.5, 0.0, 0.9, 0.5, 0.5,…\n$ PNHA       <dbl> 3.6, 2.9, 5.0, 0.9, 5.7, 0.2, 0.1, 0.9, 0.0, 1.6, 0.7, 2.8,…\n$ PNHNH      <dbl> 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.2, 0.0,…\n$ PNHO       <dbl> 0.2, 0.2, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0,…\n$ PNHT       <dbl> 3.5, 4.0, 3.8, 0.0, 2.4, 1.0, 1.3, 3.1, 0.4, 2.7, 3.2, 3.6,…\n$ PHISP      <dbl> 5.3, 6.1, 5.5, 0.9, 4.9, 1.3, 0.9, 1.6, 1.1, 3.4, 0.5, 4.2,…\n$ POP65_     <dbl> 1922, 1964, 1400, 108, 847, 173, 271, 129, 54, 1372, 73, 55…\n$ PCTPOP65_  <dbl> 8.0, 7.9, 4.8, 25.1, 8.6, 18.8, 24.2, 12.0, 20.7, 11.6, 18.…\n$ MEDAGE     <dbl> 37.3, 32.6, 34.5, 49.1, 40.9, 46.6, 47.6, 44.3, 47.3, 40.8,…\n$ VACNS      <dbl> 376, 769, 531, 15, 172, 39, 32, 22, 14, 249, 18, 158, 8, 18…\n$ PVACNS     <dbl> 4.3, 7.9, 5.1, 6.5, 4.9, 9.2, 5.8, 5.3, 9.9, 5.6, 8.8, 7.2,…\n$ PHOWN      <dbl> 71.1, 59.7, 73.8, 49.7, 83.1, 60.4, 44.8, 63.8, 38.3, 73.9,…\n$ PWOMORT    <dbl> 11.2, 9.0, 4.7, 39.3, 10.3, 28.2, 38.7, 21.8, 43.9, 17.4, 2…\n$ PRENT      <dbl> 19.9, 34.4, 22.6, 18.1, 7.4, 15.9, 27.0, 18.3, 31.7, 10.5, …\n$ PLT18SP    <dbl> 30.4, 43.6, 29.9, 31.2, 22.1, 14.1, 28.9, 24.5, 43.9, 26.7,…\n$ REPORT_2_P <chr> \"http://mdpgis.mdp.state.md.us/Census2010/PDF/00_SF1DP_2Pro…\n$ REPORT_9_P <chr> \"http://mdpgis.mdp.state.md.us/census2010/PDF/00_SF1_9PROFI…\n\n\nYou can see that ZCTA5N, the column representing the Zip Code Tabulation Area, is a numeric column. But should it be? Do we ever want to know the average zip code in Maryland? Zip codes and ZCTAs look like numbers but really are character columns. Let’s change that so that we can be sure to join them correctly with other data where the zip codes are not numbers. We’ll use mutate:\n\nmaryland_zcta <- maryland_zcta %>% mutate(across(ZCTA5N, as.character))\n\nWhat’s happening here is that we’re telling R to take all of the values in the ZCTA5N column and make them “as.character”. If we wanted to change a column to numeric, we’d do “as.numeric”. When you join two dataframes, the join columns must be the same datatype.\nJoining datasets allows you to expand the range and sophistication of questions you’re able to ask. It is one of the most powerful tools in a journalist’s toolkit."
  },
  {
    "objectID": "rvest.html",
    "href": "rvest.html",
    "title": "24  Scraping data with Rvest",
    "section": "",
    "text": "Why? Because they have a website. That’s it. That’s their reason. They say they don’t have to give you the data because they’ve already given you the data, never mind that they haven’t given to you in a form you can actually load into R with ease.\nLucky for us, there’s a way for us to write code to get data even when an agency hasn’t made it easy: webscraping.\nOne of the most powerful tools you can learn as a data journalist is how to scrape data from the web. Scraping is the process of programming a computer to act like a human that opens a web browser, goes to a website, ingests the HTML from that website into R and turns it into data.\nThe degree of difficulty here goes from “Easy” to “So Hard You Want To Throw Your Laptop Out A Window.” And the curve between the two can be steep. You can learn how to scrape “Easy” in a day. The hard ones take a little more time, but it’s often well worth the effort because it lets you get stories you couldn’t get without it.\nIn this chapter, we’ll show you an easy one. And in the next chapter, we’ll so you a moderately harder one.\nLet’s start easy.\nWe’re going to use a library called rvest, which you can install it the same way we’ve done all installs: go to the console and install.packages(\"rvest\").\nLike so many R package names, rvest is a bad pun. You’re supposed to read it to sound like “harvest”, as in “harvesting” information from a website the same way you’d harvest crops in a field.\nWe’ll load these packages first:\n\nlibrary(rvest)\nlibrary(tidyverse)\nlibrary(janitor)\n\nFor this example, we’re going to work on loading a simple table of data from the Maryland State Board of Elections. This is a table of unofficial election results from the July Republican gubernatorial primary election.\nLet’s suppose we can’t find a table like that for download, but we do see a version on the SBOE website at this URL: https://elections.maryland.gov/elections/2022/primary_results/gen_detail_results_2022_1_1_Republican.html.\n\n\n\n\n\nWe could get this table into R with the following manual steps: highlighting the text, copying it into Excel, saving it as a csv, and reading it into R. Or, we could write a few lines of webscraping code to have R do that for us!\nIn this simple example, it’s probably faster to do it manually than have R do it for us. And this table is unlikely to change much in the future.\nWhy would we ever write code to grab a single table? There’s several reasons:\n\nOur methods are transparent. If a colleague wants to run our code from scratch to factcheck our work, they don’t need to repeat the manual steps, which are harder to document than writing code.\nLet’s suppose we wanted to grab the same table every day, to monitor for changes (like, say, a table on a health department website that has COVID case numbers that update every day). Writing a script once, and pressing a single button every day is going to be much more efficient than doing this manually every day.\nIf we’re doing it manually, we’re more likely to make a mistake, like maybe failing to copy every row from the whole table.\nIt’s good practice to prepare us to do more complex scraping jobs. As we’ll see in the next chapter, if we ever want to grab the same table from hundreds of pages, writing code is much faster and easier than going to a hundred different pages ourselves and downloading data.\n\nSo, to scrape, the first thing we need to do is start with the URL. Let’s store it as an object called naics_url.\n\nsboe_url <- \"https://elections.maryland.gov/elections/2022/primary_results/gen_detail_results_2022_1_1_Republican.html\"\n\nWhen we go to the web page, we can see a nicely-designed page that contains our information.\nBut what we really care about, for our purposes, is the html code that creates that page.\nIn our web browser, if we right-click anywhere on the page and select “view source” from the popup menu, we can see the source code. Or you can just copy this into Google Chrome: view-source:https://elections.maryland.gov/elections/2022/primary_results/gen_detail_results_2022_1_1_Republican.html.\nHere’s a picture of what some of the source code looks like.\n\n\n\n\n\nWe’ll use those HTML tags – things like <table> and <tr> – to grab the info we need.\nOkay, step 1.\nLet’s write a bit of code to tell R to go to the URL for the page and ingest all of that HTML code. In the code below, we’re starting with our URL and using the read_html() function from rvest to ingest all of the page html, storing it as an object called naics_industry.\n\n# read in the html\nresults <- sboe_url %>%\n  read_html()\n\n# display the html below\nresults\n\n{html_document}\n<html lang=\"en\">\n[1] <head>\\n<meta http-equiv=\"Content-Type\" content=\"text/html; charset=UTF-8 ...\n[2] <body>\\n      <div class=\"container\">\\n      <div class=\"skipNav\">\\n      ...\n\n\nIf you’re running this code in R Studio, in our environment window at right, you’ll see results as a “list of 2”.\nThis is not a dataframe, it’s a different type of data structure a “nested list.”\nIf we click on the name “results” in our environment window, we can see that it’s pulled in the html and shown us the general page structure. Nested within the <html> tag is the <head> and <body>, the two fundamental sections of most web pages. We’re going to pull information out of the <body> tag in a bit.\n\n\n\n\n\nNow, our task is to just pull out the section of the html that contains the information we need.\nBut which part do we need from that mess of html code? To figure that out, we can go back to the page in a web browser like chrome, and use built in developer tools to “inspect” the html code underlying the page.\nOn the page, find the data we want to grab – “Table 2. NAICS Sectors” - and right click on the word “Sector” in the column header of the table. That will bring up a dropdown menu. Select “Inspect”, which will pop up a window called the “element inspector” that shows us where different elements on the page are located, what html tags created those elements, and other info.\n\n\n\n\n\nThe entire table that we want of results is actually contained inside an html <table>. It has a <tbody> that contains one row <tr> per county.\nBecause it’s inside of a table, and not some other kind of element (like a <div>), rvest has a special function for easily extracting and converting html tables, called html_table(). This function extracts all the html tables on the page, but this page only has one so we’re good.\n\n# read in the html and extract all the tables\nresults <- sboe_url %>%\n  read_html() %>%\n  html_table()\n\n# show the dataframe\n\nresults\n\n[[1]]\n# A tibble: 25 × 5\n   Jurisdiction     Dan Cox and Gordana Schifanelli\\r\\…¹ Robin…² Kelly…³ Joe W…⁴\n   <chr>            <chr>                                <chr>   <chr>   <chr>  \n 1 Allegany         4,469                                399     2,259   99     \n 2 Anne Arundel     18,039                               711     18,581  513    \n 3 Baltimore City   1,771                                168     2,010   156    \n 4 Baltimore County 18,129                               842     18,247  649    \n 5 Calvert          6,081                                335     3,628   186    \n 6 Caroline         2,533                                49      1,409   39     \n 7 Carroll          10,462                               404     9,997   268    \n 8 Cecil            4,998                                203     3,487   128    \n 9 Charles          3,473                                176     1,709   127    \n10 Dorchester       2,475                                75      916     42     \n# … with 15 more rows, and abbreviated variable names\n#   ¹​`Dan Cox and Gordana Schifanelli\\r\\n\\r\\n\\r\\nRepublican`,\n#   ²​`Robin Ficker and LeRoy F. Yegge, Jr.\\r\\n\\r\\n\\r\\nRepublican`,\n#   ³​`Kelly Schulz and Jeff Woolford\\r\\n\\r\\n\\r\\nRepublican`,\n#   ⁴​`Joe Werner and Minh Thanh Luong\\r\\n\\r\\n\\r\\nRepublican`\n\n\nIn the environment window at right, look at results Note that it’s now a “list of 1”.\nThis gets a little complicated, but what you’re seeing here is a nested list that contains one data frame – also called tibbles – one for each table that exists on the web page we scraped.\nSo, all we need to do now is to store that single dataframe as an object. We can do that with this code, which says “keep only the second dataframe from our nested list.\n\n# Read in all html from table, store all tables on page as nested list of dataframes.\nresults <- sboe_url %>%\n  read_html() %>%\n  html_table()\n\n# Just keep the first dataframe in our list\n\nresults <- results[[1]]\n\n# show the dataframe\n\nresults\n\n# A tibble: 25 × 5\n   Jurisdiction     Dan Cox and Gordana Schifanelli\\r\\…¹ Robin…² Kelly…³ Joe W…⁴\n   <chr>            <chr>                                <chr>   <chr>   <chr>  \n 1 Allegany         4,469                                399     2,259   99     \n 2 Anne Arundel     18,039                               711     18,581  513    \n 3 Baltimore City   1,771                                168     2,010   156    \n 4 Baltimore County 18,129                               842     18,247  649    \n 5 Calvert          6,081                                335     3,628   186    \n 6 Caroline         2,533                                49      1,409   39     \n 7 Carroll          10,462                               404     9,997   268    \n 8 Cecil            4,998                                203     3,487   128    \n 9 Charles          3,473                                176     1,709   127    \n10 Dorchester       2,475                                75      916     42     \n# … with 15 more rows, and abbreviated variable names\n#   ¹​`Dan Cox and Gordana Schifanelli\\r\\n\\r\\n\\r\\nRepublican`,\n#   ²​`Robin Ficker and LeRoy F. Yegge, Jr.\\r\\n\\r\\n\\r\\nRepublican`,\n#   ³​`Kelly Schulz and Jeff Woolford\\r\\n\\r\\n\\r\\nRepublican`,\n#   ⁴​`Joe Werner and Minh Thanh Luong\\r\\n\\r\\n\\r\\nRepublican`\n\n\nWe now have a proper dataframe, albeit with some lengthy column headers.\nFrom here, we can do a little cleaning. First we’ll use clean_names() to lower the column names. Then use rename() to replace the candidate column names with simpler versions. We can just use the column positions instead of writing out the full names, which is nice.\nThen let’s use slice() to remove the last row – row number 25 – which contains totals and percentages that we don’t need. Finally, we’ll make sure the vote tallies are numbers using mutate and gsub(), which we use to replace all the commas with nothing.\n\n# Read in all html from table, get the HTML table.\nresults <- sboe_url %>%\n  read_html() %>%\n  html_table()\n\n# Standardize column headers, remove last row\n\nresults <- results[[1]] %>%\n  clean_names() %>%\n  rename(cox = 2, ficker = 3, schulz = 4, werner = 5) %>%\n  slice(-25) %>%\n  mutate(cox = as.numeric(gsub(\",\",\"\", cox))) %>%\n  mutate(ficker = as.numeric(gsub(\",\",\"\", ficker))) %>%\n  mutate(schulz = as.numeric(gsub(\",\",\"\", schulz))) %>%\n  mutate(werner = as.numeric(gsub(\",\",\"\", werner)))\n\n\n# show the dataframe\nresults\n\n# A tibble: 24 × 5\n   jurisdiction       cox ficker schulz werner\n   <chr>            <dbl>  <dbl>  <dbl>  <dbl>\n 1 Allegany          4469    399   2259     99\n 2 Anne Arundel     18039    711  18581    513\n 3 Baltimore City    1771    168   2010    156\n 4 Baltimore County 18129    842  18247    649\n 5 Calvert           6081    335   3628    186\n 6 Caroline          2533     49   1409     39\n 7 Carroll          10462    404   9997    268\n 8 Cecil             4998    203   3487    128\n 9 Charles           3473    176   1709    127\n10 Dorchester        2475     75    916     42\n# … with 14 more rows\n\n\nAnd there we go. We now have a nice tidy dataframe of Maryland GOP gubernatorial primary results that we could ask some questions of."
  },
  {
    "objectID": "census.html",
    "href": "census.html",
    "title": "25  Intro to APIs: The Census",
    "section": "",
    "text": "There is truly an astonishing amount of data collected by the US Census Bureau. First, there’s the Census that most people know – the every 10 year census. That’s the one mandated by the Constitution where the government attempts to count every person in the US. It’s a mind-boggling feat to even try, and billions get spent on it. That data is used first for determining how many representatives each state gets in Congress. From there, the Census gets used to divide up billions of dollars of federal spending.\nTo answer the questions the government needs to do that, a ton of data gets collected. That, unfortunately, means the Census is exceedingly complicated to work with. The good news is, the Census has an API – an application programming interface. What that means is we can get data directly through the Census Bureau via calls over the internet.\nLet’s demonstrate.\nWe’re going to use a library called tidycensus which makes calls to the Census API in a very tidy way, and gives you back tidy data. That means we don’t have to go through the process of importing the data from a file. I can’t tell you how amazing this is, speaking from experience. The documentation for this library is here. Another R library for working with Census APIs (there is more than one) is this one from Hannah Recht, a journalist with Kaiser Health News.\nFirst we need to install tidycensus using the console: install.packages(\"tidycensus\", dependencies = TRUE). You also should install the sf and rgdal packages.\nTo use the API, you need an API key. To get that, you need to apply for an API key with the Census Bureau. It takes a few minutes and you need to activate your key via email. Once you have your key, you need to set that for this session. Just FYI: Your key is your key. Do not share it around.\nThe two main functions in tidycensus are get_decennial, which retrieves data from the 2000 and 2010 Censuses (and soon the 2020 Census), and get_acs, which pulls data from the American Community Survey, a between-Censuses annual survey that provides estimates, not hard counts, but asks more detailed questions. If you’re new to Census data, there’s a very good set of slides from Kyle Walker, the creator of tidycensus, and he’s working on a book that you can read for free online.\nIt’s important to keep in mind that Census data represents people - you, your neighbors and total strangers. It also requires some level of definitions, especially about race & ethnicity, that may or may not match how you define yourself or how others define themselves.\nSo to give you some idea of how complicated the data is, let’s pull up just one file from the decennial Census. We’ll use Summary File 1, or SF1. That has the major population and housing stuff.\nNote: There are thousands of variables in SF1. That’s not a typo. Open it in your environment by double clicking. As you scroll down, you’ll get an idea of what you’ve got to choose from.\nIf you think that’s crazy, try the SF3 file from 2000.\nYes. That’s more than 16,000 variables to choose from. I told you. Astonishing.\nSo let’s try to answer a question using the Census. What is the fastest growing state since 2000?\nTo answer this, we need to pull the total population by state in each of the decennial census. Here’s 2000.\nNow 2010.\nLet’s take a peek at 2010.\nAs you can see, we have a GEOID, NAME, then variable and value. Variable and value are going to be the same. Because those are named the same thing, to merge them together, we need to rename them.\nNow we join the data together.\nAnd now we calculate the percent change.\nAnd just like that: Nevada.\nYou may be asking: hey, wasn’t there a 2020 Census? Where’s that data? The answer is that it’s coming, slowly - the Census Bureau has a schedule of releases."
  },
  {
    "objectID": "census.html#the-acs",
    "href": "census.html#the-acs",
    "title": "25  Intro to APIs: The Census",
    "section": "25.1 The ACS",
    "text": "25.1 The ACS\nIn 2010, the Census Bureau replaced SF3 with the American Community Survey. The Good News is that the data would be updated on a rolling basis. The bad news is that it’s more complicated because it’s more like survey data with a large sample. That means there’s margins of error and confidence intervals to worry about. By default, using get_acs fetches data from the 5-year estimates (currently 2016-2020), but you can specify 1-year estimates for jurisdictions with at least 65,000 people (many counties and cities).\nHere’s an example using the 5-year ACS estimates:\nWhat is Maryland’s richest county?\nWe can measure this by median household income. That variable is B19013_001, so we can get that data like this (I’m narrowing it to the top 20 for simplicity):\n\nmd <- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\nmd <- md %>% arrange(desc(estimate)) %>% top_n(20, estimate)\n\nmd\n\n# A tibble: 20 × 5\n   GEOID NAME                             variable  estimate   moe\n   <chr> <chr>                            <chr>        <dbl> <dbl>\n 1 24027 Howard County, Maryland          medincome   124042  3448\n 2 24009 Calvert County, Maryland         medincome   112696  3287\n 3 24031 Montgomery County, Maryland      medincome   111812  1361\n 4 24017 Charles County, Maryland         medincome   103678  1654\n 5 24003 Anne Arundel County, Maryland    medincome   103225  1817\n 6 24021 Frederick County, Maryland       medincome   100685  1927\n 7 24013 Carroll County, Maryland         medincome    99569  3051\n 8 24035 Queen Anne's County, Maryland    medincome    96467  4785\n 9 24037 St. Mary's County, Maryland      medincome    95864  3872\n10 24025 Harford County, Maryland         medincome    94003  2398\n11 24033 Prince George's County, Maryland medincome    86994   865\n12 24015 Cecil County, Maryland           medincome    79415  3479\n13 24005 Baltimore County, Maryland       medincome    78724  1641\n14 24041 Talbot County, Maryland          medincome    73102  4031\n15 24047 Worcester County, Maryland       medincome    65396  3856\n16 24043 Washington County, Maryland      medincome    63510  1930\n17 24045 Wicomico County, Maryland        medincome    60366  2437\n18 24029 Kent County, Maryland            medincome    60208  5185\n19 24011 Caroline County, Maryland        medincome    59042  4215\n20 24023 Garrett County, Maryland         medincome    54542  3487\n\n\nHoward, Calvert, Montgomery, Anne Arundel, Charles. What do they all have in common? Lots of suburban flight from DC and Baltimore. But do the margins of error let us say one county is richer than the other. We can find this out visually using error bars. Don’t worry much about the code here – we’ll cover that soon enough.\n\nmd %>%\n  mutate(NAME = gsub(\" County, Maryland\", \"\", NAME)) %>%\n  ggplot(aes(x = estimate, y = reorder(NAME, estimate))) +\n  geom_errorbarh(aes(xmin = estimate - moe, xmax = estimate + moe)) +\n  geom_point(color = \"red\") +\n  labs(title = \"Household income by county in Maryland\",\n       subtitle = \"2016-2020 American Community Survey\",\n       y = \"\",\n       x = \"ACS estimate (bars represent margin of error)\")\n\n\n\n\nAs you can see, some of the error bars are quite wide. Some are narrow. But if the bars overlap, it means the difference between the two counties is within the margin of error, and the differences aren’t statistically significant. So is the difference between Calvert and Montgomery significant? Nope. Is the difference between Howard and everyone else significant? Yes it is.\nLet’s ask another question of the ACS – did any counties lose income from the time of the global financial crisis to the current 5-year window?\nLet’s re-label our first household income data.\n\nmd20 <- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\n\nAnd now we grab the 2010 median household income.\n\nmd10 <- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2010)\n\nGetting data from the 2006-2010 5-year ACS\n\n\nWhat I’m going to do next is a lot, but each step is simple. I’m going to join the data together, so each county has one line of data. Then I’m going to rename some fields that repeat. Then I’m going to calculate the minimium and maximum value of the estimate using the margin of error. That’ll help me later. After that, I’m going to calculate a perent change and sort it by that change.\n\nmd10 %>%\n  inner_join(md20, by=c(\"GEOID\", \"NAME\")) %>%\n  rename(estimate2010=estimate.x, estimate2020=estimate.y) %>%\n  mutate(min2010 = estimate2010-moe.x, max2010 = estimate2010+moe.x, min2020 = estimate2020-moe.y, max2020 = estimate2020+moe.y) %>%\n  select(-variable.x, -variable.y, -moe.x, -moe.y) %>%\n  mutate(change = ((estimate2020-estimate2010)/estimate2010)*100) %>%\n  arrange(change)\n\n# A tibble: 24 × 9\n   GEOID NAME             estim…¹ estim…² min2010 max2010 min2020 max2020 change\n   <chr> <chr>              <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>\n 1 24011 Caroline County…   58799   59042   56740   60858   54827   63257  0.413\n 2 24039 Somerset County…   42443   44980   39092   45794   40432   49528  5.98 \n 3 24041 Talbot County, …   63017   73102   60081   65953   69071   77133 16.0  \n 4 24017 Charles County,…   88825  103678   87268   90382  102024  105332 16.7  \n 5 24019 Dorchester Coun…   45151   52799   43470   46832   49020   56578 16.9  \n 6 24047 Worcester Count…   55487   65396   52749   58225   61540   69252 17.9  \n 7 24045 Wicomico County…   50752   60366   49313   52191   57929   62803 18.9  \n 8 24035 Queen Anne's Co…   81096   96467   78068   84124   91682  101252 19.0  \n 9 24023 Garrett County,…   45760   54542   43729   47791   51055   58029 19.2  \n10 24031 Montgomery Coun…   93373  111812   92535   94211  110451  113173 19.7  \n# … with 14 more rows, and abbreviated variable names ¹​estimate2010,\n#   ²​estimate2020\n\n\nSo according to this, Somerset and Caroline counties had the smallest change between 2010 and 2020, while all other jurisdictions saw double-digit percentage increases.\nBut did they?\nLook at the min and max values for both. Is the change statistically significant?\nThe ACS data has lots of variables, just like the decennial Census does. To browse them, you can do this:\n\nv20 <- load_variables(2020, \"acs5\", cache=TRUE)\n\nAnd then view v20 to see what kinds of variables are available via the API."
  },
  {
    "objectID": "census.html#wide-results",
    "href": "census.html#wide-results",
    "title": "25  Intro to APIs: The Census",
    "section": "25.2 “Wide” Results",
    "text": "25.2 “Wide” Results\nAlthough one of the chief strengths of tidycensus is that it offers a, well, tidy display of Census data, it also has the ability to view multiple variables spread across columns. This can be useful for creating percentages and comparing multiple variables."
  },
  {
    "objectID": "census.html#sorting-results",
    "href": "census.html#sorting-results",
    "title": "25  Intro to APIs: The Census",
    "section": "25.3 Sorting Results",
    "text": "25.3 Sorting Results\nYou’ll notice that we’ve used arrange to sort the results of tidycensus functions, although that’s done after we create a new variable to hold the data. There’s another way to use arrange that you should know about, one that you can use for exploratory analysis. An example using median household income from 2020:\n\nmd20 <- get_acs(geography = \"county\",\n              variables = c(medincome = \"B19013_001\"),\n              state = \"MD\",\n              year = 2020)\n\nGetting data from the 2016-2020 5-year ACS\n\narrange(md20, desc(estimate))\n\n# A tibble: 24 × 5\n   GEOID NAME                          variable  estimate   moe\n   <chr> <chr>                         <chr>        <dbl> <dbl>\n 1 24027 Howard County, Maryland       medincome   124042  3448\n 2 24009 Calvert County, Maryland      medincome   112696  3287\n 3 24031 Montgomery County, Maryland   medincome   111812  1361\n 4 24017 Charles County, Maryland      medincome   103678  1654\n 5 24003 Anne Arundel County, Maryland medincome   103225  1817\n 6 24021 Frederick County, Maryland    medincome   100685  1927\n 7 24013 Carroll County, Maryland      medincome    99569  3051\n 8 24035 Queen Anne's County, Maryland medincome    96467  4785\n 9 24037 St. Mary's County, Maryland   medincome    95864  3872\n10 24025 Harford County, Maryland      medincome    94003  2398\n# … with 14 more rows\n\n\nIn this case we don’t save the sorted results to a variable, we can just see the output in the console."
  },
  {
    "objectID": "visualizing-for-reporting.html",
    "href": "visualizing-for-reporting.html",
    "title": "26  Visualizing your data for reporting",
    "section": "",
    "text": "Visualizing data is becoming a much greater part of journalism. Large news organizations are creating graphics desks that create complex visuals with data to inform the public about important events.\nTo do it well is a course on its own. And not every story needs a feat of programming and art. Sometimes, you can help yourself and your story by just creating a quick chart, which helps you see patterns in the data that wouldn’t otherwise surface.\nGood news: one of the best libraries for visualizing data is in the tidyverse and it’s pretty simple to make simple charts quickly with just a little bit of code. It’s called ggplot2.\nLet’s revisit some data we’ve used in the past and turn it into charts. First, let’s load libraries. When we load the tidyverse, we get ggplot2.\nThe dataset we’ll use is the voter registration data for Maryland. Let’s load it."
  },
  {
    "objectID": "visualizing-for-reporting.html#bar-charts",
    "href": "visualizing-for-reporting.html#bar-charts",
    "title": "26  Visualizing your data for reporting",
    "section": "26.1 Bar charts",
    "text": "26.1 Bar charts\nThe first kind of chart we’ll create is a simple bar chart.\nIt’s a chart designed to show differences between things – the magnitude of one thing, compared to the next thing, and the next, and the next.\nSo if we have thing, like a county, or a state, or a group name, and then a count of that group, we can make a bar chart.\nSo what does the chart of the top 10 maryland counties with the most total PPP loans look like?\nFirst, we’ll create a dataframe of those top 10, called maryland_ppp_top_counties.\n\nmaryland_ppp_top_counties <- ppp_maryland %>%\n  group_by(project_county_name) %>%\n  summarise(\n    total_loans = n()\n  ) %>%\n  arrange(desc(total_loans)) %>%\n  head(10)\n\nmaryland_ppp_top_counties\n\n# A tibble: 10 × 2\n   project_county_name total_loans\n   <chr>                     <int>\n 1 MONTGOMERY                38782\n 2 PRINCE GEORGES            34409\n 3 BALTIMORE                 28789\n 4 BALTIMORE CITY            20004\n 5 ANNE ARUNDEL              17336\n 6 HOWARD                    12011\n 7 FREDERICK                  6666\n 8 HARFORD                    6301\n 9 CHARLES                    4398\n10 CARROLL                    4040\n\n\nNow let’s create a bar chart using ggplot.\nWith ggplot, the first thing we’ll always do is draw a blank canvas that will house our chart. We start with our dataframe name, and then (%>%) we invoke the ggplot() function to make that blank canvas. All this does is make a gray box, the blank canvas that will hold our chart.\n\nmaryland_ppp_top_counties %>%\n  ggplot()\n\n\n\n\nNext we need to tell ggplot what kind of chart to make.\nIn ggplot, we work with two key concepts called geometries (abbreivated frequently as geom) and asthetics (abbreviated as aes).\nGeometries are the shape that the data will take; think of line charts, bar charts, scatterplots, histograms, pie charts and other common graphics forms.\nAsesthetics help ggplot know what component of our data to visualize – why we’ll visualize values from one column instead of another.\nIn a bar chart, we first pass in the data to the geometry, then set the aesthetic.\nIn the codeblock below, we’ve added a new function, geom_bar().\nUsing geom_bar() – as opposed to geom_line() – says we’re making a bar chart.\nInside of that function, the asthetic, aes, says which columns to use in drawing the chart.\nWe’re setting the values on the x axis (horizontal) to be the name of the county. We set weight to total loans, and it uses that value to “weight” or set the height of each bar.\nOne quirk here with ggplot.\nAfter we’ve invoked the ggplot() function, you’ll notice we’re using a + symbol. It means the same thing as %>% – “and then do this”. It’s just a quirk of ggplot() that after you invoke the ggplot() function, you use + instead of %>%. It makes no sense to me either, just something to live with.\n\nmaryland_ppp_top_counties %>%\n  ggplot() +\n  geom_bar(aes(x=project_county_name, weight=total_loans))\n\n\n\n\nThis is a very basic chart. But it’s hard to derive much meaning from this chart, because the counties aren’t ordered from highest to lowest by total_loans. We can fix that by using the reorder() function to do just that:\n\nmaryland_ppp_top_counties %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans))\n\n\n\n\nThis is a little more useful. But the bottom is kind of a mess, with overlapping names. We can fix that by flipping it from a vertical bar chart (also called a column chart) to a horizontal one. coord_flip() does that for you.\n\nmaryland_ppp_top_counties %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +\n  coord_flip()\n\n\n\n\nIs this art? No. Does it quickly tell you something meaningful? It does.\nWe’re mainly going to use these charts to help us in reporting, so style isn’t that important.\nBut it’s worth mentioning that we can pretty up these charts for publication, if we wanted to, with some more code. To style the chart, we can change or even modify the “theme”, a kind of skin that makes the chart look better.\nIt’s kind of like applying CSS to html. Here I’m changing the theme slightly to remove the gray background with one of ggplot’s built in themes, theme_minimal()\n\nmaryland_ppp_top_counties %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +\n  coord_flip() +\n  theme_minimal()\n\n\n\n\nThe ggplot universe is pretty big, and lots of people have made and released cool themes for you to use. Want to make your graphics look kind of like The Economist’s graphics? There’s a theme for that.\nFirst, you have to install and load a package that contains lots of extra themes, called ggthemes.\n\n#install.packages('ggthemes')\nlibrary(ggthemes)\n\nAnd now we’ll apply the economist theme from that package with theme_economist()\n\nmaryland_ppp_top_counties %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +\n  coord_flip() +\n  theme_economist()\n\n\n\n\nThose axis titles are kind of a mess. Let’s change “count” on the x axis to “total loans” and change “reorder(project_county_name,total_loans)” to “county”. And while we’re at it, let’s add a basic title and a source as a caption. We’ll use a new function, labs(), which is short for labels.\n\nmaryland_ppp_top_counties %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(project_county_name,total_loans), weight=total_loans)) +\n  coord_flip() +\n  theme_economist() +\n  labs(\n    title=\"Maryland Counties with Most PPP Loans\",\n    x = \"total loans\",\n    y = \"county\",\n    caption = \"source: SBA PPP loan database\"\n\n  )\n\n\n\n\nViola. Not super pretty, but good enough to show an editor to help them understand the conclusions you reached with your data analysis."
  },
  {
    "objectID": "visualizing-for-reporting.html#line-charts",
    "href": "visualizing-for-reporting.html#line-charts",
    "title": "26  Visualizing your data for reporting",
    "section": "26.2 Line charts",
    "text": "26.2 Line charts\nLet’s look at how to make another common chart type that will help you understand patterns in your data.\nLine charts can show change over time. It works much the same as a bar chart, code wise, but instead of a weight, it uses a y.\nSo, let’s create a dataframe with a count of Maryland loans for each date in our dataframe.\n\nppp_maryland_loans_by_date <- ppp_maryland %>%\n  group_by(date_approved) %>%\n  summarise(\n    total_loans=n()\n  )\n\nppp_maryland_loans_by_date\n\n# A tibble: 235 × 2\n   date_approved total_loans\n   <date>              <int>\n 1 2020-04-03             28\n 2 2020-04-04            262\n 3 2020-04-05            487\n 4 2020-04-06            830\n 5 2020-04-07           1164\n 6 2020-04-08           1354\n 7 2020-04-09           1777\n 8 2020-04-10           1728\n 9 2020-04-11           1243\n10 2020-04-12            528\n# … with 225 more rows\n\n\nAnd now let’s make a line chart to look for patterns in this data.\nWe’ll put the date on the x axis and total loans on the y axis.\n\nppp_maryland_loans_by_date %>%\n  ggplot() +\n  geom_line(aes(x=date_approved, y=total_loans))\n\n\n\n\nIt’s not super pretty, but there’s an obvious pattern! There are a ton of loans right at the beginning of the program. There’s a trickle for the next few months, and then no loans at all for several months.\nAt the beginning of 2021, there’s another spike, and a pretty steady level with some fluctuations until July 2021. We know from previous chapters the explanation for this: there was a flood of loans when the program was first authorized, but it eventually ran out of money, and then it was later reauthorized.\nRight now, it’s kind of hard to see specifics, though. Exactly when did loans fall to zero? August 2020?\nWe can’t really tell. So let’s modify the x axis to have one tick mark and label per month. We can do that with a function called scale_x_date().\nWe’ll set the date_breaks to appear for every month; if we wanted every week, we’d say date_breaks = “1 week”. We can set the date to appear as month abbreviated name (%b) and four-digit year (%Y).\n\nppp_maryland_loans_by_date %>%\n  ggplot() +\n  geom_line(aes(x=date_approved, y=total_loans)) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b-%Y\")\n\n\n\n\nThose are a little hard to read, so we can turn them 45 degrees to remove the overlap using the theme() function for styling. With “axis.text.x = element_text(angle = 45, hjust=1)” we’re saying, turn the date labels 45 degrees.\n\nppp_maryland_loans_by_date %>%\n  ggplot() +\n  geom_line(aes(x=date_approved, y=total_loans)) +\n  scale_x_date(date_breaks = \"1 month\", date_labels = \"%b-%Y\") +\n  theme(\n    axis.text.x = element_text(angle = 45,  hjust=1)\n  )\n\n\n\n\nAgain, this isn’t as pretty as we could make it. But by charting this, we can quickly see a pattern that can help guide our reporting.\nWe’re just scratching the surface of what ggplot can do, and chart types. There’s so much more you can do, so many other chart types you can make. But the basics we’ve shown here will get you started."
  },
  {
    "objectID": "visualizing-for-publication.html",
    "href": "visualizing-for-publication.html",
    "title": "27  Visualizing your data for publication",
    "section": "",
    "text": "Doing data visualization well, and at professional level, takes time, skill and practice to perfect. Understanding it and doing it at a complex level is an entire class on it’s own. It uses some of the same skills here – grouping, filtering, calculating – but then takes that data and turns it into data pictures.\nBut simple stuff – and even some slightly complicated stuff – can be done with tools made for people who aren’t data viz pros.\nThe tool we’re going to use is called Datawrapper.\nFirst, let’s get some data and work with it. Let’s use a simple CSV of total votes cast for four Maryland Republicans who ran for statewide office in the 2022 general election. Let’s look at it."
  },
  {
    "objectID": "visualizing-for-publication.html#datawrapper",
    "href": "visualizing-for-publication.html#datawrapper",
    "title": "27  Visualizing your data for publication",
    "section": "27.1 Datawrapper",
    "text": "27.1 Datawrapper\nMaking charts in Datawrapper is preposterously simple, which is the point. There are dozens of chart types, and dozens of options. To get from a csv to a chart to publication is very, very easy.\nFirst, go to datawrapper.de and sign up for an account. It’s free.\nOnce logged in, you’ll click on New Chart.\n\n\n\n\n\nThe first thing we’ll do is upload our CSV. Click on XLS/CSV and upload the file.\n\n\n\n\n\nNext up is to check and see what Datawrappper did with our data when we uploaded it. As you can see from the text on the left, if it’s blue, it’s a number. If it’s green, it’s a date. If it’s black, it’s text. Red means there’s a problem. This data is very clean, so it imports cleanly. Click on the “Proceed” button.\n\n\n\n\n\nNow we make a chart. Bar chart comes up by default, which is good, because with totals, that’s what we have.\nClick on Refine. The first option we want to change is column we’re using for the bars, which defaults to “Early Voting”. Let’s make it “Votes. Let’s also choose to sort the bars so that the largest value (and bar appears first). We do that by clicking on the”Sort bars” button.\n\n\n\n\n\nNow we need to annotate our charts. Every chart needs a title, a source line and a credit line. Most need chatter (called description here). Click on the “Annotate” tab to get started.\nReally think about the title and description: the title is like a headline and the description is provides some additional context. Another way to think about it: the title is the most important lesson from the graphic, and the description could be the next most important lesson or could provide more context to the title.\n\n\n\n\n\nTo publish, we click the “Publish & Embed” tab. Some publication systems allow for the embedding of HTML into a post or a story. Some don’t. The only way to know is to ask someone at your publication. Every publication system on the planet, though, can publish an image. So there’s always a way to export your chart as a PNG file, which you can upload like any photo.\n\n\n\n\n\n\n27.1.1 Making a Map\nLet’s create a choropleth map - one that shows variations between the percentage of votes received by Wes Moore across Maryland counties. We’ll read that in from the data folder.\n\nmd_gov_county <- read_csv(\"data/md_gov_county.csv\")\n\nIn order to make a map, we need to be able to tell Datawrapper that a certain column contains geographic information (besides the name of the county). The easiest way to do that for U.S. maps is to use something called a FIPS Code. You should read about them so you understand what they are, and think of them as a unique identifier for some geographical entity like a state or county. Our md_gov_county dataframe has a FIPS code for each county, but if you ever need one for a county, this is a solved problem thanks to the Tigris library that we used in pre_lab 9.\nWe’ll need to write code to add columns showing the total number of votes for each county and the percentage of votes received by Wes Moore in each county, then replace the CSV file in the data folder with it\n\nmd_gov_county <- md_gov_county %>% \n  mutate(Total = Cox + Moore + Lashar + Wallace + Harding + `Write-ins`) %>% \n  mutate(PctMoore = Moore/Total * 100)\n\nwrite_csv(md_gov_county, \"data/md_gov_county_with_percent.csv\")\n\nGo back to Datawrapper and click on “New Map”. Click on “Choropleth map” and then choose “USA >> Counties (2022)” for the map base and click the Proceed button.\nNow we can upload the md_gov_county_with_percent.csv file we just saved using the Upload File button. It should look like the following image:\n\n\n\n\n\nWe’ll need to make sure that Datawrapper understands what the data is and where the FIPS code is. Click on the “Match” tab and make sure that yours looks like the image below:\n\n\n\n\n\nClick the “Proceed” button (you should have to click it twice, since the first time it will tell you that there’s no data for 3,197 counties - the rest of the U.S.). That will take you to the Visualize tab.\nYou’ll see that the map currently is of the whole nation, and we only have Maryland data. Let’s fix that.\nLook for “Hide regions without data” under Appearance, and click the slider icon to enable that feature. You should see a map zoomed into Maryland with some counties in various colors.\nBut it’s a little rough visually, so let’s clean that up.\nLook for the “Show color legend” label and add a caption for the legend, which is the horizontal bar under the title. Then click on the “Annotate” tab to add a title, description, data source and byline. The title should represent the headline, while the description should be a longer phrase that tells people what they are looking at.\nThat’s better, but check out the tooltip by hovering over a county. It’s not super helpful. Let’s change the tooltip behavior to show the county name and a better-formatted number.\nClick the “Customize tooltips” button so it expands down. Change {{ fips_code }} to {{ county }} and {{ pctmoore }} to {{ FORMAT(pctmoore, “00.0%”)}}\nExperiment with the “Show labels” options to see if you can add county labels to your map.\nOk, that looks better. Let’s publish!\nClick the “Proceed” button until you get to the “Publish & Embed” tab, then click “Publish Now”."
  },
  {
    "objectID": "geographic-basics.html",
    "href": "geographic-basics.html",
    "title": "28  Geographic data basics",
    "section": "",
    "text": "Up to now, we’ve been looking at patterns in data for what is more than this, or what’s the middle look like. We’ve calculated metrics like percentages, or looked at how data changes over time.\nAnother way we can look at the data is geographically. Is there a spatial pattern to our data? Can we learn anything by using distance as a metric? What if we merge non-geographic data into geographic data?\nThe bad news is that there isn’t a One Library To Rule Them All when it comes to geo queries in R. But there’s one emerging, called Simple Features, that is very good.\nGo to the console and install it with install.packages(\"sf\")\nTo understand geographic queries, you have to get a few things in your head first:"
  },
  {
    "objectID": "geographic-basics.html#importing-and-viewing-data",
    "href": "geographic-basics.html#importing-and-viewing-data",
    "title": "28  Geographic data basics",
    "section": "28.1 Importing and viewing data",
    "text": "28.1 Importing and viewing data\nLet’s start with the absolute basics of geographic data: loading and viewing. Load libraries as usual.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\n\nFirst: an aside on geographic data. There are many formats for geographic data, but data type you’ll see the most is called the shapefile. It comes from a company named ESRI, which created the most widely-used GIS software in the world. For years, they were the only game in town, really, and the shapefile became ubiquitous, especially so in government and utilities.\nSo more often than not, you’ll be dealing with a shapefile. But a shapefile isn’t just a single file – it’s a collection of files that combined make up all the data that allow you to use it. There’s a .shp file – that’s the main file that pulls it all together – but it’s important to note if your shapefiles has a .prj file, which indicates that the projection is specified. There’s also a newer format from ESRI called a geodatabase (.gdb) that contains all of the elements that shapefiles do. We can use the sf library to read either format.\nThe data we’re going to be working with is a file from the Maryland Citizens Redistricting Commission showing data about political districts in the state and their demographic characteristics.\nSimilar to readr, the sf library has functions to read geographic data. In this case, we’re going to use st_read to read in our hospitals data. And then glimpse it to look at the columns.\n\nblocks <- st_read(\"data/Adjusted2020.gdb\")\n\nReading layer `Census_Blocks' from data source \n  `/Users/derekwillis/code/datajournalismbook-elections/data/Adjusted2020.gdb' \n  using driver `OpenFileGDB'\nSimple feature collection with 83827 features and 26 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -79.48765 ymin: 37.88661 xmax: -74.98628 ymax: 39.72304\nGeodetic CRS:  NAD83\n\nglimpse(blocks)\n\nRows: 83,827\nColumns: 27\n$ Block               <chr> \"240479514002073\", \"240479514001005\", \"24047951400…\n$ BlockGroup          <chr> \"240479514002\", \"240479514001\", \"240479514001\", \"2…\n$ Tract               <chr> \"24047951400\", \"24047951400\", \"24047951400\", \"2404…\n$ County              <chr> \"24047\", \"24047\", \"24047\", \"24047\", \"24047\", \"2404…\n$ State               <chr> \"24\", \"24\", \"24\", \"24\", \"24\", \"24\", \"24\", \"24\", \"2…\n$ VTD                 <chr> \"2404704-001\", \"2404702-001\", \"2404702-001\", \"2404…\n$ Adj_Population      <int> 0, 4, 0, 31, 18, 2, 28, 3, 18, 5, 45, 0, 5, 0, 0, …\n$ Adj_Hispanic_Origin <int> 0, 0, 0, 2, 1, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH_Wht          <int> 0, 4, 0, 29, 13, 0, 16, 1, 8, 3, 16, 0, 1, 0, 0, 0…\n$ Adj_NH_Blk          <int> 0, 0, 0, 0, 3, 0, 7, 0, 1, 0, 28, 0, 3, 0, 0, 0, 0…\n$ Adj_NH_Ind          <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH_Asn          <int> 0, 0, 0, 0, 0, 0, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH_Hwn          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH_Oth          <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH_2__Races     <int> 0, 0, 0, 0, 1, 1, 0, 2, 6, 1, 0, 0, 1, 0, 0, 0, 1,…\n$ Adj_18__Pop         <int> 0, 4, 0, 30, 15, 2, 21, 1, 12, 4, 32, 0, 4, 0, 0, …\n$ Adj_H18__Pop        <int> 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH18__Wht       <int> 0, 4, 0, 28, 11, 0, 13, 1, 8, 3, 7, 0, 1, 0, 0, 0,…\n$ Adj_NH18__Blk       <int> 0, 0, 0, 0, 3, 0, 7, 0, 0, 0, 25, 0, 3, 0, 0, 0, 0…\n$ Adj_NH18__Ind       <int> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH18__Asn       <int> 0, 0, 0, 0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH18__Hwn       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH18__Oth       <int> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ Adj_NH18__2__Races  <int> 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ Shape_Length        <dbl> 0.256035239, 0.069209540, 0.156886172, 0.278946567…\n$ Shape_Area          <dbl> 7.494290e-05, 1.873535e-04, 3.687426e-05, 1.309177…\n$ Shape               <MULTIPOLYGON [°]> MULTIPOLYGON (((-75.42637 3..., MULTI…\n\n\nThis looks like a normal dataframe, and mostly it is. We have one row per Census block - the smallest geographic census unit - and each column is some feature of that block: the census tract, county and state it is located in and more. What sets this data apart from other dataframes we’ve used is the last column, “geometry”, which is of a new data type. It’s not a character or a number, it’s a “MULTIPOLYGON”, which is composed of a shape that, when we plot it on a map, will be highlighted. If you’ve ever filled in a “connect the dots” picture by drawing lines between points, in order to reveal a hidden shape, then you’re familiar with the concept.\nYou can see the full description of the data.\nLet’s look at just Baltimore City census blocks. Good news – sf plays very nicely with the tidyverse, so we can filter data the way we are accustomed. The County column represents what is called a FIPS code, which is a unique code for a geographic unit like state or county. All Maryland FIPS codes begin with “24”, which is the code for the state as a whole.\n\nbaltimore_city_blocks <- blocks %>%\n  filter(County == \"24510\")\n\nbaltimore_city_blocks\n\nSimple feature collection with 10025 features and 26 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -76.71152 ymin: 39.19721 xmax: -76.52945 ymax: 39.37221\nGeodetic CRS:  NAD83\nFirst 10 features:\n             Block   BlockGroup       Tract County State         VTD\n1  245102705011005 245102705011 24510270501  24510    24 2451027-011\n2  245102705011004 245102705011 24510270501  24510    24 2451027-011\n3  245102705013006 245102705013 24510270501  24510    24 2451027-010\n4  245102705013007 245102705013 24510270501  24510    24 2451027-010\n5  245102705011012 245102705011 24510270501  24510    24 2451027-010\n6  245102705021015 245102705021 24510270502  24510    24 2451027-008\n7  245102705021013 245102705021 24510270502  24510    24 2451027-008\n8  245102705021014 245102705021 24510270502  24510    24 2451027-008\n9  245102705021011 245102705021 24510270502  24510    24 2451027-008\n10 245102705021010 245102705021 24510270502  24510    24 2451027-008\n   Adj_Population Adj_Hispanic_Origin Adj_NH_Wht Adj_NH_Blk Adj_NH_Ind\n1              21                   3          8          6          0\n2              47                   4         22         17          0\n3              45                   1          5         32          0\n4              67                   2         18         43          0\n5              25                   0         13          5          0\n6              37                   2         17         18          0\n7              54                   4         29         20          0\n8              41                   2          6         29          0\n9              12                   0          1          5          0\n10             19                   5          9          5          0\n   Adj_NH_Asn Adj_NH_Hwn Adj_NH_Oth Adj_NH_2__Races Adj_18__Pop Adj_H18__Pop\n1           0          0          1               3          20            3\n2           3          0          0               1          40            3\n3           2          0          0               5          39            1\n4           1          0          0               3          55            2\n5           4          0          1               2          25            0\n6           0          0          0               0          23            2\n7           0          0          0               1          47            3\n8           0          0          4               0          29            0\n9           1          0          0               5           7            0\n10          0          0          0               0          19            5\n   Adj_NH18__Wht Adj_NH18__Blk Adj_NH18__Ind Adj_NH18__Asn Adj_NH18__Hwn\n1              8             5             0             0             0\n2             19            14             0             3             0\n3              5            26             0             2             0\n4             17            32             0             1             0\n5             13             5             0             4             0\n6             12             9             0             0             0\n7             24            20             0             0             0\n8              6            19             0             0             0\n9              1             1             0             0             0\n10             9             5             0             0             0\n   Adj_NH18__Oth Adj_NH18__2__Races Shape_Length   Shape_Area\n1              1                  3  0.004460665 8.112280e-07\n2              0                  1  0.005213052 1.167317e-06\n3              0                  5  0.005749833 1.816332e-06\n4              0                  3  0.009587319 3.818661e-06\n5              1                  2  0.004994883 1.492671e-06\n6              0                  0  0.004975204 1.464421e-06\n7              0                  0  0.012717356 6.443919e-06\n8              4                  0  0.004851208 1.433677e-06\n9              0                  5  0.004014872 7.556780e-07\n10             0                  0  0.003981790 7.430995e-07\n                            Shape\n1  MULTIPOLYGON (((-76.5405 39...\n2  MULTIPOLYGON (((-76.53746 3...\n3  MULTIPOLYGON (((-76.53486 3...\n4  MULTIPOLYGON (((-76.53509 3...\n5  MULTIPOLYGON (((-76.53303 3...\n6  MULTIPOLYGON (((-76.53278 3...\n7  MULTIPOLYGON (((-76.5299 39...\n8  MULTIPOLYGON (((-76.53184 3...\n9  MULTIPOLYGON (((-76.53112 3...\n10 MULTIPOLYGON (((-76.53258 3...\n\n\nWe have 10,025 blocks, according to this data. Now we can try to get a sense of the population in them. In how many blocks does the number of Hispanic population 18 and over exceed both non-Hispanic white 18 and over population and non-Hispanic Black 18 and over population?\n\nbaltimore_city_blocks %>%\n  filter(Adj_H18__Pop > Adj_NH18__Wht & Adj_H18__Pop > Adj_NH18__Blk)\n\nSimple feature collection with 386 features and 26 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -76.71119 ymin: 39.20299 xmax: -76.52952 ymax: 39.36776\nGeodetic CRS:  NAD83\nFirst 10 features:\n             Block   BlockGroup       Tract County State         VTD\n1  245102605013003 245102605013 24510260501  24510    24 2451026-004\n2  245102605013000 245102605013 24510260501  24510    24 2451026-004\n3  245102605011005 245102605011 24510260501  24510    24 2451026-005\n4  245102605011019 245102605011 24510260501  24510    24 2451026-005\n5  245102605011011 245102605011 24510260501  24510    24 2451026-004\n6  245102605011012 245102605011 24510260501  24510    24 2451026-004\n7  245102605011002 245102605011 24510260501  24510    24 2451026-004\n8  245102605011008 245102605011 24510260501  24510    24 2451026-004\n9  245102605011009 245102605011 24510260501  24510    24 2451026-004\n10 245102604041015 245102604041 24510260404  24510    24 2451026-011\n   Adj_Population Adj_Hispanic_Origin Adj_NH_Wht Adj_NH_Blk Adj_NH_Ind\n1             180                  85         51         39          0\n2              26                  17          5          0          0\n3              71                  56          8          4          0\n4             109                  54         35          9          2\n5              94                  47         34          8          0\n6              90                  57         25          2          0\n7              74                  56          6         11          0\n8             103                  62         27          8          0\n9              59                  33         18          7          0\n10             10                   6          0          4          0\n   Adj_NH_Asn Adj_NH_Hwn Adj_NH_Oth Adj_NH_2__Races Adj_18__Pop Adj_H18__Pop\n1           2          0          0               3         131           53\n2           0          0          0               4          14           10\n3           0          0          0               3          50           36\n4           6          0          0               3          83           37\n5           2          0          0               3          64           28\n6           6          0          0               0          55           26\n7           1          0          0               0          50           34\n8           0          0          0               6          78           48\n9           0          0          0               1          49           24\n10          0          0          0               0           7            6\n   Adj_NH18__Wht Adj_NH18__Blk Adj_NH18__Ind Adj_NH18__Asn Adj_NH18__Hwn\n1             47            28             0             2             0\n2              4             0             0             0             0\n3              8             4             0             0             0\n4             32             5             1             6             0\n5             24             8             0             2             0\n6             25             2             0             2             0\n7              6             9             0             1             0\n8             24             0             0             0             0\n9             17             7             0             0             0\n10             0             1             0             0             0\n   Adj_NH18__Oth Adj_NH18__2__Races Shape_Length   Shape_Area\n1              0                  1  0.005735603 1.580966e-06\n2              0                  0  0.006037951 1.841948e-06\n3              0                  2  0.003040933 5.478480e-07\n4              0                  2  0.003975398 7.063020e-07\n5              0                  2  0.004041064 7.579205e-07\n6              0                  0  0.003965293 7.160955e-07\n7              0                  0  0.003695767 6.496640e-07\n8              0                  6  0.003821795 6.676600e-07\n9              0                  1  0.003384148 5.786680e-07\n10             0                  0  0.024130223 1.573279e-05\n                            Shape\n1  MULTIPOLYGON (((-76.52971 3...\n2  MULTIPOLYGON (((-76.52958 3...\n3  MULTIPOLYGON (((-76.54528 3...\n4  MULTIPOLYGON (((-76.54295 3...\n5  MULTIPOLYGON (((-76.54244 3...\n6  MULTIPOLYGON (((-76.541 39....\n7  MULTIPOLYGON (((-76.54008 3...\n8  MULTIPOLYGON (((-76.54054 3...\n9  MULTIPOLYGON (((-76.53773 3...\n10 MULTIPOLYGON (((-76.53792 3...\n\n\nSo there are 386 Census blocks with adult Hispanic population greater than either white or Black adult population. Where are they? We can simply plot them on a longitude-latitude grid using ggplot.\n\nbaltimore_hispanic_blocks <- baltimore_city_blocks %>%\n  filter(Adj_H18__Pop > Adj_NH18__Wht & Adj_H18__Pop > Adj_NH18__Blk)\n\n\nbaltimore_hispanic_blocks %>%\n  ggplot() +\n  geom_sf() +\n  theme_minimal()\n\n\n\n\nEach shape is a Census block.\nIf you know anything about the shape of Baltimore, you can kinda identify it here. The untouched area in the lower right is probably the harbor, right? But this map is not exactly ideal. It would help to have a state and county map layered underneath of it, to help make sense of the spatial nature of this data.\nThis is where layering becomes more clear. First, we want to go out and get another shapefile, this one showing Maryland jurisdiction outlines.\nInstead of loading it from our local machine, like we did above, we’re going to use a package to directly download it from the U.S. Census. The package is called tigris and it’s developed by the same person who made tidycensus.\nIn the console, install tigris with the install packages function\nThen load it:\n\nlibrary(tigris)\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nNow, let’s use the counties() function from tigris to pull down a shapefile of all U.S. counties.\n\ncounties <- counties()\n\nRetrieving data for the year 2020\n\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |                                                                      |   1%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |=======                                                               |  11%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |=========                                                             |  14%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |=======================                                               |  34%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |==============================                                        |  44%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |===================================================                   |  74%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==========================================================            |  84%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |=================================================================     |  94%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\nglimpse(counties)\n\nRows: 3,234\nColumns: 18\n$ STATEFP  <chr> \"31\", \"53\", \"35\", \"31\", \"31\", \"72\", \"46\", \"48\", \"06\", \"21\", \"…\n$ COUNTYFP <chr> \"039\", \"069\", \"011\", \"109\", \"129\", \"085\", \"099\", \"327\", \"091\"…\n$ COUNTYNS <chr> \"00835841\", \"01513275\", \"00933054\", \"00835876\", \"00835886\", \"…\n$ GEOID    <chr> \"31039\", \"53069\", \"35011\", \"31109\", \"31129\", \"72085\", \"46099\"…\n$ NAME     <chr> \"Cuming\", \"Wahkiakum\", \"De Baca\", \"Lancaster\", \"Nuckolls\", \"L…\n$ NAMELSAD <chr> \"Cuming County\", \"Wahkiakum County\", \"De Baca County\", \"Lanca…\n$ LSAD     <chr> \"06\", \"06\", \"06\", \"06\", \"06\", \"13\", \"06\", \"06\", \"06\", \"06\", \"…\n$ CLASSFP  <chr> \"H1\", \"H1\", \"H1\", \"H1\", \"H1\", \"H1\", \"H1\", \"H1\", \"H1\", \"H1\", \"…\n$ MTFCC    <chr> \"G4020\", \"G4020\", \"G4020\", \"G4020\", \"G4020\", \"G4020\", \"G4020\"…\n$ CSAFP    <chr> NA, NA, NA, \"339\", NA, \"490\", NA, NA, NA, NA, \"534\", \"352\", N…\n$ CBSAFP   <chr> NA, NA, NA, \"30700\", NA, \"41980\", \"43620\", NA, NA, NA, \"22300…\n$ METDIVFP <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n$ FUNCSTAT <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"…\n$ ALAND    <dbl> 1477645345, 680976231, 6016818946, 2169272970, 1489645188, 87…\n$ AWATER   <dbl> 10690204, 61568965, 29090018, 22847034, 1718484, 32509, 18188…\n$ INTPTLAT <chr> \"+41.9158651\", \"+46.2946377\", \"+34.3592729\", \"+40.7835474\", \"…\n$ INTPTLON <chr> \"-096.7885168\", \"-123.4244583\", \"-104.3686961\", \"-096.6886584…\n$ geometry <MULTIPOLYGON [°]> MULTIPOLYGON (((-97.01952 4..., MULTIPOLYGON (((…\n\n\nThis looks pretty similar to our Census blocks shapefile, in that it looked mostly like a normal dataframe with the exception of the new geometry column.\nThis county shapefile has all 3233 U.S. counties. We only want the Maryland counties, so we’re going to filter the data to only keep Maryland counties. There is no STATE column, but there is a STATEFP column, with each number representing a state. Maryland’s FP number is 24.\n\nmd_counties <- counties %>%\n  filter(STATEFP == \"24\")\n\nTo see what this looks like, let’s plot it out with ggplot. We can pretty clearly see the shapes of Maryland counties.\n\nmd_counties %>%\n  ggplot() +\n  geom_sf() +\n  theme_minimal()\n\n\n\n\nHey, look, it’s Maryland! Of course, we just need Baltimore City, so let’s get that:\n\nbaltimore_city <- md_counties %>%\n  filter(COUNTYFP == \"510\")\n\nWith this map, we can layer our Census block data.\nSomething to note: The layers are rendered in the order they appear. So the first geom_sf is rendered first. The second geom_sf is rendered ON TOP OF the first one.\nWe’re also going to change things up a bit to put the datasets we want to display INSIDE of the geom_sf() function, instead of starting with a dataframe. We have two to plot now, so it’s easier this way.\n\nggplot() +\n  geom_sf(data=baltimore_city) +\n  geom_sf(data=baltimore_hispanic_blocks) +\n  theme_minimal()\n\n\n\n\nWhat can we tell from this?\nWell, Census blocks where adult Hispanic population outnumbers both white and Black adult population are clustered in the city’s eastern and southwestern areas. There are fewer such blocks in the northern parts of the city. We’re also missing the water - the harbor, rivers and streams. But we can add those, too! The city offers various GIS data, including the locations of waterways.\n\nwater <- st_read(\"data/Water.shp\")\n\nReading layer `Water' from data source \n  `/Users/derekwillis/code/datajournalismbook-elections/data/Water.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 468 features and 5 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: 1393746 ymin: 557989.5 xmax: 1445801 ymax: 621654.7\nProjected CRS: NAD83 / Maryland (ftUS)\n\n\nNow let’s add that to our previous map. The order here matters - we want the outline of the city first, followed by water and then the Hispanic population blocks:\n\nggplot() +\n  geom_sf(data=baltimore_city) +\n  geom_sf(data=water) +\n  geom_sf(data=baltimore_hispanic_blocks) +\n  theme_minimal()\n\n\n\n\nBetter, but this is a pretty blunt visualization. Not all Census blocks are equally sized. Some have more people than the others, for example. It’s hard to differentiate that here.\nWe can get a sense of where the largest Hispanic populations are, by changing the color of the shapes according to the number of people. We do this by setting the aesthetic – or aes – to use the Adj_H18__POP column inside of the geom_sf function. To make the differences easier to see, we’re going to change the fill of the city white, too, and use a special color palette, viridis magma. We’re also going to make the shapes slightly bigger.\n\nggplot() +\n  geom_sf(data=baltimore_city, fill=\"white\") +\n  geom_sf(data=water) +\n  geom_sf(data=baltimore_hispanic_blocks, aes(color=Adj_H18__Pop), size=2) +\n  scale_colour_viridis_b(option=\"magma\") +\n  theme_minimal()\n\n\n\n\nWith these changes, what else can we make out here? Well, definitely the eastern and southwestern blocks have larger Hispanic populations, and then there’s the northwest corner that stands out, too. You should have questions."
  },
  {
    "objectID": "geographicanalysis.html",
    "href": "geographicanalysis.html",
    "title": "29  Geographic analysis",
    "section": "",
    "text": "First, let’s load the libraries we’ll need. We’re also going to load tidycensus and set an API key for tidycensus.\n\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(janitor)\nlibrary(tidycensus)\ncensus_api_key(\"549950d36c22ff16455fe196bbbd01d63cfbe6cf\")\n\nAnd now let’s load a dataframe of Maryland county population information from the 2020 Census, but this time we’ll add in county geographic data.\n\nmaryland_counties <- read_csv(\"data/md_counties_2020.csv\")\n\nRows: 24 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): SUMLEV, GEOCODE, NAME, BASENAME, FILEID, STUSAB, CHARITER, CIFSN\ndbl (31): GEOID, LOGRECNO, TOTAL_POP, POP_ONE_RACE, POP_WHITE, POP_BLACK, PO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmaryland_county_shapes <- st_read(\"data/Maryland2020_County.json\")\n\nReading layer `Maryland2020_County' from data source \n  `/Users/derekwillis/code/datajournalismbook-elections/data/Maryland2020_County.json' \n  using driver `ESRIJSON'\nSimple feature collection with 24 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -79.48765 ymin: 37.8866 xmax: -74.98628 ymax: 39.72304\nGeodetic CRS:  NAD83\n\nmaryland_counties_with_shapes <- maryland_counties %>% inner_join(maryland_county_shapes, by=c('GEOID'='GEOID20'))\n\nError in `inner_join()`:\n! Can't join on `x$GEOID` x `y$GEOID` because of incompatible types.\nℹ `x$GEOID` is of type <double>>.\nℹ `y$GEOID` is of type <character>>.\n\n\nAnd there’s that join error: we’ve got incompatible data types. We can quickly fix that:\n\nmaryland_counties <- read_csv(\"data/md_counties_2020.csv\") %>% mutate(GEOID = as.character(GEOID))\n\nRows: 24 Columns: 39\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): SUMLEV, GEOCODE, NAME, BASENAME, FILEID, STUSAB, CHARITER, CIFSN\ndbl (31): GEOID, LOGRECNO, TOTAL_POP, POP_ONE_RACE, POP_WHITE, POP_BLACK, PO...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nmaryland_county_shapes <- st_read(\"data/Maryland2020_County.json\")\n\nReading layer `Maryland2020_County' from data source \n  `/Users/derekwillis/code/datajournalismbook-elections/data/Maryland2020_County.json' \n  using driver `ESRIJSON'\nSimple feature collection with 24 features and 18 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -79.48765 ymin: 37.8866 xmax: -74.98628 ymax: 39.72304\nGeodetic CRS:  NAD83\n\nmaryland_counties_with_shapes <- maryland_counties %>% inner_join(maryland_county_shapes, by=c('GEOID'='GEOID20'))\n\nFor the rest of this chapter, we’re going to work on building a map that will help us gain insight into geographic patterns in registered voters and population by county and in Maryland. Our starting question: by examining the number of voters and the population in each county, what regional geographic patterns can we identify?\nFirst we need to load voter registration data - since the population data is from 2020, we’ll use the county totals as of the 2020 general election. And we’ll make sure that the FIPS code that we need to join on is a character column:\n\nmaryland_voters_by_county <- read_csv(\"data/maryland_voters_2020g.csv\") %>% mutate(FIPS=as.character(FIPS))\n\nRows: 24 Columns: 11\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (1): County\ndbl (10): FIPS, DEM, REP, BAR, GRN, LIB, WCP, OTH, UNA, TOTAL\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nNow let’s combine our two dataframes:\n\nmaryland_voters_by_county_with_pop <- maryland_voters_by_county %>% inner_join(maryland_counties_with_shapes, by=c('FIPS'='GEOID'))\n\nNow we can do some calculations and visualize them on a map. Let’s calculate the number of Unaffiliated (no party) voters per 10,000 people and take a look at just the numbers.\n\nmd_una_per_10k <- maryland_voters_by_county_with_pop %>%\n  mutate(una_per_10k = UNA/TOTAL_POP*10000) %>%\n  arrange(desc(una_per_10k)) %>%\n  select(County, DEM, REP, OTH, UNA, TOTAL, una_per_10k, geometry)\n\nmd_una_per_10k\n\n# A tibble: 24 × 8\n   County       DEM    REP   OTH    UNA  TOTAL una_p…¹                  geometry\n   <chr>      <dbl>  <dbl> <dbl>  <dbl>  <dbl>   <dbl>             <POLYGON [°]>\n 1 Frederick  72487  68767   801  42961 186356   2069. ((-77.62017 39.48469, -7…\n 2 Howard    118705  52748  2035  51909 226634   2067. ((-77.041 39.23419, -77.…\n 3 Calvert    24587  28181   567  14178  67984   1999. ((-76.69195 38.68377, -7…\n 4 Anne Aru… 174494 135457  2715  90162 405616   1977. ((-76.81615 39.12143, -7…\n 5 Carroll    33662  63967  1072  25770 125361   1908. ((-77.19802 39.57543, -7…\n 6 Cecil      21601  30880   727  15110  68819   1878. ((-75.93711 39.72188, -7…\n 7 Harford    66258  80038  1535  37942 187092   1875. ((-76.45929 39.50719, -7…\n 8 Queen An…  11091  18930   280   7133  37678   1824. ((-76.04684 39.16321, -7…\n 9 Saint Ma…  26373  30661   606  15661  73833   1813. ((-76.82022 38.43505, -7…\n10 Montgome… 410935 105561  5816 147417 673198   1800. ((-77.43151 39.23217, -7…\n# … with 14 more rows, and abbreviated variable name ¹​una_per_10k\n\n\nLet’s take a look at the result of this table. We might call this where the swing voters are, although political science tells us that most independents actually vote with a party most of the time. Frederick, Howard, Calvert, Anne Arundel and Carroll counties all have some competitive races at the local and state level, so that makes some sense. Montgomery, the state’s largest jurisdiction, has more independents than Republicans.\nOkay, now let’s visualize. We’re going to build a choropleth map, with the color of each county – the fill – set according to the number of unaffiliated voters per 10K on a color gradient.\n\nggplot() +\n  geom_sf(data=md_una_per_10k, aes(fill=una_per_10k)) +\n  theme_minimal()\n\nError in `check_required_aesthetics()`:\n! stat_sf requires the following missing aesthetics: geometry\n\n\nSo this error message is pretty sneaky, because it’s saying that it can’t find a geometry column. But we have a geometry column - it’s called geometry! What’s happening here is that ggplot expects certain details in order to be able to locate the column with geometry in it, and sometimes when we build a new dataframe through joins those details get dropped. Luckily, we can explicitly tell ggplot where to find it:\n\nggplot() +\n  geom_sf(data=md_una_per_10k, aes(fill=una_per_10k, geometry=geometry)) +\n  theme_minimal()\n\n\n\n\nThis map is okay, but the color scale makes it hard to draw fine-grained differences. Let’s try applying the magma color scale we learned in the last chapter.\n\nggplot() +\n  geom_sf(data=md_una_per_10k, aes(fill=una_per_10k, geometry=geometry)) +\n  theme_minimal() +\n  scale_fill_viridis_b(option=\"magma\")\n\n\n\n\nSome interesting regional patterns finally emerge.\nThe highest number of unaffiliated voters per capita are clustered close to DC and Baltimore, with the notable exception of Prince George’s County. The further west you go, the lower number of unaffiliated voters per capita there are (and that’s somewhat true for the Eastern Shore, too). You could swap out UNA for Republicans or Democrats, or look at this another way, but what you’re trying to see here are potential patterns for further exploration."
  },
  {
    "objectID": "textanalysis.html",
    "href": "textanalysis.html",
    "title": "30  An intro to text analysis",
    "section": "",
    "text": "Throughout this course, we’ve been focused on finding information in structured data. We’ve learned a lot of techniques to do that, and we’ve learned how the creative mixing and matching of those skills can find new insights.\nWhat happens when the insights are in unstructured data? Like a block of text?\nTurning unstructured text into data to analyze is a whole course in and of itself – and one worth taking if you’ve got the credit hours – but some simple stuff is in the grasp of basic data analysis.\nTo do this, we’ll need a new library – tidytext, which you can guess by the name plays very nicely with the tidyverse. So install it with install.packages(\"tidytext\") and we’ll get rolling.\nHere’s the question we’re going to go after: How did federal politicians talk about the coronavirus pandemic on Twitter?\nTo answer this question, we’ll use a dataset of tweets posted by federal politicians from both campaign and official accounts that mentioned either “COVID” or “coronavirus” beginning on Feb. 1, 2020. This dataset doesn’t include retweets, only original tweets. Let’s read in this data and examine it:\nWe can see what it looks like with head:\nWhat we want to do is to make the content column easier to analyze. Let’s say we want to find out the most commonly used words. We’ll probably want to remove URLs from the text of the tweets since they aren’t actual words. Let’s use mutate to make that happen:\nIf you are trying to create a list of unique words, R will treat differences in capitalization as unique and also will include punctuation by default, even using its unique function:\nFortunately, this is a solved problem with tidytext, which has a function called unnest_tokens that will convert the text to lowercase and remove all punctuation. The way that unnest_tokens works is that we tell it what we want to call the field we’re creating with this breaking apart, then we tell it what we’re breaking apart – what field has all the text in it. For us, that’s the content column:\nNow we can look at the top words in this dataset. Let’s limit ourselves to making a plot of the top 25 words:\nWell, that’s a bit underwhelming - a lot of very common (and short) words. This also is a solved problem in working with text data, and words like “a” and “the” are known as “stop words”. In most cases you’ll want to remove them from your analysis since they are so common. Tidytext provides a dataframe of them:\nThen we’re going to use a function we haven’t used yet called an anti_join, which filters out any matches. So we’ll anti_join the stop words and get a list of words that aren’t stop words.\nFrom there, we can get a simple word frequency by just grouping them together and counting them. We can borrow the percent code from above to get a percent of the words our top 10 words represent.\nThose seem like more relevant unique words. Now, here’s where we can start to do more interesting and meaningful analysis. Let’s create two dataframes of unique words based on time: one for all of 2020 and the other for all of 2021:\nThen we can create top 10 lists for both of them and compare:\nIn the 2021 top 10 list, “vaccine” and its variations are much more prominent, which makes sense, while “testing” drops out of the top 10 compared to 2020."
  },
  {
    "objectID": "textanalysis.html#going-beyond-a-single-word",
    "href": "textanalysis.html#going-beyond-a-single-word",
    "title": "30  An intro to text analysis",
    "section": "30.1 Going beyond a single word",
    "text": "30.1 Going beyond a single word\nThe next step in text analysis is using ngrams. An ngram is any combination of words that you specify. Two word ngrams are called bigrams (bi-grams). Three would be trigrams. And so forth.\nThe code to make ngrams is similar to what we did above, but involves some more twists.\nSo this block is is going to do the following:\n\nUse the covid_tweets data we created above, and filter for pre-2021 tweets.\nUnnest the tokens again, but instead we’re going to create a field called bigram, break apart summary, but we’re going to specify the tokens in this case are ngrams of 2.\nWe’re going to make things easier to read and split bigrams into word1 and word2.\nWe’re going to filter out stopwords again, but this time we’re going to do it in both word1 and word2 using a slightly different filtering method.\nBecause of some weirdness in calculating the percentage, we’re going to put bigram back together again, now that the stop words are gone.\nWe’ll then group by, count and create a percent just like we did above.\nWe’ll then use top_n to give us the top 10 bigrams.\n\n\ncovid_tweets %>%\n  filter(created < '2021-01-01') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19             32302   3.52 \n 2 health care           4388   0.478\n 3 public health         3908   0.426\n 4 covid19 pandemic      3206   0.349\n 5 town hall             3014   0.328\n 6 coronavirus pandemic  2915   0.318\n 7 19 pandemic           2670   0.291\n 8 covid relief          2246   0.245\n 9 relief package        2062   0.225\n10 covid ー              1832   0.200\n\n\nAnd we already have a different, more nuanced result. Health was among the top single words, and we can see that “health care” and “public health” are among the top 2-word phrases. What about after 2021?\n\ncovid_tweets %>%\n  filter(created >= '2021-01-01') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram               n percent\n   <chr>            <int>   <dbl>\n 1 covid 19         17357   5.35 \n 2 19 vaccine        2638   0.814\n 3 covid relief      2159   0.666\n 4 19 pandemic       1495   0.461\n 5 covid19 vaccine   1227   0.379\n 6 covid vaccine     1099   0.339\n 7 health care        908   0.280\n 8 covid19 pandemic   872   0.269\n 9 public health      860   0.265\n10 19 relief          805   0.248\n\n\nWhile “covid 19” is still the leading phrase, vaccine-related phrases dominate the top 10, and “public health” and “health care” have slipped down the list.\nSo far, we’ve only looked at the entire set of tweets, not any characteristics of who posted them. Would these lists be any different for Democrats and Republicans? To find out, we just need to add to our filter.\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', party == 'D') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19             22182   3.57 \n 2 health care           3398   0.547\n 3 public health         3314   0.534\n 4 town hall             2481   0.400\n 5 covid19 pandemic      2426   0.391\n 6 19 pandemic           1923   0.310\n 7 coronavirus pandemic  1903   0.307\n 8 covid relief          1525   0.246\n 9 relief package        1478   0.238\n10 social distancing     1373   0.221\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', party == 'R') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19              9901   3.45 \n 2 coronavirus pandemic   966   0.336\n 3 health care            943   0.328\n 4 ー 19                  782   0.272\n 5 covid ー               782   0.272\n 6 covid19 pandemic       769   0.268\n 7 cares act              759   0.264\n 8 19 pandemic            733   0.255\n 9 american people        711   0.248\n10 covid relief           703   0.245\n\n\nNow we can begin to see some differences between the parties. We also could do the same for different kinds of accounts: the title column represents the role of the account, and if it includes “Candidate” then the tweet is from a campaign account. Let’s compare House of Representatives’ official and campaign tweets during 2020:\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', title == 'House Representative') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19             15796   3.53 \n 2 town hall             2265   0.507\n 3 health care           2093   0.468\n 4 public health         2033   0.455\n 5 covid19 pandemic      1851   0.414\n 6 19 pandemic           1522   0.341\n 7 coronavirus pandemic  1457   0.326\n 8 covid relief          1108   0.248\n 9 telephone town        1051   0.235\n10 relief package        1010   0.226\n\ncovid_tweets %>%\n  filter(created < '2021-01-01', title == 'House Candidate') %>%\n  unnest_tokens(bigram, content, token = \"ngrams\", n = 2) %>%\n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>%\n  filter(!word1 %in% stop_words$word) %>%\n  filter(!word2 %in% stop_words$word) %>%\n  mutate(bigram = paste(word1, word2, sep=\" \")) %>%\n  group_by(bigram) %>%\n  tally(sort=TRUE) %>%\n  mutate(percent = (n/sum(n))*100) %>%\n  top_n(10)\n\nSelecting by percent\n\n\n# A tibble: 10 × 3\n   bigram                   n percent\n   <chr>                <int>   <dbl>\n 1 covid 19              8378   3.38 \n 2 public health          778   0.314\n 3 covid ー               722   0.291\n 4 ー 19                  721   0.291\n 5 health care            721   0.291\n 6 19 pandemic            519   0.209\n 7 social distancing      518   0.209\n 8 coronavirus pandemic   512   0.207\n 9 town hall              482   0.195\n10 covid relief           458   0.185\n\n\nThere are some differences here, too, but also some potential challenges to doing an analysis. For one, there are variations of words like “vaccine” that could probably be standardized - maybe using OpenRefine - that would give us cleaner results. There might be some words among our list of stop words that actually are meaningful in this context."
  },
  {
    "objectID": "textanalysis.html#sentiment-analysis",
    "href": "textanalysis.html#sentiment-analysis",
    "title": "30  An intro to text analysis",
    "section": "30.2 Sentiment Analysis",
    "text": "30.2 Sentiment Analysis\nAnother popular use of text analysis is to measure the sentiment of a word - whether it expresses a positive or negative idea - and tidytext has built-in tools to make that possible. We use word counts like we’ve already calculated and bring in a dataframe of words (called a lexicon) along with their sentiments using a function called get_sentiments. The most common dataframe is called “bing” which has nothing to do with the Microsoft search engine. Let’s load it:\n\nbing <- get_sentiments(\"bing\")\n\nbing_word_counts_2020 <- unique_words_2020 %>%\n  inner_join(bing) %>%\n  count(word, sentiment, sort = TRUE)\n\nJoining, by = \"word\"\n\nbing_word_counts_2021 <- unique_words_2021 %>%\n  inner_join(bing) %>%\n  count(word, sentiment, sort = TRUE)\n\nJoining, by = \"word\"\n\nView(bing_word_counts_2020)\nView(bing_word_counts_2021)\n\nGauging the sentiment of a word can be heavily dependent on the context, and as with other types of text analysis sometimes larger patterns are more meaningful than individual results. But the potential with text analysis is vast: knowing what words and phrases that public officials employ can be a way to evaluate their priorities, cohesiveness and tactics for persuading voters and their colleagues. And those words and phrases are data."
  },
  {
    "objectID": "basicstats.html",
    "href": "basicstats.html",
    "title": "31  Basic Stats: Linear Regression and The T-Test",
    "section": "",
    "text": "“Reveal’s analysis found that businesses in states that Trump won in 2016 received a far greater share of the small-business relief funds than those won by his Democratic rival, Hillary Clinton. Eight of the top 10 recipient states – ranked according to the proportion of each state’s businesses that received funding – went to Trump in 2016. Meanwhile, seven of the bottom 10 states, where the lowest proportion of businesses received funding, went to Clinton. Taken together, 32% of businesses in states that Trump won got Paycheck Protection Program dollars, we found, compared with 22% of businesses in states that went to Clinton.”\nIt continued: “The figures were so stark that they sparked concerns of political interference. Rep. Jackie Speier, a California Democrat who serves on the House Oversight and Reform Committee, said the data raise questions about whether stimulus dollars were deliberately funneled to states that voted for Trump and have Republican governors.”\nThe story didn’t present any evidence of political meddling. Instead, it offered the results of several lines of data analysis that attempted to answer this central question: did red states get a bigger slice of the PPP pie than blue states?\nMostly, it used basic descriptive statistics, calculating rates, ranking states and computing averages. But the data set it used also presents an opportunity to use two slightly more advanced statistical analysis methods to look for patterns: linear regression, to examine relationships, and a t.test, to confirm the statistical validity of an average between two groups. So, let’s do that here.\nFirst, let’s load libraries. We’re going to load janitor, the tidyverse and a new package, corrr, which will help us do linear regression a bit easier than base R.\n\nlibrary(janitor)\nlibrary(tidyverse)\nlibrary(corrr)\n\nNow let’s load the data we’ll be using. It has five fields:\n\nstate_name\nvote_2016: whether Trump or Clinton won the state’s electoral vote.\npct_trump: the percentage of the vote Trump received in the state.\nbusinesses_receiving_ppe_pct: the percentage of the state’s small businesses that received a PPP loan.\nppe_amount_per_employee: the average amount of money provided by PPP per small business employee in the state.\n\n\nreveal_data <- read_rds(\"data/reveal_data.rds\")\n\nreveal_data\n\n# A tibble: 51 × 5\n   state_name   vote_2016 pct_trump businesses_receiving_ppe_pct ppe_amount_pe…¹\n   <chr>        <chr>         <dbl>                        <dbl>           <dbl>\n 1 North Dakota Trump          63.0                           58            7928\n 2 Nebraska     Trump          58.8                           56            7244\n 3 South Dakota Trump          61.5                           53            6541\n 4 Oklahoma     Trump          65.3                           50            6499\n 5 Mississippi  Trump          57.9                           49            5674\n 6 Iowa         Trump          51.2                           48            6642\n 7 Kansas       Trump          56.6                           47            7087\n 8 Hawaii       Clinton        29.4                           47            7417\n 9 Maine        Clinton        43.5                           45            6617\n10 Arkansas     Trump          60.6                           44            5549\n# … with 41 more rows, and abbreviated variable name ¹​ppe_amount_per_employee\n\n\n\n32 Linear Regression\nLet’s start with this question: did small businesses in states that voted more strongly for Trump get loans at higher rate than small businesses in Democratic states? We can answer it by examining the relationship or correlation between two variables, pct_trump and businesses_receiving_ppe_pct. How much do they move in tandem? Do states with more Trump support see bigger average PPP loans? Do extra Trumpy states get even more? Do super blue states get the least?\nLet’s start by plotting them to get a sense of the pattern.\n\nreveal_data %>%\n  ggplot() +\n  geom_point(aes(x=pct_trump,y=businesses_receiving_ppe_pct)) +\n  geom_smooth(aes(x=pct_trump,y=businesses_receiving_ppe_pct), method=\"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nIt’s a bit messy, but we can see something of a pattern here in the blob of dots. Generally, the dots are moving from the lower left (less Trumpy states that got loans at a lower rate) to upper right (red states that got loans at a higher rate). The blue “line of best fit” shows the general direction of the relationship.\nLet’s test another variable, the average amount of money provided by PPP per small business employee in the state.\n\nreveal_data %>%\n  ggplot() +\n  geom_point(aes(x=pct_trump,y=ppe_amount_per_employee)) +\n  geom_smooth(aes(x=pct_trump,y=ppe_amount_per_employee), method=\"lm\")\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\nThis one is a bit messier. There may be a slight upward slope in this blob of dots, but it’s not quite as apparent. It seems less certain that there’s a relationship between these two variables.\nWe can be a bit more precise by calculating a statistic called the correlation coefficient, also called “r”. r is a value between 1 and -1. An r of 1 indicates a strong positive correlation.\nAn increase in air temperature and air conditioning use at home is strongly-positively correlated: the hotter it gets, the more we have to use air conditioning. If we were to plot those two variables, we might not get 1, but we’d get close to it.\nAn r of -1 indicates a strong negative correlation. An increase in temperature and home heating use is strongly negatively correlated: the hotter it gets, the less heat we use indoors. We might not hit -1, but we’d probably get close to it.\nA correlation of 0 indicates no relationship.\nAll r values will fall somewhere on this scale, and how to interpret them isn’t always straightforward. They’re best used to give general guidance when exploring patterns.\nWe can calculate r with a function from the corrr package called “correlate()”. First, we remove the non-numeric values from our reveal_data (state name and a binary vote_2016 column), then we correlate.\n\nreveal_data %>%\n  select(-state_name, -vote_2016) %>%\n  correlate() %>%\n  select(term, pct_trump)\n\nCorrelation computed with\n• Method: 'pearson'\n• Missing treated using: 'pairwise.complete.obs'\n\n\n# A tibble: 3 × 2\n  term                         pct_trump\n  <chr>                            <dbl>\n1 pct_trump                       NA    \n2 businesses_receiving_ppe_pct     0.522\n3 ppe_amount_per_employee          0.221\n\n#glimpse(reveal_data)\n\nThe table this function produces generally confirms our interpretation of the two graphs above. The relationship between a state’s pct_trump and ppe_amount_per employee is positive, but at .22 (on a scale of -1 to 1), the relationship isn’t particularly strong. That’s why the second graphic above was messier than the first.\nThe relationship between businesses in a state receiving ppe and the state’s Trump vote is a bit stronger, if still moderate, .52 (on a scale of -1 to 1). Is this finding statistically valid? We can get a general sense of that by calculating the p-value of this correlation, a test of statistical significance. For that, we can use the cor.test function.\n\ncor.test(reveal_data$pct_trump, reveal_data$businesses_receiving_ppe_pct)\n\n\n    Pearson's product-moment correlation\n\ndata:  reveal_data$pct_trump and reveal_data$businesses_receiving_ppe_pct\nt = 4.2818, df = 49, p-value = 8.607e-05\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n 0.2875734 0.6971386\nsample estimates:\n      cor \n0.5218038 \n\n\nThis output is quite a bit uglier, but for our purposes there are two key pieces of information from this chunk of unfamiliar words. First, it shows the correlation calculated above: r 0.5218. Two, it shows the p-value, which is 0.00008607. That’s very low, as far as p-values go, which indicates that there’s a very slim chance that our finding is a statistical aberration.\nNow let’s test the other one, the relationship between the pct_trump and the ppe_amount_per_employee.\n\ncor.test(reveal_data$pct_trump, reveal_data$ppe_amount_per_employee)\n\n\n    Pearson's product-moment correlation\n\ndata:  reveal_data$pct_trump and reveal_data$ppe_amount_per_employee\nt = 1.5872, df = 49, p-value = 0.1189\nalternative hypothesis: true correlation is not equal to 0\n95 percent confidence interval:\n -0.05798429  0.46818515\nsample estimates:\n     cor \n0.221133 \n\n\nAgain, it shows our r value of .22, which was weaker. And the p-value here is a much larger 0.12. That indicates a higher chance of our finding being a statistical aberration, high enough that I wouldn’t rely on its validity.\np < .05 is accepted in many scientific disciplines – and by many data journalists – as the cutoff for statistical significance. But there’s heated debate about that level, and some academics question whether p-values should be relied on so heavily.\nAnd to be clear, a low p-value does not prove that we’ve found what we set out to find. There’s nothing on this graph or in the regression model output that proves that Trump’s administration tipped the scales in favor of states that voted for it. It’s entirely possible that there’s some other variable – or variables – not considered here that explain this pattern.\nAll we know is that we’ve identified a potentially promising pattern, worthy of additional reporting and analysis to flesh out.\n\n\n33 T-tests\nLet’s suppose we want to ask a related set of questions: did Trump states get higher ppp loan amounts per employee than states won by Clinton? Or did a larger percentage of businesses in states won by Trump receive, on average, a higher rate of PPP loans on average than states won by Clinton.\nWe can do this because, in our data, we have a column with two possible categorical values, Clinton or Trump, for each state.\nWe could just calculate the averages like we’re used to doing.\n\nreveal_data %>%\n  group_by(vote_2016) %>%\n  summarise(\n    mean_ppp_amount_per_employee = mean(ppe_amount_per_employee),\n    mean_businesses_receiving_ppe_pct = mean(businesses_receiving_ppe_pct)\n  )\n\n# A tibble: 2 × 3\n  vote_2016 mean_ppp_amount_per_employee mean_businesses_receiving_ppe_pct\n  <chr>                            <dbl>                             <dbl>\n1 Clinton                          5704.                              28.2\n2 Trump                            6021.                              37.2\n\n\nExamining this, it appears that in both categories there’s a difference.\nThe average amount of ppp loans per employee in Clinton states is smaller than Trump states (6,000 to 5,700). And the average percentage of businesses that got loans in Trump states was larger – 37% – than Clinton states – 28%. Should we report these as meaningful findings?\nA t-test can help us answer that question. It can tell us where there’s a statistically significant difference between the means of two groups. Have we found a real difference, or have we chanced upon a statistical aberration? Let’s see by calculating it for the average loan amount.\n\nt.test(ppe_amount_per_employee ~ vote_2016, data = reveal_data)\n\n\n    Welch Two Sample t-test\n\ndata:  ppe_amount_per_employee by vote_2016\nt = -1.2223, df = 36.089, p-value = 0.2295\nalternative hypothesis: true difference in means between group Clinton and group Trump is not equal to 0\n95 percent confidence interval:\n -843.7901  209.1329\nsample estimates:\nmean in group Clinton   mean in group Trump \n             5703.571              6020.900 \n\n\nWe see our two means, for Trump and Clinton, the same as we calculated above. The t-value is approximately 1, the p-value here is .2295, both of which should which should give us pause that we’ve identified something meaningful. More on t-tests here\nLet’s try the percentage of businesses getting ppp loans.\n\nt.test(businesses_receiving_ppe_pct ~ vote_2016, data = reveal_data)\n\n\n    Welch Two Sample t-test\n\ndata:  businesses_receiving_ppe_pct by vote_2016\nt = -3.182, df = 45.266, p-value = 0.002643\nalternative hypothesis: true difference in means between group Clinton and group Trump is not equal to 0\n95 percent confidence interval:\n -14.68807  -3.30241\nsample estimates:\nmean in group Clinton   mean in group Trump \n             28.23810              37.23333 \n\n\nThis is a bit more promising. T is much stronger – about 3 – and the p-value is .002. Both of these should give us assurance that we’ve found something statistically meaningful. Again, this doesn’t prove that Trump is stacking the deck for states. It just suggests there’s a pattern worth following up on."
  },
  {
    "objectID": "writingwithdata.html",
    "href": "writingwithdata.html",
    "title": "32  Writing with numbers",
    "section": "",
    "text": "The number one sin of all early career data journalist is to get really, really, really attached to the analysis you’ve done and include every number you find.\nDon’t do that.\nNumbers tell you what. Numbers rarely tell you why. What question has driven most people since they were three years old? Why. The very first thing to do is realize that is the purpose of reporting. You’ve done the analysis to determine the what. Now go do the reporting to do the why. Or as an old editor of mine used to say “Now go do that reporting shit you do.”\nThe trick to writing a numbers story is to frame your story around people. Sometimes, your lead can be a number, if that number is compelling. Often, your lead is a person, a person who is one of the numbers you are writing about.\nTell their story. Briefly. Then, let us hear from them. Let them speak about what it is you are writing about.\nThen come the numbers."
  },
  {
    "objectID": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "href": "writingwithdata.html#how-to-write-about-numbers-without-overwhelming-with-numbers.",
    "title": "32  Writing with numbers",
    "section": "32.1 How to write about numbers without overwhelming with numbers.",
    "text": "32.1 How to write about numbers without overwhelming with numbers.\nWriting complex stories is often a battle against that complexity. You don’t want to overwhelm. You want to simplify where you can. The first place you can do that is only use exact numbers where an exact number is called for.\nWhere you can, do the following:\n\nUsing ratios instead of percents\nOften, it’s better to put it in counts of 10. 6 of 10, 4 of 10. It’s easy to translate that from a percentage to a ratio.\nBut be careful when your number is 45 percent. Is that 4 in 10 or 5 in 10?\nIf a ratio doesn’t make sense, round. There’s 287,401 people in Lincoln, according to the Census Bureau. It’s easier, and no less accurate, to say there’s more than 287,000 people in Lincoln.\n\nA critical question your writing should answer: As compared to what?\nHow does this compare to the average? The state? The nation? The top? The bottom?\nOne of the most damning numbers in the series of stories Craig Pittman and I wrote that became the book Paving Paradise was comparing approvals and denials.\nWe were looking at the US Army Corps of Engineers and their permitting program. We were able to get a dataset of just a few years of permits that was relatively clean. From that, we were able to count the number of times the corps had said yes to a developer to wipe out wetlands the law protected and how many times they said no.\nThey said yes 12,000 times. They said no once.\nThat one time? Someone wanted to build an eco-lodge in the Everglades. Literally. Almost every acre of the property was wetlands. So in order to build it, the developer would have to fill in the very thing they were going to try to bring people into. The corps said no."
  },
  {
    "objectID": "writingwithdata.html#when-exact-numbers-matter",
    "href": "writingwithdata.html#when-exact-numbers-matter",
    "title": "32  Writing with numbers",
    "section": "32.2 When exact numbers matter",
    "text": "32.2 When exact numbers matter\nSometimes ratios and rounding are not appropriate.\nThis is being written in the days of the coronavirus. Case counts are where an exact number is called for. You don’t say that there are more than 70 cases in Lancaster County on the day this was written. You specify. It’s 75.\nYou don’t say almost 30 deaths. It’s 28.\nWhere this also comes into play is any time there are deaths: Do not round bodies."
  },
  {
    "objectID": "writingwithdata.html#an-example",
    "href": "writingwithdata.html#an-example",
    "title": "32  Writing with numbers",
    "section": "32.3 An example",
    "text": "32.3 An example\nRead this story from USA Today and the Arizona Republic. Notice first that the top sets up a conflict: People say one thing, and that thing is not true.\n\nNo one could have anticipated such a catastrophe, people said. The fire’s speed was unprecedented, the ferocity unimaginable, the devastation unpredictable.\n\n\nThose declarations were simply untrue. Though the toll may be impossible to predict, worst-case fires are a historic and inevitable fact.\n\nThe first voice you hear? An expert who studies wildfires.\n\nPhillip Levin, a researcher at the University of Washington and lead scientist for the Nature Conservancy in Washington, puts it this way: “Fire is natural. But the disaster happens because people didn’t know to leave, or couldn’t leave. It didn’t have to happen.”\n\nThen notice how they take what is a complex analysis using geographic information systems, raster analysis, the merging of multiple different datasets together and show that it’s quite simple – the averaging together of pixels on a 1-5 scale.\nThen, the compare what they found to a truly massive fire: The Paradise fire that burned 19,000 structures.\n\nAcross the West, 526 small communities — more than 10 percent of all places — rank higher.\n\nAnd that is how it’s done. Simplify, round, ratios: simple metrics, powerful results."
  }
]