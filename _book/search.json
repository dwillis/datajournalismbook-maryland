[
  {
    "objectID": "aggregates.html#libraries",
    "href": "aggregates.html#libraries",
    "title": "15  Aggregates",
    "section": "15.1 Libraries",
    "text": "15.1 Libraries\nR is a statistical programming language that is purpose built for data analysis.\nBase R does a lot, but there are a mountain of external libraries that do things to make R better/easier/more fully featured. We already installed the tidyverse – or you should have if you followed the instructions for the last assignment – which isn’t exactly a library, but a collection of libraries. Together, they make up the Tidyverse. Individually, they are extraordinarily useful for what they do. We can load them all at once using the tidyverse name, or we can load them individually. Let’s start with individually.\nThe two libraries we are going to need for this assignment are readr and dplyr. The library readr reads different types of data in. For this assignment, we’re going to read in csv data or Comma Separated Values data. That’s data that has a comma between each column of data.\nThen we’re going to use dplyr to analyze it.\nTo use a library, you need to import it. Good practice – one I’m going to insist on – is that you put all your library steps at the top of your notebooks.\nThat code looks like this:\n\nlibrary(readr)\n\nTo load them both, you need to do this:\n\nlibrary(readr)\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nBut, because those two libraries – and several others that we’re going to use over the course of this class – are so commonly used, there’s a shortcut to loading all of the libraries we’ll need:\n\nlibrary(tidyverse)\n\nYou can keep doing that for as many libraries as you need."
  },
  {
    "objectID": "aggregates.html#importing-data",
    "href": "aggregates.html#importing-data",
    "title": "15  Aggregates",
    "section": "15.2 Importing data",
    "text": "15.2 Importing data\nThe first thing we need to do is get some data to work with. We do that by reading it in. In our case, we’re going to read a datatable from an “rds” file, which is a format for storing data with R. Later in the course, we’ll more frequently work with a format called a CSV. A CSV is a stripped down version of a spreadsheet you might open in a program like Excel, in which each column is separated by a comma. RDS files are less common when getting data from other people. But reading in CSVs is less foolproof than reading in rds files, so for now we’ll work with rds.\nThe rds file we’re going to read in contains information about classes offered at the University of Maryland since 2017.\nSo step 1 is to import the data. The code to import the data looks like this:\numd_courses <- read_rds(\"umd_courses.rds\")\nLet’s unpack that.\nThe first part – umd_courses – is the name of a variable.\nA variable is just a name that we’ll use to refer to some more complex thing. In this case, the more complex thing is the data we’re importing into R that will be stored as a dataframe, which is one way R stores data.\nWe can call this variable whatever we want. The variable name doesn’t matter, technically. We could use any word. You could use your first name, if you like. Generally, though, we want to give variables names that are descriptive of the thing they refer to. Which is why we’re calling this one umd_courses. Variable names, by convention are one word all lower case (or two or more words connected by an underscore). You can end a variable with a number, but you can’t start one with a number.\nThe <- bit, you’ll recall from the basics, is the variable assignment operator. It’s how we know we’re assigning something to a word. Think of the arrow as saying “Take everything on the right of this arrow and stuff it into the thing on the left.” So we’re creating an empty vessel called umd_courses and stuffing all this data into it.\nread_rds() is a function, one that only works when we’ve loaded the tidyverse. A function is a little bit of computer code that takes in information and follows a series of pre-determined steps and spits it back out. A recipe to make pizza is a kind of function. We might call it make_pizza().\nThe function does one thing. It takes a preset collection of ingredients – flour, water, oil, cheese, tomato, salt – and passes them through each step outlined in a recipe, in order. Things like: mix flour and water and oil, knead, let it sit, roll it out, put tomato sauce and cheese on it, bake it in an oven, then take it out.\nThe output of our make pizza() function is a finished pie.\nWe’ll make use of a lot of pre-written functions from the tidyverse and other packages, and even write some of our own. Back to this line of code:\numd_courses <- read_rds(\"umd_courses.rds\")\nInside of the read_rds() function, we’ve put the name of the file we want to load. Things we put inside of function, to customize what the function does, are called arguments.\nThe easiest thing to do, if you are confused about how to find your data, is to put your data in the same folder as as your notebook (you’ll have to save that notebook first). If you do that, then you just need to put the name of the file in there (maryland_winred.rds). If you put your data in a folder called “data” that sits next to your data notebook, your function would instead look like this:\n\numd_courses <- read_rds(\"data/umd_courses.rds\")\n\nIn this data set, each row represents a course offered at UMD by a department during a specific term, with some other information optionally included, like seats in the class, the instructor(s) and a description.\nAfter loading the data, it’s a good idea to get a sense of its shape. What does it look like? There are several ways we can examine it.\nBy looking in the R Studio environment window, we can see the number of rows (called “obs.”, which is short for observations), and the number of columns (called variables). We can double click on the dataframe name in the environment window, and explore it like a spreadsheet.\nThere are several useful functions for getting a sense of the dataset right in our markdown document.\nIf we run glimpse(umd_courses), it will give us a list of the columns, the data type for each column and and the first few values for each column.\n\nglimpse(umd_courses)\n\nRows: 60,672\nColumns: 8\n$ id             <chr> \"LING889\", \"LING200\", \"LING689\", \"LING499\", \"LING899\", …\n$ title          <chr> \"Directed Research\", \"Introductory Linguistics\", \"Indep…\n$ description    <chr> NA, \"Credit only granted for: HESP120 or LING200.\\nAddi…\n$ term           <dbl> 202112, 202112, 202112, 202112, 202112, 202112, 202112,…\n$ department     <chr> \"Linguistics\", \"Linguistics\", \"Linguistics\", \"Linguisti…\n$ instructors    <chr> NA, \"Michelle Morrison\", NA, NA, NA, NA, NA, NA, NA, NA…\n$ seats          <dbl> 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 30, 45, 5, 1, 34, 29, 6,…\n$ syllabus_count <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n\n\nIf we type head(umd_courses), it will print out the columns and the first six rows of data.\n\nhead(umd_courses)\n\n# A tibble: 6 × 8\n  id      title   description   term department instructors seats syllabus_count\n  <chr>   <chr>   <chr>        <dbl> <chr>      <chr>       <dbl>          <dbl>\n1 LING889 Direct…  <NA>       202112 Linguisti… <NA>            0             NA\n2 LING200 Introd… \"Credit on… 202112 Linguisti… Michelle M…    20             NA\n3 LING689 Indepe…  <NA>       202112 Linguisti… <NA>            0             NA\n4 LING499 Direct…  <NA>       202112 Linguisti… <NA>            0             NA\n5 LING899 Doctor…  <NA>       202112 Linguisti… <NA>            0             NA\n6 LING698 Direct…  <NA>       202112 Linguisti… <NA>            0             NA\n\n\nWe can also click on the data name in the R Studio environment window to explore it interactively."
  },
  {
    "objectID": "aggregates.html#group-by-and-count",
    "href": "aggregates.html#group-by-and-count",
    "title": "15  Aggregates",
    "section": "15.3 Group by and count",
    "text": "15.3 Group by and count\nSo what if we wanted to know how many classes were offered in each term?\nTo do that by hand, we’d have to take each of the 60,672 individual rows (or observations or records) and sort them into a pile. We’d put them in groups – one for each term – and then count them.\ndplyr has a group by function in it that does just this. A massive amount of data analysis involves grouping like things together and then doing simple things like counting them, or averaging them together. So it’s a good place to start.\nSo to do this, we’ll take our dataset and we’ll introduce a new operator: |>. The best way to read that operator, in my opinion, is to interpret that as “and then do this.” This is called the “pipe operator” and it’s a huge part of writing R statements. So much so that there’s a keyboard shortcut for this: cmd-shift-m on the Mac and ctrl-shift-m on Windows. In order to enable this shortcut, you’ll need to set an option under Tools -> Global Options. Make sure you check the box that\n\nDon’t like that character? R also has one that does the same thing: %>%. They both work.\nWe’re going to establish a pattern that will come up again and again throughout this book: data |> function. In English: take your data set and then do this specific action to it.\nThe first step of every analysis starts with the data being used. Then we apply functions to the data.\nIn our case, the pattern that you’ll use many, many times is: data |> group_by(COLUMN NAME) |> summarize(VARIABLE NAME = AGGREGATE FUNCTION(COLUMN NAME))\nIn our dataset, the column with term information is called “term”\nHere’s the code to count the number of courses in each term:\n\numd_courses |>\n  group_by(term) |>\n  summarise(\n    count_classes = n()\n  )\n\n# A tibble: 22 × 2\n     term count_classes\n    <dbl>         <int>\n 1 201712           487\n 2 201801          4479\n 3 201805          1231\n 4 201808          4465\n 5 201905          1183\n 6 201908          4537\n 7 201912           516\n 8 202001          4648\n 9 202005          1357\n10 202008          4508\n# ℹ 12 more rows\n\n\nSo let’s walk through that.\nWe start with our dataset – umd_courses – and then we tell it to group the data by a given field in the data. In this case, we wanted to group together all the terms, signified by the field name term, which you could get from using the glimpse() function. After we group the data, we need to count them up.\nIn dplyr, we use the summarize() function, which can do alot more than just count things.\nInside the parentheses in summarize, we set up the summaries we want. In this case, we just want a count of the number of classes for each term grouping. The line of code count_classes = n(), says create a new field, called count_classes and set it equal to n(). n() is a function that counts the number of rows or records in each group. Why the letter n? The letter n is a common symbol used to denote a count of something. The number of things (or rows or observations or records) in a dataset? Statisticians call it n. There are n number of classes in this dataset.\nWhen we run that, we get a list of terms with a count next to them. But it’s not in any order.\nSo we’ll add another “and then do this” symbol – %>% – and use a new function called arrange(). Arrange does what you think it does – it arranges data in order. By default, it’s in ascending order – smallest to largest. But if we want to know the term with the most classes, we need to sort it in descending order. That looks like this:\n\numd_courses |>\n  group_by(term) |>\n  summarise(\n    count_classes = n()\n  ) |>\n  arrange(desc(count_classes))\n\n# A tibble: 22 × 2\n     term count_classes\n    <dbl>         <int>\n 1 202208          4975\n 2 202308          4785\n 3 202001          4648\n 4 202301          4578\n 5 202108          4545\n 6 201908          4537\n 7 202008          4508\n 8 202201          4493\n 9 201801          4479\n10 201808          4465\n# ℹ 12 more rows\n\n\nThe term labeled 202208, representing the Fall 2022 term, has the most classes.\nWe can, if we want, group by more than one thing. The courses data contains a column detailing the department.\nWe can group by “term” and “department” to see how many courses each department offered in each term. We’ll sort by department and term.\n\numd_courses |>\n  group_by(term, department) |>\n  summarise(\n    count_classes = n()\n  ) |>\n  arrange(term, department)\n\n`summarise()` has grouped output by 'term'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 3,237 × 3\n# Groups:   term [22]\n     term department                                   count_classes\n    <dbl> <chr>                                                <int>\n 1 201712 African American Studies                                 6\n 2 201712 Agricultural and Resource Economics                      1\n 3 201712 American Studies                                         4\n 4 201712 Animal Science                                           5\n 5 201712 Anthropology                                            25\n 6 201712 Applied Mathematics & Scientific Computation             3\n 7 201712 Arabic                                                   1\n 8 201712 Art History & Archaeology                                3\n 9 201712 Art Studio                                              13\n10 201712 Asian American Studies                                   1\n# ℹ 3,227 more rows"
  },
  {
    "objectID": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "href": "aggregates.html#other-summarization-methods-summing-mean-median-min-and-max",
    "title": "15  Aggregates",
    "section": "15.4 Other summarization methods: summing, mean, median, min and max",
    "text": "15.4 Other summarization methods: summing, mean, median, min and max\nIn the last example, we grouped like records together and counted them, but there’s so much more we can do to summarize each group.\nLet’s say we wanted to know the total number of seats offered in each term? For that, we could use the sum() function to add up all of the values in the column “seats”. We put the column we want to total – “seats” – inside the sum() function sum(seats). Note that we can simply add a new summarize function here, keeping our count_classes field in our output table.\n\numd_courses |>\n  group_by(term) |>\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats)\n  ) |>\n  arrange(desc(total_seats))\n\n# A tibble: 22 × 3\n     term count_classes total_seats\n    <dbl>         <int>       <dbl>\n 1 202208          4975      230796\n 2 202108          4545      223321\n 3 202308          4785      221036\n 4 202008          4508      217314\n 5 201908          4537      217075\n 6 201808          4465      208839\n 7 202301          4578      207465\n 8 202201          4493      206075\n 9 202101          4400      204248\n10 202001          4648      202688\n# ℹ 12 more rows\n\n\nWe can also calculate the average number of seats for each department – the mean – and the number that sits at the midpoint of our data – the median.\n\numd_courses |>\n  group_by(term) |>\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats),\n    mean_seats = mean(seats),\n    median_seats = median(seats)\n  ) |>\n  arrange(desc(total_seats))\n\n# A tibble: 22 × 5\n     term count_classes total_seats mean_seats median_seats\n    <dbl>         <int>       <dbl>      <dbl>        <dbl>\n 1 202208          4975      230796       46.4           22\n 2 202108          4545      223321       49.1           25\n 3 202308          4785      221036       46.2           24\n 4 202008          4508      217314       48.2           22\n 5 201908          4537      217075       47.8           24\n 6 201808          4465      208839       46.8           20\n 7 202301          4578      207465       45.3           25\n 8 202201          4493      206075       45.9           24\n 9 202101          4400      204248       46.4           24\n10 202001          4648      202688       43.6           20\n# ℹ 12 more rows\n\n\nWe see something interesting here. The mean number of seats is higher than the median number in most cases, but the difference isn’t always huge. In some cases the mean gets skewed by larger or lower amounts. Examining both the median – which is less sensitive to extreme values – and the mean – which is more sensitive to extreme values – gives you a clearer picture of the composition of the data.\nWhat about the highest and lowest number of seats for each department? For that, we can use the min() and max() functions.\n\numd_courses |>\n  group_by(department) |>\n  summarise(\n    count_classes = n(),\n    total_seats = sum(seats),\n    mean_seats = mean(seats),\n    median_seats = median(seats),\n    min_seats = min(seats),\n    max_seats = max(seats)\n  ) |>\n  arrange(desc(total_seats))\n\n# A tibble: 211 × 7\n   department        count_classes total_seats mean_seats median_seats min_seats\n   <chr>                     <int>       <dbl>      <dbl>        <dbl>     <dbl>\n 1 Business and Man…          1900      162543       85.5           45         0\n 2 Computer Science           1067      118136      111.            45         0\n 3 Mathematics                1030      106235      103.            27         0\n 4 English                    1617      102124       63.2           23         0\n 5 Biological Scien…          1199       97653       81.4           31         0\n 6 Chemistry                   669       85176      127.            38         0\n 7 Information Stud…          1127       78820       69.9           40         0\n 8 Engineering Scie…           609       78807      129.            45         0\n 9 Economics                  1137       71420       62.8           35         0\n10 Communication              1440       70165       48.7           30         0\n# ℹ 201 more rows\n# ℹ 1 more variable: max_seats <dbl>\n\n\nFrom this, we can see that every department has courses with 0 seats listed. We’ll need to account for those in any analysis, probably by excluding them. The variation in mean and median seats is interesting, too: look at Mathematics and English compared to Computer Science and Engineering, for example. These patterns aren’t random: departments make choices about the courses they offer.\nIt would be interesting to see what the course with the largest number of seats is. To do that, we could simply take our original data set and sort it from highest to lowest on seats\n\numd_courses |>\n  arrange(desc(seats))\n\n# A tibble: 60,672 × 8\n   id      title  description   term department instructors seats syllabus_count\n   <chr>   <chr>  <chr>        <dbl> <chr>      <chr>       <dbl>          <dbl>\n 1 UNIV100 The S… \"Credit on… 201808 Universit… Ciarra Joy…  2876             NA\n 2 UNIV100 The S… \"Credit on… 202108 Universit… Raymond Na…  2685             NA\n 3 UNIV100 The S… \"Credit on… 202208 Universit… Vickie Hil…  2575             NA\n 4 ENES140 Disco… \"Additiona… 202201 Engineeri… James Green  2500             NA\n 5 ENES140 Disco… \"Additiona… 202101 Engineeri… James Green  2500             NA\n 6 ENES140 Disco… \"Additiona… 202001 Engineeri… James Green  2500             NA\n 7 ENES140 Disco… \"Additiona… 201801 Engineeri… James Green  2500             NA\n 8 ENES140 Disco… \"Additiona… 202301 Engineeri… James Green  2500              1\n 9 UNIV100 The S… \"Credit on… 202308 Universit… Maiya Smit…  2409              4\n10 UNIV100 The S… \"Credit on… 202008 Universit… Raymond Na…  2233             NA\n# ℹ 60,662 more rows\n\n\nUNIV100 and ENES140 have a LOT of seats. Are they virtual classes? Hybrid? What else could we ask of this data?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "",
    "text": "1 Introduction\nIf you were at all paying attention in pre-college science classes, you have probably seen this equation:\nIn English, that says we can know how far something has traveled if we know how fast it’s going and for how long. If we multiply the rate by the time, we’ll get the distance.\nIf you remember just a bit about algebra, you know we can move these things around. If we know two of them, we can figure out the third. So, for instance, if we know the distance and we know the time, we can use algebra to divide the distance by the time to get the rate.\nIn 2012, the South Florida Sun Sentinel found a story in this formula.\nPeople were dying on South Florida tollways in terrible car accidents. What made these different from other car fatal car accidents that happen every day in the US? Police officers driving way too fast were causing them.\nBut do police regularly speed on tollways or were there just a few random and fatal exceptions?\nThanks to Florida’s public records laws, the Sun Sentinel got records from the toll transponders in police cars in south Florida. The transponders recorded when a car went through a given place. And then it would do it again. And again.\nGiven that those places are fixed – they’re toll plazas – and they had the time it took to go from one toll plaza to another, they had the distance and the time.\nIt took high school algebra to find how fast police officers were driving. And the results were shocking.\nTwenty percent of police officers had exceeded 90 miles per hour on toll roads. In a 13-month period, officers drove between 90 and 110 mph more than 5,000 times. And these were just instances found on toll roads. Not all roads have tolls.\nThe story was a stunning find, and the newspaper documented case after case of police officers violating the law and escaping punishment. And, in 2013, they won the Pulitzer Prize for Public Service.\nAll with simple high school algebra."
  },
  {
    "objectID": "index.html#modern-data-journalism",
    "href": "index.html#modern-data-journalism",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.1 Modern data journalism",
    "text": "1.1 Modern data journalism\nIt’s a single word in a single job description, but a Buzzfeed job posting in 2017 is another indicator in what could be a profound shift in how data journalism is both practiced and taught.\n“We’re looking for someone with a passion for news and a commitment to using data to find amazing, important stories — both quick hits and deeper analyses that drive conversations,” the posting seeking a data journalist says. It goes on to list five things BuzzFeed is looking for: Excellent collaborator, clear writer, deep statistical understanding, knowledge of obtaining and restructuring data.\nAnd then there’s this:\n“You should have a strong command of at least one toolset that (a) allows for filtering, joining, pivoting, and aggregating tabular data, and (b) enables reproducible workflows.”\nThis is not the data journalism of 20 years ago. When it started, it was a small group of people in newsrooms using spreadsheets and databases. Data journalism now encompases programming for all kinds of purposes, product development, user interface design, data visualization and graphics on top of more traditional skills like analyzing data and writing stories.\nIn this book, you’ll get a taste of modern data journalism through programming in R, a statistics language. You’ll be challenged to think programmatically while thinking about a story you can tell to readers in a way that they’ll want to read. They might seem like two different sides of the brain – mutually exclusive skills. They aren’t. I’m confident you’ll see programming is a creative endeavor and storytelling can be analytical.\nCombining them together has the power to change policy, expose injustice and deeply inform."
  },
  {
    "objectID": "index.html#installations",
    "href": "index.html#installations",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.2 Installations",
    "text": "1.2 Installations\nThis book is all in the R statistical language. To follow along, you’ll do the following:\n\nInstall the R language on your computer. Go to the R Project website, click download R and select a mirror closest to your location. Then download the version for your computer.\nInstall R Studio Desktop. The free version is great.\n\nGoing forward, you’ll see passages like this:\n\ninstall.packages(\"tidyverse\")\n\nThat is code that you’ll need to run in your R Studio. When you see that, you’ll know what to do."
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.3 About this book",
    "text": "1.3 About this book\nThis book is the collection of class materials originally written for Matt Waite’s Data Journalism class at the University of Nebraska-Lincoln’s College of Journalism and Mass Communications. It has been substantially updated by Derek Willis and Sean Mussenden for data journalism classes at the University of Maryland Philip Merrill College of Journalism, with contributions from Sarah Cohen of Arizona State University.\nThere’s some things you should know about it:\n\nIt is free for students.\nThe topics will remain the same but the text is going to be constantly tinkered with.\nWhat is the work of the authors is copyright Matt Waite 2020, Sarah Cohen 2022 and Derek Willis and Sean Mussenden 2023.\nThe text is Attribution-NonCommercial-ShareAlike 4.0 International Creative Commons licensed. That means you can share it and change it, but only if you share your changes with the same license and it cannot be used for commercial purposes. I’m not making money on this so you can’t either.\n\nAs such, the whole book – authored in Quarto – in its original form is open sourced on Github. Pull requests welcomed!"
  },
  {
    "objectID": "index.html#what-well-cover",
    "href": "index.html#what-well-cover",
    "title": "Data Journalism with R and the Tidyverse",
    "section": "1.4 What we’ll cover",
    "text": "1.4 What we’ll cover\n\nGoogle Sheets\nPublic records and open data\nR Basics\nReplication\nData basics and structures\nAggregates\nMutating\nWorking with dates\nFilters\nCleaning I: Data smells\nCleaning II: Janitor\nCleaning III: Open Refine\nCleaning IV: Pulling Data from PDFs\nJoins\nBasic data scraping\nGetting data from APIs: Census\nVisualizing for reporting: Basics\nVisualizing for reporting: Publishing\nGeographic data basics\nGeographic queries\nGeographic visualization\nText analysis basics\nBasic statistics\nWriting with and about data\nData journalism ethics"
  },
  {
    "objectID": "filters.html#combining-filters",
    "href": "filters.html#combining-filters",
    "title": "16  Filters and selections",
    "section": "16.1 Combining filters",
    "text": "16.1 Combining filters\nSo let’s say we wanted to see all the courses in the Theatre department with at least 15 seats. We can do this a number of ways. The first is we can chain together a whole lot of filters.\n\ntheatre_seats_15 <- umd_courses |> filter(department == \"Theatre\") |> filter(seats >= 15)\n\nnrow(theatre_seats_15)\n\n[1] 231\n\n\nThat gives us 231 records But that’s repetitive, no? We can do better using a single filter and boolean operators – AND and OR. In this case, AND is & and OR is |.\nThe difference? With AND, all conditions must be true to be included. With OR, any of those conditions things can be true and it will be included.\nHere’s the difference.\n\nand_theatre_seats_15 <- umd_courses |> filter(department == \"Theatre\" & seats >= 15)\n\nnrow(and_theatre_seats_15)\n\n[1] 231\n\n\nSo AND gives us the same answer we got before. What does OR give us?\n\nand_theatre_seats_15 <- umd_courses |> filter(department == \"Theatre\" | seats >= 15)\n\nnrow(and_theatre_seats_15)\n\n[1] 41011\n\n\nSo there’s 41,011 rows that are EITHER Theatre classes OR have at least 15 seats. OR is additive; AND is restrictive.\nA general tip about using filter: it’s easier to work your way towards the filter syntax you need rather than try and write it once and trust the result. Each time you modify your filter, check the results to see if they make sense. This adds a little time to your process but you’ll thank yourself for doing it because it helps avoid mistakes."
  }
]