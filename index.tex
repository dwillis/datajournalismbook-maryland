% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[many]{tcolorbox}}
\makeatother
\makeatletter
\@ifundefined{shadecolor}{\definecolor{shadecolor}{rgb}{.97, .97, .97}}
\makeatother
\makeatletter
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Data Journalism with R and the Tidyverse},
  pdfauthor={Matt Waite \& Sarah Cohen (original authors); updated by Derek Willis and Sean Mussenden},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{Data Journalism with R and the Tidyverse}
\usepackage{etoolbox}
\makeatletter
\providecommand{\subtitle}[1]{% add subtitle to \maketitle
  \apptocmd{\@title}{\par {\large #1 \par}}{}{}
}
\makeatother
\subtitle{Code, data and visuals for storytellers}
\author{Matt Waite \& Sarah Cohen (original authors); updated by Derek
Willis and Sean Mussenden}
\date{1/23/2023}

\begin{document}
\maketitle
\ifdefined\Shaded\renewenvironment{Shaded}{\begin{tcolorbox}[boxrule=0pt, borderline west={3pt}{0pt}{shadecolor}, sharp corners, frame hidden, interior hidden, enhanced, breakable]}{\end{tcolorbox}}\fi

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

If you were at all paying attention in pre-college science classes, you
have probably seen this equation:

\begin{verbatim}
d = rt or distance = rate*time
\end{verbatim}

In English, that says we can know how far something has traveled if we
know how fast it's going and for how long. If we multiply the rate by
the time, we'll get the distance.

If you remember just a bit about algebra, you know we can move these
things around. If we know two of them, we can figure out the third. So,
for instance, if we know the distance and we know the time, we can use
algebra to divide the distance by the time to get the rate.

\begin{verbatim}
d/t = r or distance/time = rate
\end{verbatim}

In 2012, the South Florida Sun Sentinel found a story in this formula.

People were dying on South Florida tollways in terrible car accidents.
What made these different from other car fatal car accidents that happen
every day in the US? Police officers driving way too fast were causing
them.

But do police regularly speed on tollways or were there just a few
random and fatal exceptions?

Thanks to Florida's public records laws, the Sun Sentinel got records
from the toll transponders in police cars in south Florida. The
transponders recorded when a car went through a given place. And then it
would do it again. And again.

Given that those places are fixed -- they're toll plazas -- and they had
the time it took to go from one toll plaza to another, they had the
distance and the time.

\href{http://www.sun-sentinel.com/news/local/speeding-cops/fl-speeding-cops-20120211,0,3706919.story}{It
took high school algebra to find how fast police officers were driving.
And the results were shocking.}

Twenty percent of police officers had exceeded 90 miles per hour on toll
roads. In a 13-month period, officers drove between 90 and 110 mph more
than 5,000 times. And these were just instances found on toll roads. Not
all roads have tolls.

The story was a stunning find, and the newspaper documented case after
case of police officers violating the law and escaping punishment. And,
in 2013, they won the Pulitzer Prize for Public Service.

All with simple high school algebra.

\hypertarget{modern-data-journalism}{%
\section{Modern data journalism}\label{modern-data-journalism}}

It's a single word in a single job description, but a Buzzfeed job
posting in 2017 is another indicator in what could be a profound shift
in how data journalism is both practiced and taught.

``We're looking for someone with a passion for news and a commitment to
using data to find amazing, important stories --- both quick hits and
deeper analyses that drive conversations,'' the posting seeking a data
journalist says. It goes on to list five things BuzzFeed is looking for:
Excellent collaborator, clear writer, deep statistical understanding,
knowledge of obtaining and restructuring data.

And then there's this:

\textbf{``You should have a strong command of at least one toolset that
(a) allows for filtering, joining, pivoting, and aggregating tabular
data, and (b) enables reproducible workflows.''}

This is not the data journalism of 20 years ago. When it started, it was
a small group of people in newsrooms using spreadsheets and databases.
Data journalism now encompases programming for all kinds of purposes,
product development, user interface design, data visualization and
graphics on top of more traditional skills like analyzing data and
writing stories.

In this book, you'll get a taste of modern data journalism through
programming in R, a statistics language. You'll be challenged to think
programmatically while thinking about a story you can tell to readers in
a way that they'll want to read. They might seem like two different
sides of the brain -- mutually exclusive skills. They aren't. I'm
confident you'll see programming is a creative endeavor and storytelling
can be analytical.

Combining them together has the power to change policy, expose injustice
and deeply inform.

\hypertarget{installations}{%
\section{Installations}\label{installations}}

This book is all in the R statistical language. To follow along, you'll
do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Install the R language on your computer. Go to the
  \href{https://www.r-project.org/}{R Project website}, click download R
  and select a mirror closest to your location. Then download the
  version for your computer.
\item
  Install \href{https://www.rstudio.com/products/rstudio/\#Desktop}{R
  Studio Desktop}. The free version is great.
\end{enumerate}

Going forward, you'll see passages like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

That is code that you'll need to run in your R Studio. When you see
that, you'll know what to do.

\hypertarget{about-this-book}{%
\section{About this book}\label{about-this-book}}

This book is the collection of class materials originally written for
Matt Waite's Data Journalism class at the University of
Nebraska-Lincoln's College of Journalism and Mass Communications. It has
been substantially updated by Derek Willis and Sean Mussenden for data
journalism classes at the University of Maryland Philip Merrill College
of Journalism, with contributions from Sarah Cohen of Arizona State
University.

There's some things you should know about it:

\begin{itemize}
\tightlist
\item
  It is free for students.
\item
  The topics will remain the same but the text is going to be constantly
  tinkered with.
\item
  What is the work of the authors is copyright Matt Waite 2020, Sarah
  Cohen 2022 and Derek Willis and Sean Mussenden 2023.
\item
  The text is
  \href{https://creativecommons.org/licenses/by-nc-sa/4.0/}{Attribution-NonCommercial-ShareAlike
  4.0 International} Creative Commons licensed. That means you can share
  it and change it, but only if you share your changes with the same
  license and it cannot be used for commercial purposes. I'm not making
  money on this so you can't either.\\
\item
  As such, the whole book -- authored in Quarto -- in its original form
  is \href{https://github.com/dwillis/datajournalismbook-maryland}{open
  sourced on Github}. Pull requests welcomed!
\end{itemize}

\hypertarget{what-well-cover}{%
\section{What we'll cover}\label{what-well-cover}}

\begin{itemize}
\tightlist
\item
  Google Sheets
\item
  Public records and open data
\item
  R Basics
\item
  Replication
\item
  Data basics and structures
\item
  Aggregates
\item
  Mutating
\item
  Working with dates
\item
  Filters
\item
  Cleaning I: Data smells
\item
  Cleaning II: Janitor
\item
  Cleaning III: Open Refine
\item
  Cleaning IV: Pulling Data from PDFs
\item
  Joins
\item
  Basic data scraping
\item
  Getting data from APIs: Census
\item
  Visualizing for reporting: Basics
\item
  Visualizing for reporting: Publishing
\item
  Geographic data basics
\item
  Geographic queries
\item
  Geographic visualization
\item
  Text analysis basics
\item
  Basic statistics
\item
  Writing with and about data
\item
  Data journalism ethics
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{start-story}{%
\chapter{Learn a new way to read}\label{start-story}}

Getting started in data journalism often feels as if you've left the
newsroom and entered the land of statistics, computer programming and
data science. This chapter will help you start seeing data reporting in
a new way, by learning how to study great works of the craft as a writer
rather than a reader.

\begin{figure}

{\centering \includegraphics[width=0.9\textwidth,height=\textheight]{./images/start-story-cobb.png}

}

\caption{jelani cobb}

\end{figure}

\href{https://twitter.com/jelani9/status/1473046796205117443}{Jelani
Cobb} tweeted, ``an engineer doesn't look at a bridge the same way
pedestrians or drivers do.'' They see it as a ``language of angles and
load bearing structures.'' We just see a bridge. While he was referring
to long-form writing, reporting with data can also be learned by example
-- if you spend enough time with the examples.

Almost all good writers and reporters try to learn from exemplary work.
I know more than one reporter who studies prize-winning journalism to
hone their craft. This site will have plenty of examples, but you should
stay on the lookout for others.

\hypertarget{read-like-a-reporter}{%
\section{Read like a reporter}\label{read-like-a-reporter}}

Try to approach data or empirical reporting as a reporter first, and a
consumer second. The goal is to triangulate how the story was
discovered, reported and constructed. You'll want to think about why
\emph{this} story, told this way, at this time, was considered
newsworthy enough to publish when another approach on the same topic
might not have been.

\hypertarget{what-were-the-questions}{%
\subsection*{What were the questions?}\label{what-were-the-questions}}
\addcontentsline{toc}{subsection}{What were the questions?}

In data journalism, we often start with a tip, or a hypothesis.
Sometimes it's a simple question. Walt Bogdanich of The New York Times
is renowned for seeing stories around every corner. Bogdanich has said
that the prize-winning story
``\href{https://www.nytimes.com/2008/09/21/nyregion/21lirr.html}{A
Disability Epidemic Among a Railroad's Retirees}'' came from a simple
question he had when railway workers went on strike over pension
benefits -- how much were they worth? The story led to an FBI
investigation and arrests, along with pension reform at the largest
commuter rail in the country.

The hypothesis for some stories might be more directed. In 2021, the
Howard Center for Investigative Journalism at ASU published
``\href{https://cronkitenews.azpbs.org/littlevictims/little-victims-everywhere/chapter-1.html}{Little
victims everywhere}'', a set of stories on the lack of justice for
survivors of child sexual assault on Native American reservations. That
story came after previous reporters for the center analyzed data from
the Justice Department showing that the FBI dropped most of the cases it
investigated, and the Justice Department then only prosecuted about half
of the matters referred to it by investigators. The hypothesis was that
they were rarely pursued because federal prosecutors -- usually focused
on immigration, white collar crime and drugs -- weren't as prepared to
pursue violent crime in Indian Country.

When studying a data-driven investigation, try to imagine what the
reporters were trying to prove or disprove, and what they used to do it.
In journalism, we rely on a mixture of quantitative and qualitative
methods. It's not enough to prove the ``numbers'' or have the
statistical evidence. That is just the beginning of the story. We are
supposed to ground-truth them with the stories of actual people and
places.

\hypertarget{go-beyond-the-numbers}{%
\subsection*{Go beyond the numbers}\label{go-beyond-the-numbers}}
\addcontentsline{toc}{subsection}{Go beyond the numbers}

It's easy to focus on the numbers or statistics that make up the key
findings, or the reason for the story. Some reporters make the mistake
of thinking all of the numbers came from the same place -- a rarity in
most long-form investigations. Instead, the sources have been woven
together and are a mix of original research and research done by others.
Try to pay attention to any sourcing done in the piece. Sometimes, it
will tell you that the analysis was original. Other times it's more
subtle.

But don't just look at the statistics being reported in the story. In
many (most?) investigations, some of the key people, places or time
elements come directly from a database.

When I was analyzing Paycheck Protection Program loan data for
ProPublica, one fact hit me as I was looking at a handful of
sketchy-looking records: a lot of them were from a single county in
coastal New Jersey. It turned out to be a
\href{https://www.propublica.org/article/ppp-farms}{pretty good story}.

Often, the place that a reporter visits is determined by examples found
in data. In \href{pdfs/rural_development.pdf}{this story on rural
development} funds, all of the examples came from an analysis of the
database. Once the data gave us a good lead, the reporters examined
press releases and other easy-to-get sources before calling and visiting
the recipients or towns.

\hypertarget{reading-tips}{%
\section{Reading tips}\label{reading-tips}}

You'll get better at reading investigations and data-driven work over
time, but for now, remember to go beyond the obvious:

\begin{itemize}
\item
  Where might the reporters have found their key examples, and what made
  them good characters or illustrations of the larger issue? Could they
  have come from the data?
\item
  What do you think came first -- a narrative single example that was
  broadened by data (naively, qualitative method), or a big idea that
  was illustrated with characters (quantitative method)?
\item
  What records were used? Were they public records, leaks, or
  proprietary data?
\item
  What methods did they use? Did they do their own testing, use
  statistical analysis, or geographic methods? You won't always know,
  but look for a methodology section or a description alongside each
  story.
\item
  How might you localize or adapt these methods to find your own
  stories?
\item
  Pick out the key findings (usually in the nut graf or in a series of
  bullets after the opening chapter): are they controversial? How might
  they have been derived? What might have been the investigative
  hypothesis? Have they given critics their due and tried to falsify
  their own work?
\item
  How effective is the writing and presentation of the story? What makes
  it compelling journalism rather than a dry study? How might you have
  done it differently? Is a video story better told in text, or would a
  text story have made a good documentary? Are the visual elements well
  integrated? Does the writing draw you in and keep you reading? Think
  about structure, story length, entry points and graphics all working
  together.
\item
  Are you convinced? Are there holes or questions that didn't get
  addressed?
\end{itemize}

\hypertarget{analyze-data-for-story-not-study}{%
\section{Analyze data for story, not
study}\label{analyze-data-for-story-not-study}}

As journalists we'll often be using data, social science methods and
even interviewing differently than true experts. We're seeking stories,
not studies. Recognizing news in data is one of the hardest skills for
less experienced reporters new to data journalism. This list of
potential newsworthy data points is adapted from Paul Bradshaw's
``\href{https://leanpub.com/DataJournalismHeist}{Data Journalism
Heist}''.

\includegraphics[width=0.9\textwidth,height=\textheight]{./images/start-story-heist.png}

\begin{itemize}
\tightlist
\item
  Compare the claims of powerful people and institutions against facts
  -- the classic investigative approach.
\item
  Report on \emph{unexpected} highs and lows (of change, or of some
  other characteristic)
\item
  Look for outliers -- individual values that buck a trend seen in the
  rest
\item
  Verify or bust some myths
\item
  Find signs of distress, happiness or dishonesty or any other emotion.
\item
  Uncover \emph{new} or \emph{under-reported} long-term trends.
\item
  Find data suggesting your area is \emph{the same} or \emph{different}
  than most others of its kind.
\end{itemize}

Bradshaw also did a recent study of data journalism pieces:
``\href{https://onlinejournalismblog.com/2020/08/11/here-are-the-7-types-of-stories-most-often-found-in-data/}{Here
are the angles journalists use most often to tell the stories in
data}'', in Online Journalism Blog. I'm not sure I agree, only because
he's looking mainly at visualizations rather than stories, but they're
worth considering.

\hypertarget{exercises}{%
\section{Exercises}\label{exercises}}

\begin{itemize}
\item
  If you're a member of Investigative Reporters and Editors, go to the
  site and find a recent prize-winning entry (usually text rather than
  broadcast). Get a copy of the IRE contest entry from the Resources
  page. Try to match up what the reporters said they did and how they
  did it with key portions of the story.
\item
  The next time you find a good data source, try to find a story that
  references it. If your data is local, you might look for a story that
  used similar data elsewhere, such as 911 response times or overdose
  deaths. But many stories use federal datasets that can easily be
  localized. Look at a description of the dataset and then the story to
  see how the data might have been used.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{start-math}{%
\chapter{Newsroom math}\label{start-math}}

\begin{quote}
\emph{Statistics are people with the tears washed off}

- Paul Brodeur
\end{quote}

Jo Craven McGinty, then of The New York Times, used simple rates and
ratios to discover that a 6-story brick New Jersey hospital was the most
expensive in the nation. In 2012, Bayonne Medical Center ``charged the
highest amounts in the country for nearly one-quarter of the most common
hospital treatments,'' the
\href{https://www.nytimes.com/2013/05/17/business/bayonne-medical-center-has-highest-us-billing-rates.html}{Times
story said}.

To do this story, McGinty only needed to know the number of the
procedures reported to the government and the total amount each hospital
charged. Dividing those to find an average price, then ranking the most
common procedures, led to this surprising result.

\hypertarget{why-numbers}{%
\section{Why numbers?}\label{why-numbers}}

Using averages, percentages and percent change is the bread and butter
of data journalism, leading to stories ranging from home price
comparisons to school reports and crime trends. It may have been
charming at one time for reporters to announce that they didn't ``do''
math, but no longer. Instead, it is now an announcement that the
reporter can only do some of the job. You will never be able to tackle
complicated, in-depth stories without reviewing basic math.

The good news is that most of the math and statistics you need in a
newsroom isn't nearly as difficult as high school algebra. You learned
it somewhere around the 4th grade. You then had a decade to forget it
before deciding you didn't like math. But mastering this most basic
arithmetic again is a requirement in the modern age.

In working with typical newsroom math, you will need to learn how to:

\begin{itemize}
\tightlist
\item
  Overcome your fear of numbers
\item
  Integrate numbers into your reporting
\item
  Routinely compute averages, differences and rates
\item
  Simplify and select the right numbers for your story
\end{itemize}

While this chapter covers general tips, you can find specific
instructions for typical newsroom math in this
\protect\hyperlink{appendix-math}{Appendix A}

\hypertarget{overcoming-your-fear-of-math}{%
\section{Overcoming your fear of
math}\label{overcoming-your-fear-of-math}}

When we learned to read, we got used to the idea that 26 letters in
American English could be assembled into units that we understand
without thinking -- words, sentences, paragraphs and books. We never got
the same comfort level with 10 digits, and neither did our audience.

Think of your own reaction to seeing a page of words. Now imagine it as
a page of numbers.

Instead, picture the number ``five''. It's easy. It might be fingers or
it might be a team on a basketball court. But it's simple to understand.

Now picture the number 275 million. It's hard. Unfortunately, 275
billion isn't much harder, even though it's magnitudes larger. (A
million seconds goes by in about 11 days but you may not have been alive
for a billion seconds -- about 36 years.)

The easiest way to get used to some numbers is to learn ways to cut them
down to size by calculating rates, ratios or percentages. In your
analysis, keep an eye out for the simplest \emph{accurate} way to
characterize the numbers you want to use. ``Characterize'' is the
important word here -- it's not usually necessary to be overly precise
so long as your story doesn't hinge on a nuanced reading of small
differences. (And is anything that depends on that news? It may not be.)

Here's one example of putting huge numbers in perspective. Pay attention
to what you really can picture - it's probably the \$21 equivalent.

The Chicago hedge fund billionaire Kenneth C. Griffin, for example,
earns about \$68.5 million a month after taxes, according to court
filings made by his wife in their divorce. He has given a total of
\$300,000 to groups backing Republican presidential candidates. That is
a huge sum on its face, yet is the equivalent of only \$21.17 for a
typical American household, according to Congressional Budget Office
data on after-tax income.
\emph{``\href{https://www.nytimes.com/interactive/2015/10/11/us/politics/2016-presidential-election-super-pac-donors.html}{Buying
Power}'', Nicholas Confessore, Sarah Cohen and Karen Yourish, The New
York Times, October 2015}

Originally the reporters had written it even more simply, but editors
found the facts so unbelievable that they wanted give readers a chance
to do the math themselves. That's reasonable, but here's an even simpler
way to say it: ``earned nearly \$1 billion after taxes\ldots He has
given \$300,000 to groups backing candidates, the equivalent of a dinner
at Olive Garden for the typical American family , based on Congressional
Budget Office income data.'' (And yes, the reporter checked the price
for an Olive Garden meal at the time for four people.)

\hypertarget{put-math-in-its-place}{%
\section{Put math in its place}\label{put-math-in-its-place}}

For journalists, numbers -- or facts -- make up the third leg of a stool
supported by human stories or anecdotes, and insightful comment from
experts. They serve us in three ways:

\begin{itemize}
\item
  \textbf{\emph{As summaries}}. Almost by definition, a number counts
  something, averages something, or otherwise summarizes something.
  Sometimes, it does a good job, as in the average height of Americans.
  Sometimes it does a terrible job, as in the average income of
  Americans. Try to find summaries that accurately characterize the real
  world.
\item
  \textbf{\emph{As opinions}}. Sometimes it's an opinion derived after
  years of impartial study. Sometimes it's an opinion tinged with
  partisan or selective choices of facts. Use them accordingly.
\item
  \textbf{\emph{As guesses}}. Sometimes it's a good guess, sometimes
  it's an off-the-cuff guess. And sometimes it's a hopeful guess. Even
  when everything is presumably counted many times, it's still a (very
  nearly accurate) guess. Yes, the ``audits'' of presidential election
  results in several states in 2021 found a handful of errors -- not a
  meaningful number, but a few just the same.
\end{itemize}

Once you find the humanity in your numbers, by cutting them down to size
and relegating them to their proper role, you'll find yourself less
fearful. You'll be able to characterize what you've learned rather than
numb your readers with every number in your notebook. You may even find
that finding facts on your own is fun.

\hypertarget{going-further}{%
\section{Going further}\label{going-further}}

\hypertarget{tipsheets}{%
\subsection*{Tipsheets}\label{tipsheets}}
\addcontentsline{toc}{subsection}{Tipsheets}

\begin{itemize}
\tightlist
\item
  Steve Doig's
  ``\href{https://businessjournalism.org/2017/09/newsroom-math-crib-sheet/}{Math
  Crib Sheet}''
\item
  \protect\hyperlink{appendix-math}{Appendix A}: Common newsroom math,
  adapted from drafts of the book
  \href{https://www.ire.org/product/numbers-in-the-newsroom-using-math-and-statistics-in-news-second-edition-e-version/}{\emph{Numbers
  in the Newsroom}}, by Sarah Cohen.
\end{itemize}

\hypertarget{reading-and-viewing}{%
\subsection*{Reading and viewing}\label{reading-and-viewing}}
\addcontentsline{toc}{subsection}{Reading and viewing}

\begin{itemize}
\item
  ``\href{https://www.poynter.org/news/avoiding-numeric-novocain-writing-well-numbers}{Avoiding
  Numeric Novocain: Writing Well with Numbers},'' by Chip Scanlan,
  Poynter.com
\item
  T. Christian Miller's
  ``\href{https://cronkitedata.s3.amazonaws.com/docs/data-writing-tmiller.pdf}{Writing
  the data-driven story}''
\item
  A viral Twitter thread:
\end{itemize}

\hypertarget{exercises-1}{%
\section{Exercises}\label{exercises-1}}

\begin{itemize}
\tightlist
\item
  Imagine that someone gave you \$1 million and you could spend it on
  anything you want. Write down a list of things that would add up to
  about that amount. That should be easy. Now, imagine someone gave you
  \$1 billion and you could spend it on whatever you want, but anything
  left over after a year had to be returned. How would you spend it?
  (You can give away money, but it can't be more than 50\% of a
  charity's annual revenues. So you can't give 10 \$100 million gifts!)
  See how far you get trying to spend it. A few homes, a few yachts,
  student loan repayments for all of your friends? You've hardly gotten
  started.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{start-data-def}{%
\chapter{Defining ``Data''}\label{start-data-def}}

\begin{quote}
data /ˈdeɪ.tə/ :

information in an electronic form that can be stored and used by a
computer, or information, especially facts or numbers, collected to be
examined and \textgreater considered and used to help decision-making

-- Cambridge Dictionary -- sort of \footnote{I flipped the order of
  these two definitions!}
\end{quote}

\hypertarget{the-birth-of-a-dataset}{%
\section{The birth of a dataset}\label{the-birth-of-a-dataset}}

Most journalism uses data collected for one purpose for something
entirely different. Understanding its original uses -- what matters to
the people who collected it, and what doesn't -- will profoundly affect
its accuracy or usefulness.

\hypertarget{trace-data-and-administrative-records}{%
\subsection*{Trace data and administrative
records}\label{trace-data-and-administrative-records}}
\addcontentsline{toc}{subsection}{Trace data and administrative records}

In
``\href{https://www.amazon.com/Art-Access-Strategies-Acquiring-Records/dp/1604265507}{The
Art of Access}'', David Cullier and Charles N. Davis describe a process
of tracking down the life and times of a dataset. Their purpose is to
make sure they know how to request it from a government agency. The same
idea applies to using data that we acquire elsewhere.

Understanding how and why data exists is crucial to understanding what
you, as a reporter, might do with it.

Anything you can systematically search or analyze could be considered
one piece of of data. As reporters, we usually deal with data that was
created in the process of doing something else -- conducting an
inspection, delivering a tweet, or scoring a musical. In the sciences,
this flotsam and jetsom that is left behind is called ``digital trace
data'' if it was born digitally.

In journalism and in the social sciences, many of our data sources were
born during some government process -- a safety inspection, a traffic
ticket, or the filing of a death certificate. These administrative
records form the basis of much investigative reporting and they are
often the subject of public records and FOIA requests. They were born as
part of the government doing its job, without any thought given to how
it might be used in another way. In the sciences, those are often called
``administrative records''.

This trace data might be considered the first part of the definition
above -- information that can be stored and used.

Here's how Chris Bail from Duke University
\href{https://www.youtube.com/embed/uuSWQN7uYhk}{describes it}.

\hypertarget{data-collected-and-curated-for-analysis}{%
\subsection*{Data collected and curated for
analysis}\label{data-collected-and-curated-for-analysis}}
\addcontentsline{toc}{subsection}{Data collected and curated for
analysis}

Another kind of data is that which is compiled or collected specifically
for the purpose of studying something. It might collected in the form of
a survey or a poll, or it might be a system of sampling to measure
pollution or weather. But it's there because the information has
intrinsic value AS information.

The video suggests a hard line between trace data and custom data. In
practice, it's not that clear. Many newsrooms may curate data published
in other sources or in administrative records, such as the Washington
Post's police shooting dataset. In other cases, the agencies we are
covering get already-compiled data from state and local governments.

This type of data might be considered the second type in the definition
-- tabular information that is used for decision-making.

\hypertarget{granular-and-aggregated-data}{%
\section{Granular and aggregated
data}\label{granular-and-aggregated-data}}

One of the hardest concepts for a lot of new data journalists is the
idea of \emph{granularity} of your data source. There are a lot of ways
to think about this: individual items in a list vs.~figures in a table;
original records vs.~compilations; granular data vs.~statistics.

Generally, an investigative reporter is interested in getting data that
is as close as possible to the most granular information that exists, at
least on computer files. Here's an example, which might give you a
little intuition about why it's so important to think this way:

When someone dies in the US, a standard death certificate is filled out
by a series of officials - the attending physician, the institution
where they died and even the funeral direcor.

\href{https://www.cdc.gov/nchs/data/dvs/death11-03final-acc.pdf}{Click
on this link} to see a blank version of the standard US death
certificate form -- notice the detail and the detailed instructions on
how it is supposed to be filled out. \footnote{You should do this
  whenever you get a dataset created from administrative records. That
  is, track down its origin and examine the pieces you were given and
  the pieces that were left out; look at what is written in free-form vs
  what is presented as a check box. You may need a copy of the template
  that an agency uses to collect the information, but many governments
  make these available on their websites or are willing to provide them
  without a fuss.}

A good reporter could imagine many stories coming out of these little
boxes. Limiting yourself to just to COVID-19-related stories: You could
profile the local doctor who signed the most COVID-19-related death
certificates in their city, or examine the number of deaths that had
COVID as a contributing, but not underlying or immediate, cause of
death. You could compare smoking rates in the city with the number of
decedents whose tobacco use likely contributed to their death. Maybe
you'd want to know how long patients suffered with the disease before
they died. And you could map the deaths to find the block in your town
most devastated by the virus.

Early in the pandemic, Coulter Jones and Jon Kamp examined the records
from one of the few states that makes them public, and concluded that
``\href{https://www.wsj.com/articles/coronavirus-deaths-were-likely-missed-in-michigan-death-certificates-suggest-11590073280\%5D\%20\%5E\%5BWall\%20Street\%20Journal,\%20May\%2021,\%202020}{Coronavirus
Deaths were Likely Missed in Michigan, Death Certificates Suggest}''

But you probably can't do that. The reason is that, in most states,
death certificates are not public records and are treated as secrets.
\footnote{See
  ``\href{https://www.researchgate.net/publication/348322257_Secrecy_in_Death_Records_A_Call_to_Action}{Secrecy
  in Death Records: A call to action}'', by Megain Craig and Madeleine
  Davison, Journal of Civic Information, December 2020}. Instead, state
and local governments provide limited statistics related to the deaths,
usually by county, with no detail. That's the difference between
granular data and aggregate data. Here are some of the typical (not
universal) characteristics of each:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.5000}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Granular
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Aggregate
\end{minipage} \\
\midrule()
\endhead
Intended for some purpose other than your work & Intended to be
presented as is to the public \\
Many rows (records), few columns (variables) & Many columns (variables),
few rows (records) \\
Requires a good understanding of the source & Explanatory notes usually
come with the data \\
Easy to cross-reference and compile & Often impossible to repurpose \\
Has few numeric columns & May be almost entirely numerical \\
Is intended for use in a database & Is intended for use in a
spreadsheet \\
\bottomrule()
\end{longtable}

We often have to consider the trade-offs. Granular data with the detail
we need - especially when it involves personally identifiable
information like names and addresses - can take months or years of
negotiation over public records requests, even when the law allows it.
It's often much easier to convince an agency to provide summarized or
incomplete data. Don't balk at using it if it works for you. But
understand that in the vast majority of cases, it's been summarized in a
way that's lost information that could be important to your story.

\hypertarget{nouns}{%
\section{Nouns}\label{nouns}}

That brings us to one of the most important things you must find out
about any data you begin to analyze: What ``noun'' does each row in a
tabular dataset represent? In statistics, they might be called
\emph{observations} or \emph{cases}. In data science, they're usually
called \emph{records}. Either way, every row must represent the same
thing -- a person, a place, a year, a water sample or a school. And you
can't really do anything with it until you figure out what that is.

In 2015, Sarah Cohen did a story at The New York Times called
\href{https://www.nytimes.com/2014/04/07/us/more-deportations-follow-minor-crimes-data-shows.html}{``More
Deportation Follow Minor Crimes, Records Show''}. The government had
claimed it was only removing hardened criminals from the country, but
our analysis of the data suggested that many of them were for minor
infractions.

In writing the piece, they had to work around a problem in our data: the
agency refused to provide them anything that would help us distinguish
individuals from one another. All the reporters knew was that each row
represented one deportation -- not one person! Without a column, or
\emph{field} or a \emph{variable} or an \emph{attribute} for an
individual -- say, name and date of birth, or some scrambled version of
an their DHS number -- they had no way to even estimate how often people
were deported multiple times. If you read the story, you'll see the very
careful wording, except when they had reported out and spoken to people
on the ground.

\hypertarget{further-reading}{%
\section{Further reading}\label{further-reading}}

\begin{itemize}
\item
  \href{https://datajournalism.com/read/handbook/one/understanding-data/basic-steps-in-working-with-data}{``Basic
  steps in working with data''}, the Data Journalism Handbook, Steve
  Doig, ASU Professor. He describes in this piece the problem of not
  knowing exactly how the data was compiled.
\item
  ``\href{https://www.nytimes.com/2020/07/08/podcasts/the-daily/coronavirus-data-united-states.html}{Counting
  the Infected}'' , Rob Gebellof on The Daily, July 8, 2020.
\item
  ``\href{https://eagereyes.org/basics/spreadsheet-thinking-vs-database-thinking}{Spreadsheet
  thinking vs.~Database thinking}'', by Robert Kosara, gets at the idea
  that looking at individual items is often a ``database'', and
  statistical compilations are often ``spreadsheets''.
\item
  ``\href{https://vita.had.co.nz/papers/tidy-data.pdf}{Tidy Data}'', in
  the Journal of Statistical Software (linked here in a pre-print) by
  Hadley Wickham , is the quintessential article on describing what we
  think of as ``clean'' data. For our purposes, much of what he
  describes as ``tidy'' comes when we have individual, granular records
  -- not statistical compilations. It's an academic article, but it has
  the underlying concepts that we'll be working with all year.
\end{itemize}

\hypertarget{exercises-2}{%
\section{Exercises}\label{exercises-2}}

\begin{itemize}
\tightlist
\item
  The next time you get a government statistical report, scour all of
  the footnotes to find some explanation of where the data came from.
  You'll be surprised how often they are compilations of administrative
  records - the government version of trace data.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{introduction-1}{%
\chapter{Introduction}\label{introduction-1}}

Some people consider using spreadsheets the table stakes for getting
into data journalism. It's relatively easy to see what you're doing and
you can easily share your work with your colleagues. In fact, pieces of
the
\href{https://www.pulitzer.org/winners/new-york-times-6}{Pulitzer-Prize
winning COVID-19 coverage} from The New York Times was compiled using an
elaborate and highly tuned set of Google spreadsheets with dozens of
contributors.

This guide uses Google Sheets, although you should be able to do these
exercises with Excel on the Mac or Windows. Excel on the Mac is pretty
good, bu Excel in Windows is very different -- it has much more
capability for working with large and more complex data, and provides
better tuning for import and other operations. There is a table that
compares keystrokes for Apple desktops, laptops and Windows machines for
Excel at the bottom of \href{xl-refresher}{An Excel Refresher}

\hypertarget{tutorials}{%
\section{Tutorials}\label{tutorials}}

Spreadsheets in the form of Google Sheets or Excel are used in almost
every workplace in America. This section covers most of what you need in
the newsroom, which is a different set of skills than in other
businesses.

\begin{itemize}
\item
  \href{xl-refresher.html}{An Excel Refresher} : Start over with good
  habits
\item
  \href{xl-filter-sort.html}{Sorting and filtering to find stories} :
  The first step of interviewing data
\item
  \href{xl-pivot.html}{Grouping with pivot tables}: Aggregating, and the
  super power of spreadsheets
\item
  \href{xl-formulas.html}{Formulas in Excel}: Percents, sums, and other
  basic computations used in newsrooms.
\end{itemize}

\hypertarget{practice-exercises}{%
\section{Practice exercises}\label{practice-exercises}}

\begin{itemize}
\tightlist
\item
  \href{xl-practice-noc.html}{Practice with ``notice of claims'' from
  Phoenix}: Filtering and pivot table practice using claims made against
  the city of Phoenix 2010-2020.
\end{itemize}

\bookmarksetup{startatroot}

\hypertarget{an-excel-refresher}{%
\chapter{An Excel Refresher}\label{an-excel-refresher}}

Spreadsheets are everywhere, so it's worth re-learning how to use them
well. Reporters usually use spreadsheets in three ways:

\begin{itemize}
\item
  To create original databases of events for sorting, filtering and
  counting. Examples include a long-running court case; the details of
  each opioid death in a city; a list of police shootings and their
  documents; or even a list of your own public records requests or
  contact log.
\item
  To use data created by others for fast, simple analysis and data
  cleanup. Many government agencies provide their information in
  spreadsheet form, but they often require some rejiggering before you
  can use them.
\item
  To perform simple, straightforward analysis on data and share with
  team members. This is becoming less common as more reporters learn
  programming languages, but it's still common in newsrooms to share
  data, especially through Google Sheets.
\end{itemize}

(This guide is done using a Mac. Windows machines will be a little
different, mainly because you'll have more choices in most menus. The
Mac CMD key is the same as the Windows CNTL key.)

Some reporters flinch at typing in 30 or 100 entries into a spreadsheet.
You shouldn't. If you learn to take notes in a structured way, you'll
always be able to find and verify your work. If you try to calculate a
sum of 30 numbers on a calculator, you'll have to type them all in at
least twice anyway. Also, getting used to these easy tasks on a
spreadsheet keeps your muscles trained for when you need to do more.

\hypertarget{re-learning-sheets-from-the-ground-up}{%
\section{Re-learning Sheets from the ground
up}\label{re-learning-sheets-from-the-ground-up}}

\hypertarget{the-spreadsheet-grid}{%
\subsection{The spreadsheet grid}\label{the-spreadsheet-grid}}

\includegraphics{./images/sheets-refresher-battleship.png}

When you start up a spreadsheet (try entering sheet.new into your
browser!), you'll see letters across the top and numbers down the side.
If you ever played Battleship, you'll recognize the idea -- every little
square, or cell, is referenced by the intersection of its column letter
and row number:

B2 is the cell that is currently active. You can tell because it's
outlined in the sheet and it's shown on the upper left corner.

\hypertarget{mouse-shapes}{%
\subsection{Mouse shapes}\label{mouse-shapes}}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.7500}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.2500}}@{}}
\toprule()
\endhead
\textbf{The Copy Tool}, or the thin black cross. When you see this,
you'll copy anything that's selected. This can be good or bad. &
\includegraphics{./images/xl-refresher-copy.png} \\
\textbf{The Evil Hand}. (In Windows, this is the Evil Arrow). If you use
this symbol, you will MOVE the selection to a new location. This is very
rarely a good idea or something you intend. &
\includegraphics{./images/xl-refresher-evilhand.png} \\
\bottomrule()
\end{longtable}

\hypertarget{selecting-cells-and-ranges}{%
\subsection{Selecting cells and
ranges}\label{selecting-cells-and-ranges}}

Spreadsheets act only on the cells or regions you have selected. If you
begin typing, you'll start entering information into the currently
selected cell.

To select: Hold the BFWPS over the cell and clice ONCE -- \emph{not
twice}. Check the formula bar to make sure you've selected what you
think you've got. You can also look at the bottom right of your
spreadsheet for more information.

You'll often work with \emph{ranges} of cells in formulas. These are
defined by the corners of the area you want to work on -- often a column
of information. In the example below, the range is A1:B6, with the ``:''
referring to the word ``through''.

\emph{To select a group of cells and act on them all at once}: Hover the
BFWPS over one corner, click ONCE and drag to the diagonal corner. Make
sure the Evil Hand is nowhere to be seen. The entire area will be shaded
in except for the currently selected cell. Look at the upper right
corner to see how many rows and columns you selected.

\includegraphics{./images/xl-refresher-columnselect.png}

\emph{To select a column or row} : Hover the cursor over the letter at
the top of the column. For a row, hover it over the row number in the
margin

\hypertarget{reading-the-screen}{%
\subsection{Reading the screen}\label{reading-the-screen}}

The areas of the spreadsheet have different visual clues, and learning
to read them will make your life much easier.

\hypertarget{entering-data}{%
\subsection{Entering data}\label{entering-data}}

Select the cell and start typing. The information you type won't be
locked into the cell until you hit the Return / Enter key, or move your
selection to another cell. Hit ``Escape'' to cancel the entry.

You can't do a lot of things while you're editing, so if you have a lot
of greyed out menu items, look at your formula bar to see if you are
still editing a cell.

If you're having trouble getting to a menu item or seeing the result of
your work, try hitting ``Escape'' and try again. You may not have
actually entered the information into the sheet.

\hypertarget{locking-in-headings}{%
\subsection{Locking in headings}\label{locking-in-headings}}

As your spreadsheet grows vertically with more rows, you'll want to be
able to see the top all the time. When it grows horizontally with more
columns, you'll probably want to see columns in the left, such as names.
This is called ``Freezing Panes'' -- you freeze part of the page so it
stays in place when you move around.

In Google Sheets, this is done via the View -\textgreater{} Freeze menu:

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./images/sheets-refresher-freezepanes.png}

}

\caption{freeze panes}

\end{figure}

\hypertarget{formatting-tricks}{%
\subsection{Formatting tricks}\label{formatting-tricks}}

\begin{itemize}
\item
  Use the buttons or the format dialog box to make numbers easier to
  read.
\item
  If a column is filled with a lot of text, select the column and look
  on the Home ribbon next to the formatting area for ``Wrap Text''. This
  means that when you double-click to widen a column, it will get
  taller, not wider. This is good when you need to save valuable real
  estate on the screen.
\end{itemize}

\hypertarget{getting-started-with-a-dataset}{%
\section{Getting started with a
dataset}\label{getting-started-with-a-dataset}}

\textbf{SLOW DOWN!} Don't do anything until you understand what you have
in front of you and can predict what your next mouse click will do to
it.

Most data we encounter was created by someone else for some purpose
other than ours. This means that you can't assume anything. It may not
be complete. It may be inaccurate. It may mean something completely
different than it appears at first blush.

\hypertarget{first-steps}{%
\subsection{First steps}\label{first-steps}}

\begin{itemize}
\tightlist
\item
  Document where you got the spreadsheet and how you can get back to the
  original.
\item
  Read anything you can about what it contains. Look for documentation
  that comes with the data.
\item
  Save the original into a safe place with its original name and
  metadata. Work on a copy.
\item
  If the spreadsheet shows \#\#\#\# instead of words or numbers, widen
  your columns. If it shows 7E-14 or something like that, format them as
  numbers, not ``General''.
\item
  Check your corners -- look at the top left and bottom right. Is the
  data all in one area? Are there footnotes or other non-data sections
  mixed in? We're going to want to fix that later.
\end{itemize}

\hypertarget{interview-your-data}{%
\subsection{Interview your data}\label{interview-your-data}}

\hypertarget{headings}{%
\subsubsection{Headings}\label{headings}}

The most fraught part of data reporting is understanding what each
\emph{column} actually means. These often have cryptic, bureaucratic
names. You may need to go back to the source of the data to be sure you
actually understand them.

If your data doesn't have any headings, that's going to be your first
priority. In effect, you'll need to build what we call a \emph{data
dictionary} or \emph{record layout} if one hasn't been provided. Many
reporters create these as a page in a dataset.

\hypertarget{unit-of-analysis}{%
\subsubsection{Unit of analysis}\label{unit-of-analysis}}

A \emph{unit of analysis} refers to the items that are listed in the
rows of your dataset. Ideally, every row should be at the same unit of
analysis -- a person, an inspection, or a city, for example. Summaries
should be separated by a blank row, or moved to a different sheet. Think
of this as the noun you'd use to describe every row.

\hypertarget{row-numbers}{%
\subsubsection{Row numbers}\label{row-numbers}}

The data was probably given to you in some sort of natural sort order.
Different computer systems sort differently -- some are case-sensitive,
others are not. It may depend on when and where the data as created! The
order of the data may even depend on a column you don't have. If you
don't do something now, you'll never be able to get back to the original
order, which could have meaning for both the agency and for
fact-checking.

\bookmarksetup{startatroot}

\hypertarget{sorting-and-filtering-to-find-stories}{%
\chapter{Sorting and filtering to find
stories}\label{sorting-and-filtering-to-find-stories}}

\hypertarget{a-sorting-miracle}{%
\section{A sorting miracle}\label{a-sorting-miracle}}

When Stephen Neukam - who was sitting in this class a year ago - wanted
to find out who was funding candidates for Maryland's open governor's
seat this year, he downloaded data from the State Board of Elections
that listed contributions to the wide array of hopefuls seeking to
replace Larry Hogan in Annapolis.

He wasn't sure at first what he was looking for, so he started the way
that many reporters do with data: by sorting and filtering. Were there
outliers in the list of contributions, and which candidates were getting
their money from unusual (non-Maryland) sources?

Neukam quickly found his story: in the race to be governor, Maryland
candidates, and in particular Wes Moore, a first-time Democratic
candidate, were raising millions of dollars from out of state donors.

The story,
``\href{https://cnsmaryland.org/2022/03/03/millions-in-out-of-state-donations-help-fuel-high-profile-maryland-democratic-governor-candidates/}{Millions
in out-of-state donations help fuel high-profile Maryland Democratic
governor candidates}'' helped explain where candidates were going to
fund one of the most contested primaries in recent history (Moore ended
up winning).

\hypertarget{sorting-and-filtering-as-a-reporting-tool}{%
\section{Sorting and filtering as a reporting
tool}\label{sorting-and-filtering-as-a-reporting-tool}}

Sorting and filtering can:

\begin{itemize}
\tightlist
\item
  Narrow your focus to specific items that you want to examine in your
  story.
\item
  Show you rows containing the highest and lowest values of any column.
  That can be news or it can be errors or other problems with the data.
\item
  Let you answer quick ``how many?'' questions, with a count of the rows
  that match your criteria. (In the next lesson, you'll see that pivot
  tables, or group-by queries, are much more powerful for this in most
  cases.)
\end{itemize}

\hypertarget{example-data}{%
\section{Example data}\label{example-data}}

::: \{.alert .alert-info \} -
\href{https://github.com/stephenneukam/CNS_Annapolis/tree/main/Campaign_finance}{Data
from the State Board of Elections} for use in this tutorial -
\href{https://campaignfinance.maryland.gov/Public/ViewReceiptsMain}{Documentation
from the SBOE's site} :::

\begin{itemize}
\item
  The data for this is from the Maryland State Board of Elections's
  Campaign Finance Database. There are a couple of caveats:
\item
  It includes money raised as of Jan.~12, 2022, which covers all of
  2021.
\item
  These are self-reported by campaigns, and subject to amendment in case
  of errors or omissions.
\end{itemize}

The original data download link for Wes Moore's contributions is
\url{https://github.com/stephenneukam/CNS_Annapolis/raw/main/Campaign_finance/Moore_ContributionsList.csv}.
Download it to your computer and then, in a browser, type sheet.new to
create a new Google Sheet. From there, use File -\textgreater{} Import
and choose ``Upload'' and select the file on your computer. Click the
``Import Data'' button when it appears. Then give your sheet a name,
like ``Wes Moore Contributions''.

It's a good example set for us because it's been used as the basis of
Neukam's story and it has at least one of each \emph{data type} that we
plan to deal with in Google Sheets or Excel. And, critically, the first
row contains headers, not data. Always have headers, even if you have to
add them.

\hypertarget{understanding-data-types}{%
\section{Understanding data types}\label{understanding-data-types}}

When you open the spreadsheet, the first thing to notice is its
\href{start-data-def.html}{\emph{granularity}}. Unlike Census or budget
spreadsheets, this is a list capturing specific characteristics of each
contribution. Each column has the same \emph{type} of data from top to
bottom. Those types are:

\begin{itemize}
\item
  \textbf{Text}. Text or ``character'' columns can come in long or short
  form. When they are standardized (the values can contain only one of a
  small list of values), they're called ``categorical''. If they're more
  free-form, they're might be called ``free text''. The computer doesn't
  know the difference, but you should. The Post data has examples of
  both. In spreadsheets, text is left-justified (they move toward the
  left of the cell and will line up vertically at the beginning)
\item
  \textbf{Numbers}. These are pure numbers with no commas, dollar signs
  or other embellishments. In Google Sheets these can be formatted to
  \emph{look} like numbers in different ways, but underneath they're
  just numbers. Adding up a column of numbers that has a word in it or
  has missing values will just be ignored. It will trip up most other
  languages. These are right-justified, so the last digit is always
  lined up vertically.
\item
  \textbf{Logical}: This is a subset of text. It can take one of only
  two values -- yes or no, true or false. There is no ``maybe''.
\item
  \textbf{Date and times}: These are actual dates on the calendar, which
  have magical properties. Underneath, they are a number. In Google
  Sheets and Excel, that number is the number of days since Jan.~1,
  1900.\footnote{Each language deals with dates and times a little
    differently. We'll see how R does it later on. But just know that
    dates can be tricky because of these differences and
    \href{https://infiniteundo.com/post/25326999628/falsehoods-programmers-believe-about-time}{time
    is even more tricky}} They can also have time attached to them,
  which is a fraction of a day. What this means is that the number
  44,536.5 is really Dec.~6, 2021 at noon. In Sheets, you use a format
  to tell the spreadsheet how you want to see the date or time, just the
  way you look at dollar values with commas and symbols. (If you get a
  spreadsheet with a lot of dates of 1/1/1900, it means there is a 0 in
  that column, which is sometimes a fill-in for ``I don't know.'')
\end{itemize}

Here's a picture of a date that is shown in a variety of formats.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./images/xl-filter-sort-dates.png}

}

\caption{date formats}

\end{figure}

All of these are the same, underlying value -- the number at the left.
Notice that all of these are right-justified.

This means that when you see ``Friday, December 10'', the computer sees
44540.87431. When you put the dates in order, they won't be alphabetized
with all of the Fridays shown together. Instead, they'll be arranged by
the actual date and time.

It also means that you can compute 911 response times even when it
crosses midnight, or or compute the someone's age today given a date of
birth. Keeping actual calendar dates in your data will give it much more
power than just having the words. (Sheets and Excel use the 1st of the
month as a stand-in for an actual date when all you know is the month
and year.)

\hypertarget{sorting-rows}{%
\subsection{Sorting rows}\label{sorting-rows}}

Sorting means rearranging the rows of a data table into a different
order. Some reporters take a conceptual shortcut and call this ``sorting
columns''. That thinking will only get you into trouble -- it lets you
forget that you want to keep the rows in tact while changing the order
in which you see them. In fact, in other languages it's called ``order
by'' or ``arrange'' by one or more columns -- a much clearer way to
think of it.

To sort in Google Sheets, first highlight the entire sheet by clicking
on the button above the first row and to the left of the first column.
So, between the 1 and the A. Then, look for the sort options under the
Data drop-down menu at the top of your screen, and choose ``Sort Range''
and then ``Advanced range sorting options''. Trust me, this is how you
want to do it. Check the box that says ``Data has header row'' and Sort
by Contribution Date. In this case, sorting from A to Z gives you a list
of the contributions in chronological order.

\includegraphics[width=0.8\textwidth,height=\textheight]{./images/sheets-filter-sort-sortbox.png}

\hypertarget{adding-fields-to-the-sort}{%
\subsubsection*{Adding fields to the
sort}\label{adding-fields-to-the-sort}}
\addcontentsline{toc}{subsubsection}{Adding fields to the sort}

Adding more columns to the sort box tells Sheets what to do when the
first one is the same or tied. For example, sorting first by date then
by amount gives you a list that shows all of the contributions by date
in sequence:

\includegraphics[width=0.5\textwidth,height=\textheight]{./images/sheets-filter-sort-date.png}

\hypertarget{filtering}{%
\subsection{Filtering}\label{filtering}}

Filtering means picking out only some of the rows you want to see based
on a criteria you select in a column. Think of it as casting a fishing
net -- the more filters you add, the fewer fish will be caught.

To turn on filters in Google Sheets, go to Data -\textgreater{} Create a
filter. It will add small down arrows to each column in the first row,
another reason why headers are crucial. You can filter by multiple
columns, and each filter you select adds more conditions, narrowing your
net.

To find contributions of \$5,000, use the drop-down menu under
\texttt{Contribution\ Amount} to select it and hit ``OK''.

When you do this, notice that the drop-down arrow has turned into a
solid green funnel and that any rows that don't match your filter are
hidden.

\includegraphics{./images/sheets-filter-sort-onefilter.png}

This method works for small-ish and simple-ish columns. If your column
has more than 10,000 different entries, such as names or addresses, only
the first 10,000 will be considered. We only caught these for stories
when someone did a fact-check using a different method of filtering. If
your column has a lot of distinct entries, use option that says ``Choose
One'', and then use the ``Contains'' option. Better yet, don't use
filtering for counting things at all.

Add more filters to narrow down your list of cases even more. For
example, if you wanted to see \$5,000 contributions from individuals,
you would choose ``Individual'' under Contributor Type:

\includegraphics{./images/sheets-filter-sort-twofilters.png}

\hypertarget{different-kinds-of-filters}{%
\subsubsection*{Different kinds of
filters}\label{different-kinds-of-filters}}
\addcontentsline{toc}{subsubsection}{Different kinds of filters}

There are several options under the filter drop-down menu under ``Filter
by condition'', and you also can type values into a search box to try
and filter that way (the latter option is best for text columns). There
also is a ``Filter by color'' option. My opinion: don't do this.

\includegraphics[width=0.3\textwidth,height=\textheight]{./images/sheets-filter-sort-filter-options.png}
\#\# FAQ

\hypertarget{how-do-i-turn-off-all-of-my-filters}{%
\subsection*{How do I turn off all of my
filters}\label{how-do-i-turn-off-all-of-my-filters}}
\addcontentsline{toc}{subsection}{How do I turn off all of my filters}

In the data tab, chose ``Remove filter'' to remove all of the filters.

\hypertarget{where-is-the-button-to-filter-columns}{%
\subsection*{\texorpdfstring{Where is the button to filter
\emph{columns}?}{Where is the button to filter columns?}}\label{where-is-the-button-to-filter-columns}}
\addcontentsline{toc}{subsection}{Where is the button to filter
\emph{columns}?}

Sometimes you don't want to see all of your columns -- there are too
many and they're getting confusing. There is no column filter in Sheets
or Excel (You'll see how to filter, or ``Select'', columns from a
dataset in R later.)

Instead, you can hide the columns you don't want to see. When columns
and rows are hidden, they generally won't copy to a new sheet.

\bookmarksetup{startatroot}

\hypertarget{xl-formulas}{%
\chapter{Formulas in Google Sheets}\label{xl-formulas}}

The quick review of math in Google Sheets uses the City of Phoenix's
budgeted spending for the 2018 fiscal year, compared with previous
years.

Make a copy of the
\href{https://docs.google.com/spreadsheets/d/1NGFHiJzvSsdB2wT7AehdSCjGrrwrOS1_rLk5FEaz7a8/edit?usp=sharing}{data
file} to follow along

You should get into the habit of creating unique identifiers, checking
your corners and looking for documentation before you ever start working
with a spreadsheet. These habits were covered in
\href{replication.html}{Data journalism in the age of replication} and
on \href{xl-refresher.html}{a Google Sheets refresher}.

\hypertarget{formulas-in-spreadsheets}{%
\section{Formulas in spreadsheets}\label{formulas-in-spreadsheets}}

Whether you use Google sheets or Excel, remember that every formula
begins with the equals sign \textbf{(=)}. Rather than the values you
want to work with in the formula, you'll use \emph{references} to other
cells in the sheet.

The easiest formulas are simple arithmetic: adding, subtracting,
multiplying and dividing two or more cells. You'll just use simple
operators to do this:

\begin{longtable}[]{@{}lll@{}}
\toprule()
operator & symbol & example \\
\midrule()
\endhead
addition & + & =A2+B2 \\
subtraction & - & =A2-B2 \\
multiplication & * & =A2*B2 \\
division & / & =A2/B2 \\
\bottomrule()
\end{longtable}

Here's what a spreadsheet looks like while editing some simple
arithmetic:

\begin{figure}

{\centering \includegraphics{./images/xl_mathreview_00.png}

}

\caption{formula}

\end{figure}

The other kind of formula is a \emph{function}. A function is a command
that has a name, and requires \emph{arguments} -- usually the cell
addresses or the range of addresses that it will act on. Every
programming language has functions built in and many have extensions, or
packages or libraries, that add even more as users find things they want
to do more efficiently. You begin using a function the same way you
begin a formula -- with an = sign. Here are three common functions that
create summary statistics for the numbers contained in a \emph{range} of
addresses. A range is a set of cells defined by its corner cell address:
the top left through the bottom right.

You'll usually use them on a single column at a time.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.3243}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 2\tabcolsep) * \real{0.6757}}@{}}
\toprule()
\begin{minipage}[b]{\linewidth}\raggedright
Formula
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
What it does
\end{minipage} \\
\midrule()
\endhead
=SUM(start:finish) & Adds up the numbers between \texttt{start} and
\texttt{finish} \\
=AVERAGE(start:finish) & Computes the mean of the numbers \\
=MEDIAN(start:finish) & Derives the median of the numbers \\
\bottomrule()
\end{longtable}

\ldots where ``start'' means the first cell you want to include, and
finish means the last cell. Use the cell address of the first number you
want to include , a colon, then the cell address of the last number you
want to include. You can also select them while you're editing the
formula.

Here's an example of adding up all of the rows in a list by county:

\begin{figure}

{\centering \includegraphics{./images/xl_mathreview_00A.png}

}

\caption{formula}

\end{figure}

\hypertarget{common-spreadsheet-arithmetic}{%
\section{Common spreadsheet
arithmetic}\label{common-spreadsheet-arithmetic}}

The budget document shows three years' of data: The actual spending in
the fiscal year that ended in 2016; the spending that was estimated for
the end of fiscal year 2017; and the proposed spending for fiscal year
2018. The first page of the document shows these amounts for broad
spending categories.

You may want to widen the columns and format the numbers before you
start:

\includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_01.gif}

\hypertarget{check-the-governments-math-with-sum}{%
\subsection{Check the government's math with
SUM}\label{check-the-governments-math-with-sum}}

Our first job is to make sure the government has provided us data that
adds up. To do that, we'll SUM all of the departments' spending.

To add up the numbers from 2016, enter the following formula in cell
C11, just below the number provided by the government:

\begin{verbatim}
  =SUM(C2:C8)
  and hit the enter key
\end{verbatim}

Copy that formula to the right. Notice how the formula changes the
addresses that it is using as you move to the right -- it's adjusted
them to refer to the current column.

\includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_02.gif}

What's wrong? The numbers for the budget 2018 don't add up. (Hint: look
at the page called ``notes'' for an explanation.)

\hypertarget{change-in-spending}{%
\subsection{Change in spending}\label{change-in-spending}}

The increase or decrease in projected spending from 2017 to 2018 is just
the difference between the two values, beginning in cell F3

\begin{verbatim}
  new-old, or  =E2-D2
\end{verbatim}

When you copy it down, note how the references to each row also
adjusted. In line 3, it's E3-D3, and so on. Excel and other spreadsheets
assume that, most of the time, you want these kinds of adjustments to be
made.

\includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_03.gif}

\hypertarget{percent-change}{%
\subsection{Percent change}\label{percent-change}}

We can't tell the \emph{rate} of growth for each department until we
calculate the percent change from one year to another. Now that we
already have the change, the percent change is easy. The formula is:

\begin{verbatim}
  ( new - old ) / old

  .. or just scream "NOO"
\end{verbatim}

The new-old is already in column F, so all that's left is to divide
again. In grade school, you also had to move the decimal place over two
spots, since the concept of percent change is ``out of 100''. Excel
formats will do that for you.

Remember, it's always (new-old)/old , \textbf{NOT} the big one minus the
little one. Doing it correctly, the answer could be negative, meaning
the value fell.

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_04.png}

}

\caption{``\% change''}

\end{figure}

When you're done, you can format the answer as a percentage to get it
into whole numbers.

Until you get used to it, there's no harm in doing these calculations
step by step. Excel won't complain if you have extra columns. You can
always hide them.

It's also worth comparing the picture you get by looking at raw numbers
vs.~percentages. In our case, the budget for public safety is expected
to rise by a whopping \$102 million, but it's a smaller percentage
increase than other, smaller departments.

\hypertarget{parts-of-a-whole-percent-of-total}{%
\subsection{Parts of a whole: percent of
total}\label{parts-of-a-whole-percent-of-total}}

We'd also like to know what portion of the total spending is eaten up by
each department. To do that, we need the percent of total.

In our case, let's use the total that the government gave us. In
practice, you'd have to decide what to do if your figures didn't match
those provided by officials. You can't assume that the total is wrong --
you could be missing a category, or there could be a mistake in one of
the line items.

The formula for percent of total is:

\begin{verbatim}
  category / total
\end{verbatim}

Again, Excel will multiply by 100, or move the decimal place over for
you once you format.

But you have a problem: You either have to type in each row, or you get
something like this if you try to copy:

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_05.png}

}

\caption{Wrong way}

\end{figure}

Excel has done its magic, adjusting the location of both the numerator
and the denominator when you copied. You don't have to type in each
formula one by one, though. Instead, you'll use anchors, known in
spreadsheets as ``absolute references''. Think of a dollar sign as an
anchor or stickpin, holding down the location of part of your formula.
If you put the stickpin before the letter in the formula, it holds the
column in place. If you put it before the number, it holds the row in
place. If you put it in both places, it holds the cell in place.

So our new formula for the percent of total is:

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_06.png}

}

\caption{E2/E\$10}

\end{figure}

\hypertarget{while-were-at-it-two-kinds-of-averages}{%
\section{While we're at it: two kinds of
averages}\label{while-were-at-it-two-kinds-of-averages}}

Although it doesn't make a lot of sense in this context, we'll go ahead
and calculate the \emph{average} or \emph{mean} size of each department,
and then calculate the \emph{median} size.

\textbf{Simple average, or mean}

A simple average, also known as the mean, is skewed toward very high or
very low values. Its formula is

\begin{verbatim}
    sum of pieces / # of pieces that were summed
\end{verbatim}

But in Excel, all we need is the word AVERAGE:

\begin{verbatim}
    =AVERAGE(C2:C9)
\end{verbatim}

\textbf{Median}

In Excel, you can get the median of a list of numbers by just using the
formula, MEDIAN()

\begin{verbatim}
  = MEDIAN(C2:C9)
\end{verbatim}

\hypertarget{the-final-spreadsheet}{%
\section{The final spreadsheet}\label{the-final-spreadsheet}}

\begin{figure}

{\centering \includegraphics[width=1\textwidth,height=\textheight]{./images/xl_mathreview_07.png}

}

\caption{final worksheet}

\end{figure}

Doing simple calclutions like this on data that is provided to you by
the government lets you ask better questions when you get an interview,
and may even convince officials to talk with you. There's a big
difference between asking them to tell you what the budget numbers are,
and asking them to explain specific results!

\hypertarget{faqs}{%
\section{FAQs}\label{faqs}}

\hypertarget{sheets-wont-let-me-copy-my-formula}{%
\subsubsection*{Sheets won't let me copy my
formula}\label{sheets-wont-let-me-copy-my-formula}}
\addcontentsline{toc}{subsubsection}{Sheets won't let me copy my
formula}

Make sure your formula is locked in by either hitting ``Enter'' or
``Escape''. This is a common problem if you're in the habit of
double-clicking instead of selecting a cell. There are a lot things you
can't do while Excel thinks you're still entering information.

\hypertarget{should-i-use-average-or-median}{%
\subsubsection*{Should I use average or
median?}\label{should-i-use-average-or-median}}
\addcontentsline{toc}{subsubsection}{Should I use average or median?}

It depends. Averages are easier to explain but can be misleading.
Usually, if they're very different, median will be a better
representation of the typical person, city or department. Averages in
these cases are more like totals.

\hypertarget{my-percents-are-small-numbers-with-decimal-points}{%
\subsubsection*{My percents are small numbers with decimal
points}\label{my-percents-are-small-numbers-with-decimal-points}}
\addcontentsline{toc}{subsubsection}{My percents are small numbers with
decimal points}

Use the format as a \% button to move the decimal point over two places
and insert the percentage symbol.

\bookmarksetup{startatroot}

\hypertarget{xl-practice-noc}{%
\chapter{Practice exercise}\label{xl-practice-noc}}

This exercise helps you understand the difference between filtering and
grouping, or, in Excel terms, filtering and pivot tables. In general,
you'll use filtering to look up the details of cases based on a
criteria. These are the stories. You'll use pivot tables to create
aggregated data -- count, sum, average, and the like.

In the real world, reporters move back and forth between these two
techniques. They'll go back and filter for cases once they find
something interesting in the aggregation, and will create sums and
counts if they think they see a pattern in the cases.

That's pretty abstract concept -- it will become more concrete as you go
through the motions of this exercise.

\hypertarget{data-source}{%
\section{Data source}\label{data-source}}

\href{https://cronkitedata.s3.us-west-1.amazonaws.com/xlfiles/phx_claims_flagged.xlsx}{Data
download}

\href{https://www.phoenix.gov/financesite/Documents/City\%20of\%20Phoenix\%20Claim\%20Form\%20\%28REVISED\%206-7-21\%29.pdf}{City
link to Notice of claims form and instructions to claimants}

\href{https://docs.google.com/document/d/1j-vIiuysefqOrDAN-pIR5NqCq8mKqTBmQKcuhRLXy0o/edit?usp=sharing}{Backgrounder
on the data from Helen Wieffering}

This dataset includes all ``Notice of Claims'' against the city of
Phoenix between 2010 and 2020. These claims refer to damages that people
say they suffered because of a problem in the government ranging from
slip-and-fall in a city building to use of force by police. It was
obtained by Helen Wieffering just after the end of the 2020 fiscal year,
so many of the claims from that year will be unresolved. Although the
names of the claimants are public record, they were not included in the
data compiled here. Also missing is the court case number of any cases
that went to court.

Make sure to look at the ``data\_dictionary'' sheet for the definitions
of each column before you start.

In this tutorial, I'm providing the equivalent of a data diary for you
-- if you follow these instructions, you should come up with the same
answers I did. You should keep your own diary alongside your work.

\hypertarget{sort-filter}{%
\section{Sort / filter}\label{sort-filter}}

I almost always go into the Excel preferences, and under ``Tables \&
Filters'' turn off the check that says ``Use table names in formulas''.
Sometimes it sticks, sometimes it doesn't. Have no idea why.

Arrange (sort) the claims using the ID column which is called
``orig\_order''.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What is the orig\_order (id) value of the largest PAID claim in the
  database (combined personal injury and property) ?
\item
  What department and cause were associated with that payment?
\item
  How long did it take to resolve it?
\item
  Describe the most recent claim made against the police related to law
  enforcement activities, regardless of whether it's been resolved.
\item
  Filter for pd\_total of greater than zero, and the type called
  ``Shootings''. Look at the lower left corner and record how many
  claims against the city for shootings were paid any money?
\item
  Find one other thing that you might want to research further that
  could make an interesting one-off story. These are distinct events
  that you'd like to know more about, not trends or patterns. This
  requires no calculation, just your news judgment.
\end{enumerate}

\hypertarget{pivot-table-motivation}{%
\section{Pivot table motivation}\label{pivot-table-motivation}}

Here are some questions you have about each department in the city:

\begin{itemize}
\tightlist
\item
  What was the highest and lowest payout they made?
\item
  How many claims were made, and how many were paid out?
\item
  What was the total amount paid and the average per claim?
\end{itemize}

\hypertarget{using-filters-to-answer-big-questions}{%
\subsection{Using filters to answer big
questions}\label{using-filters-to-answer-big-questions}}

Here's one way to do it, using the filters:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Remove all of your filter.
\item
  Filter by department. We'll start with Aviation, the first one on the
  list.
\item
  Select the entire table, including the headings. Copy it to a new
  sheet. Don't be surprised if you miss the headings or some of the
  columns the first couple of times through and have to do it again.\\
\item
  Rename the new sheet ``Aviation'' by double-clicking on its tab.
\item
  Make sure you're in the top left corner of the screen (use CTL-HOME
  key if you're on a real keyboard, CTL-UP and CTL-Left if you're not.)
  Freeze the column headings so you can scroll to the bottom and still
  see what you're looking at (under View, Freeze top Row)
\item
  Note the number of rows that were copied and subtract one. Record that
  as the number of claims made.
\item
  Make sure you start from the top of the sheet, and filter out
  ``Unresolved'' claims by un-selecting it. Don't touch anything before
  you note the number of records found at the bottom left (it should be
  11852 in this case). That's the number of resolved claims. Now filter
  on the same column, and uncheck ``No payment'' and note the number of
  claims. It should be 5718.
\item
  Go to the bottom of the worksheet. Skip a row, and sum the column.
  Repeat that, replacing =sum() with =max(), =min() , =average().
\item
  Write down all of your answers on a piece of paper.
\end{enumerate}

Don't be surprised if you do this wrong a few times, and you have to do
it over. But now you have the answer for the Aviation department.

Try to imagine what you'd do if you wanted to know the average and
minimum payout of just those that were settled .

\hypertarget{rinse-repeat}{%
\section{Rinse, repeat}\label{rinse-repeat}}

Now repeat that entire process with at least two other departments that
interest you.

\hypertarget{pivot-table}{%
\section{Pivot table}\label{pivot-table}}

At this point, you should be thoroughly sick of having to do things over
and over. The purpose of the pivot table is to let you compute these
aggregate, or summary, statistics all at once for each category or set
of categories. It can compute average, minimum , maximum and sum of
values, and it can count the number of rows for any column. (In Excel
they're always the same. That's not always true of all programs
depending on how they handle missing information.)

\hypertarget{create-the-pivot-table}{%
\subsection{Create the pivot table}\label{create-the-pivot-table}}

1, Create an empty pivot table from the data table.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Look for the ``Options'' tab toward the top left, and turn off
  ``Generate GetPivotData''. In the same tab, under ``Options'', set
  error values to ``N/A'', and set Empty cells to zero (meaning there
  was nothing in that category.)
\item
  Drag ``Department'' to the row area
\item
  Drag the type\_case to the filter area
\item
  Drag the ``department'' to the Values area and make sure it says
  ``Count of Department''.
\item
  Drag the pd\_total to the Values area, and make sure it says ``Sum of
  pd\_total''. If it doesn't, change the pivot table calculation to
  ``Sum''. Repeat that three times, with ``average'', ``min'' and
  ``max''.
\end{enumerate}

Try playing with the placement of items in the pivot table to get an
intuition for what they do. You can drag a column into the ``Filter''
area so that you can pick out just some of the cases for analysis. It
works the same way as the filter in the original sheet.

\hypertarget{practice-questions}{%
\subsection{Practice questions}\label{practice-questions}}

Now answer the following questions:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  What department had the most claims made against them from FY 2015-16
  to through 2019-20?
\item
  How much did the city pay in that time because of complaints against
  Streets. Did any department pay more?
\item
  On average, how much did the city pay vs.~the amount that was
  requested? Include only those complaints that have been resolved --
  not those that are still underway or have been reopened.
\end{enumerate}

\bookmarksetup{startatroot}

\hypertarget{ethics-in-data-journalism}{%
\chapter{Ethics in data journalism}\label{ethics-in-data-journalism}}

\href{https://source.opennews.org/articles/public-info-doesnt-always-want-be-free/}{This
originally appeared on Open News in March 2013}.

In 2009, a senior web editor asked me and another developer a question:
could our development group build a new news application for
Tampabay.com that displayed a gallery of mug shots? Stories about goofy
crimes with strange mug shots were popular with readers. The vision, on
the part of management, was a website that would display the mugshots
collected every day from publicly available websites by two
editors---well paid, professional editors with other responsibilities.

Newsrooms are many things. Alive. Filled with energy. Fueled by stress,
coffee and profanity. But they are also idea factories. Day after day,
ideas come from everywhere. From reporters on the beat. From editors
reading random things. From who knows where. Some of them are brilliant.
Some would never work. Most need more people and time than are
available. And some are dumber than anyone cares to admit.

We thought this idea was nuts. Why would we pay someone, let alone an
editor, to fetch mug shots from the Internet? Couldn't we do that with a
scraper?

If only this were the most complex question we would face.

Because given enough time and enough creativity, scraping a mug shot
website is easy. You need to recognize a pattern, parse some HTML and
gather the pieces you need. At least that's how it should work. Police
agencies that put mugs online usually buy software from a vendor.
Apparently, those vendors enjoy making horrific, non-standard,
broken-in-interesting-and-unique-ways HTML. You'll swear. A lot. But
you'll grind it out. And that's part of the fun. Scraping isn't any fun
with clean, semantic, valid HTML. And scraping mug shot websites, by
that definition, is tons of fun.

The complexity comes when you realize the data you are dealing with
represent real people's lives.

\hypertarget{problems}{%
\section{Problems}\label{problems}}

The first problem we faced, long before we actually had data, was that
data has a life of its own. Because we were going to put this
information in front of a big audience, Google was going to find it.
That meant if we used our normal open door policy for the Googlebot,
someone's mug shot was going to be the first record in Google for their
name, most likely. It would show up first because most people dont
actively cultivate their name on the web for visibility in Google. It
would show up first because we know how SEO works and they dont. It
would show up first because our site would have more traffic than their
site, and so Google would rank us higher.

And that record in Google would exist as long as the URL did. Longer
when you consider the cached versions Google keeps.

That was a problem because here are the things we could not know:

\begin{itemize}
\tightlist
\item
  Was this person wrongly arrested?
\item
  Was this person innocent?
\item
  Were the charges dropped against this person?
\item
  Did this person lie about any of their information?
\end{itemize}

\hypertarget{the-googlebot}{%
\section{The Googlebot}\label{the-googlebot}}

So it turned out to be very important to know the Googlebot. It's your
friend \ldots{} until it isn't. We went to our bosses and said words
that no one had said to them before: we did not want Google to index
these pages. In a news organization, the page view is the coin of the
realm. It is --- unfortunately --- how many things are evaluated when
the bosses ask if it was successful or not. So, with that in mind,
Google is your friend. Google brings you traffic. Indeed, Google is your
single largest referrer of traffic at a news organization, so you want
to throw the doors open and make friends with the Googlebot.

But here we were, saying Google wasn't our friend and that we needed to
keep the Googlebot out. And, thankfully, our bosses listened to our
argument. They too didn't want to be the first result in Google for
someone.

So, to make sure we were telling the Googlebot no, we used three lines
of defense. We told it no in robots.txt and on individual pages as a
meta tag, and we put the most interesting bits of data into a simple
JavaScript wrapper that made it hard on the bot if the first two things
failed.

The second solution had ramifications beyond the Googlebot. We decided
that we were not trying to make a complete copy of the public record.
That existed already. If you wanted to look at the actual public
records, the sheriff's offices in the area had websites and they were
the official keeper of the record. We were making browsing those images
easy, but we were not the public record.

That freedom had two consequences: it meant our scrapers could, at a
certain point and given a number of failures, just give up on getting a
mug. Data entered by humans will be flawed. There will be mistakes.
Because of that, our code would have to try and deal with that. Well,
there's an infinite number of ways people can mess things up, so we
decided that since we were not going to be an exact copy of the public
record, we could deal with the most common failures and dump the rest.
During testing, we were getting well over 98\% of mugs without having to
spend our lives coding for every possible variation of typo.

The second consequence of the decision actually came from the newspapers
lawyers. They asked a question that dumbfounded us: How long are you
keeping mugs? We never thought about it. Storage was cheap. We just
assumed we'd keep them all. But, why should we do that? If we're not a
copy of the public record, we dont have to keep them. And, since we
didnt know the result of each case, keeping them was really kind of
pointless.

So, we asked around: How long does a misdemeanor case take to reach a
judgement? The answer we got from various sources was about 60 days.
From arrest to adjudication, it took about two months. So, at the 60 day
mark, we deleted the data. We had no way of knowing if someone was
guilty or innocent, so all of them had to go. We even called the script
The Reaper.

We'd later learn that the practical impacts of this were nil. People
looked at the day's mugs and moved on. The amount of traffic a mug got
after the day of arrest was nearly zero.

\hypertarget{data-lifetimes}{%
\section{Data Lifetimes}\label{data-lifetimes}}

The life of your data matters. You have to ask yourself, Is it useful
forever? Does it become harmful after a set time? We had to confront the
real impact of deleting mugs after 60 days. People share them,
potentially lengthening their lifetime long after they've fallen off the
homepage. Delete them and that URL goes away.

We couldn't stop people from sharing links on social media---and indeed
probably didn't want to stop them from doing it. Heck, we did it while
we were building it. We kept IMing URLs to each other. And that's how we
realized we had a problem. All our work to minimize the impact on
someone wrongly accused of a crime could be damaged by someone sharing a
link on Facebook or Twitter.

There's a difference between frictionless and unobstructed sharing and
some reasonable constraints.

We couldn't stop people from posting a mug on Facebook, but we didn't
have to make it easy and we didn't have to put that mug front and
center. So we blocked Facebook from using the mug as the thumbnail image
on a shared link. And, after 60 days, the URL to the mug will throw a
404 page not found error. Because it's gone.

We couldn't block Google from memorializing someone's arrest, only to
let it live on forever on Facebook.

\hypertarget{you-are-a-data-provider}{%
\section{You Are a Data Provider}\label{you-are-a-data-provider}}

The last problem didn't come until months later. And it came in the
middle of the night. Two months after we launched, my phone rang at 1
a.m. This is never a good thing. It was my fellow developer, Jeremy
Bowers, now with NPR, calling me from a hotel in Washington DC where he
was supposed to appear in a wedding the next day. Amazon, which we were
using for image hosting, was alerting him that our bandwidth bills had
tripled on that day. And our traffic hadn't changed.

What was going on?

After some digging, we found out that another developer had scraped our
site---because we were so much easier to scrape than the Sheriff's
office sites---and had built a game out of our data called Pick the
Perp. There were two problems with this: 1. The game was going viral on
Digg (when it was still a thing) and Reddit. It was getting huge
traffic. 2. That developer had hotlinked our images. He/she was serving
them from our S3 account, which meant we were bearing the costs. And
they were going up exponentially by the minute.

What we didn't realize when we launched, and what we figured out after
Pick the Perp, was that we had become data provider, in a sense. We had
done the hard work of getting the data out of a website and we put it
into neat, semantic, easily digestible HTML. If you were after a stream
of mugshots, why go through all the hassle of scraping four different
sheriff's office's horrible HTML when you could just come get ours
easily?

Whoever built Pick the Perp, at least at the time, chose to use our
site. But, in doing so, they also chose to hotlink images---use the URL
of our S3 bucket, which cost us money---instead of hosting the images
themselves.

That was a problem we hadn't considered. People hotlink images all the
time. And, until those images are deleted from our system, they'll stay
hotlinked somewhere.

Amazon's S3 has a system where you can attach a key to a file that
expires after X period of time. In other words, the URL to your image
only lasts 15 minutes, or an hour, or however long you decide, before it
breaks. It gives you fine grained control over how long someone can use
your image URL.

So at 3 a.m., after two hours of pulling our hair out, we figured out
how to sync our image keys with our cache refreshes. So every 15
minutes, a url to an image expired and Pick the Perp came crashing down.

While the Pick the Perp example is an easy one---it's never cool to
hotlink an image---it does raise an issue to consider. Because you are
thinking carefully about how to build your app the right way doesn't
mean someone else will. And it doesn't mean they won't just go take your
data from your site. So how could you deal with that? Make the data
available as a download? Create an API that uses your same ethical
constructs? Terms of service? All have pros and cons and are worth
talking about before going forward.

\hypertarget{ethical-data}{%
\section{Ethical Data}\label{ethical-data}}

We live in marvelous times. The web offers you no end of tools to make
things on the web, to put data from here on there, to make information
freely available. But, we're an optimistic lot. Developers want to
believe that their software is being used only for good. And most people
will use it for good. But, there are times where the data you're working
with makes people uncomfortable. Indeed, much of journalism is about
making people uncomfortable, publishing things that make people angry,
or expose people who don't want to be exposed.

What I want you to think about, before you write a line of code, is what
does it mean to put your data on the internet? What could happen, good
and bad? What should you do to be responsible about it?

Because it can have consequences.

On Dec.~23, the Journal News in New York published a map of every legal
gun permit holder in their home circulation county. It was a public
record. They put it into Google Fusion Tables and Google dutifully
geocoded the addresses. It was a short distance to publication from
there.

Within days, angry gun owners had besieged the newspaper with
complaints, saying the paper had given criminals directions to people's
houses where they'd find valuable guns to steal. They said the paper had
violated their privacy. One outraged gun owner assembled a list of the
paper's staff, including their home addresses, telephone numbers, email
addresses and other details. The paper hired armed security to stand
watch at the paper.

By February, the New York state legislature removed handgun permits from
the public record, citing the Journal News as the reason.

There's no end of arguments to be had about this, but the simple fact is
this: The reason people were angry was because you could click on a dot
on the map and see a name and an address. In Fusion Tables, removing
that info window would take two clicks.

Because you can put data on the web does not mean you should put data on
the web. And there's a difference between a record being ``public'' and
``in front of a large audience.''

So before you write the first line of code, ask these questions:

\begin{itemize}
\tightlist
\item
  This data is public, but is it widely available? And does making it
  widely available and easy to use change anything?
\item
  Should this data be searchable in a search engine?
\item
  Does this data expose information someone has a reasonable expectation
  that it would remain at least semi-private?
\item
  Does this data change over time?
\item
  Does this data expire?
\item
  What is my strategy to update or delete data?
\item
  How easy should it be to share this data on social media?
\item
  How should I deal with other people who want this data? API? Bulk
  download?
\end{itemize}

Your answers to these questions will guide how you build your app. And
hopefully, it'll guide you to better decisions about how to build an app
with ethics in mind.

\bookmarksetup{startatroot}

\hypertarget{public-records}{%
\chapter{Public records}\label{public-records}}

Public records are the lifeblood of investigative reporting. They carry
their own philosophical framework, in a manner of speaking.

\begin{itemize}
\tightlist
\item
  Sunlight is the best disinfectant. Corruption hides in the shadows.
\item
  You paid for it with your taxes. It should be yours (with exceptions).
\item
  Journalism with a capital J is about holding the powerful accountable
  for their actions.
\end{itemize}

Keeping those things in mind as you navigate public records is helpful.

\hypertarget{federal-law}{%
\section{Federal law}\label{federal-law}}

Your access to public records and public meetings is a matter of the
law. As a journalist, it is your job to know this law better than most
lawyers. Which law applies depends on which branch of government you are
asking. In addition to documents and other kinds of information, FOIA
also provides access to structured datasets of the kind we'll use in
this class.

The Federal Government is covered by the Freedom of Information Act, or
FOIA. FOIA is not a universal term. Do not use it if you are not talking
to a federal agency. FOIA is a beacon of openness to the world. FOIA is
deeply flawed and frustrating.

Why?

\begin{itemize}
\tightlist
\item
  There is no real timetable with FOIA. Requests can take months, even
  years.
\item
  As a journalist, you can ask that your request be expedited.
\item
  Guess what? That requires review. More delays.
\item
  Exemptions are broad. National security, personal privacy, often
  overused.
\item
  Denied? You can appeal. More delays.
\end{itemize}

The law was enacted in 1966, but it's still poorly understood by most
federal employees, if not outright flouted by political appointees.
Lawsuits are common.

Post 9/11, the Bush administration rolled back many agency rules. Obama
ordered a ``presumption of openness'' but followed it with some of the
most restrictive policies ever seen. The Trump Administration, similar
to the Obama administration, claims to be the most transparent
administration, but has steadily removed records from open access and
broadly denied access to records.

Result? FOIA is in trouble.

\href{https://www.spj.org/foi-guide-pros.asp}{SPJ is a good resource}.

\hypertarget{state-law}{%
\section{State law}\label{state-law}}

States are -- generally -- more open than the federal government. The
distance between the government and the governed is smaller. Some
states, like Florida and Texas, are very open. Others, like Virginia and
Pennsylvania, are not. Maryland is somewhere in the middle.

These laws generally give you license to view -- and obtain a copy of --
a record held by a state or local government agency.

What is a public record? Generally speaking, public records are
information stored on paper or in an electronic format held by a state
or local government agency, but each state has its own list of types of
records -- called ``exemptions'' -- that are not subject to disclosure.

If a record has both exempt and non-exempt information mixed in, most
states require an agency to disclose it after removing the exempt
information, a process called ``redaction.'' Agencies aren't required to
create a record in order to fill your request.

In some states but not all -- the public information law (or related
case law) explicitly dictates that extracting a slice of a database
doesn't constitute creation of a record. Most states can charge you a
reasonable fee for time spent retrieving or copying records, though many
have provisions to waive those fees for journalists. Every state law
operates on a different timeline. Some only require agencies respond in
a ``reasonable'' time, but others spell out exactly how fast an agency
must respond to you, and how fast they must turn over the record.

\href{https://www.rcfp.org/open-government-guide/}{The Reporters
Committee For Freedom of the Press} has a good resource for learning the
law in your state.

Please and thank you will get you more records than any lawyer or
well-written request. Be nice. Be polite. And be persistent. Following
up regularly to check on status of a request lets an agency know they
can't ignore you (and some will try). Hunting for records is like any
other kind of reporting -- you have to do research. You have to ask
questions. Ask them: What records do you keep? For how long?

When requesting data, you are going to scare the press office and you
are going to confuse the agency lawyer. Request to have their data
person on the phone.

A good source of info? Records retention schedules, often required by
law or administrative rule at an agency. Here's an example from
\href{https://www.courts.state.md.us/sites/default/files/import/circuit/pdfs/retentionschedule_2330.pdf}{Maryland's
Circuit Courts}.

\bookmarksetup{startatroot}

\hypertarget{using-github}{%
\chapter{Using GitHub}\label{using-github}}

GitHub is a platform for managing and storing files, data and code built
atop Git, a popular open source version control software. GitHub
accounts are free and it's
\href{https://docs.github.com/en/get-started/quickstart}{easy to get
started}. The one prerequisite is that you have
\href{https://docs.github.com/en/get-started/quickstart/set-up-git}{Git
installed on your local computer}. There are installers for Mac, Windows
and Linux.

\hypertarget{how-it-works}{%
\section{How It Works}\label{how-it-works}}

Version control is based on the ideas that you want to keep track of
changes you make to a collection of files and that multiple people can
work together without getting in each other's way or having to do things
in a set order. For individual users, it's great for making sure that
you always have your work.

GitHub users work in what are known as repositories on their local
computers and also \emph{push} changes to a remote repository located on
GitHub. That remote repository is key: if you lose your computer, you
can fetch a version of your files from GitHub. If you want to work with
someone else on the same files, you can each have a local copy, push
changes to GitHub and then pull each others' changes back to your local
computers.

So, like Microsoft Word's track changes but with a remote backup and
multiple editors.

\hypertarget{getting-started}{%
\section{Getting Started}\label{getting-started}}

After installing Git and signing up for a GitHub account,
\href{https://docs.github.com/en/desktop/installing-and-configuring-github-desktop}{download
and install GitHub Desktop}. It will have you sign into your GitHub
account and then you'll have access to any existing repositories. If you
don't have any, that's fine! You can
\href{https://docs.github.com/en/desktop/installing-and-configuring-github-desktop/overview/creating-your-first-repository-using-github-desktop}{make
one locally}.

GitHub has
\href{https://docs.github.com/en/desktop/contributing-and-collaborating-using-github-desktop}{good
documentation for working in the Desktop app}, and while the emphasis in
this book will be on using GitHub for version control, it also supports
recording issues (read: problems or questions) with your files,
contributing to projects that aren't yours and more.

\hypertarget{advanced-use}{%
\section{Advanced Use}\label{advanced-use}}

Although our focus is on the GitHub Desktop app, you can use Git and
GitHub from your computer's command line interface, and GitHub has a
purpose-built \href{https://docs.github.com/en/github-cli}{command line
client}, too. GitHub can also serve as a publishing platform for many
types of files, and entire websites are hosted on
\href{https://docs.github.com/en/pages}{GitHub Pages}.

\bookmarksetup{startatroot}

\hypertarget{r-basics}{%
\chapter{R Basics}\label{r-basics}}

\href{https://www.r-project.org/about.html}{R} is a programming
language, one specifically geared toward data analysis.

Like all programming languages, it has certain built-in functions.

There are many ways you can write and execute R code. The first, and
most basic, is the console, shown here as part of a software tool called
\href{https://www.rstudio.com/products/rstudio/}{RStudio (Desktop Open
Source Edition)} that we'll be using all semester.

\begin{figure}

{\centering \includegraphics{./images/verybasics1.png}

}

\caption{ready}

\end{figure}

Think of the console like talking directly to the R language engine
that's busy working inside your computer. You use it send R commands,
making sure to use the only language it understands, which is R. The R
language engine processes those commands, and sends information back to
you.

Using the console is direct, but it has some drawbacks and some quirks
we'll get into later. Let's examine a basic example of how the console
works.

If you load up R Studio, type 2+2 into the console and hit enter it will
spit out the number 4, as displayed below.

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{2}\SpecialCharTok{+}\DecValTok{2}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

It's not very complex, and you knew the answer before hand, but you get
the idea. With R, we can compute things.

We can also store things for later use under a specific name. In
programming languages, these are called \textbf{variables}. We can
assign things to variables using this left-facing arrow:
\texttt{\textless{}-}. The \texttt{\textless{}-} is a called an
\textbf{assignment operator}.

If you load up R studio and type this code in the console\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{number }\OtherTok{\textless{}{-}} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

\ldots and then type this code, it will spit out the number 4, as show
below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{number }\SpecialCharTok{*}\NormalTok{ number}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4
\end{verbatim}

We can have as many variables as we can name. We can even reuse them
(but be careful you know you're doing that or you'll introduce errors).

If you load up R studio and type this code in the console\ldots{}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{firstnumber }\OtherTok{\textless{}{-}} \DecValTok{1}
\NormalTok{secondnumber }\OtherTok{\textless{}{-}} \DecValTok{2} 
\end{Highlighting}
\end{Shaded}

\ldots and then type this, it will split out the number 6, as shown
below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(firstnumber }\SpecialCharTok{+}\NormalTok{ secondnumber) }\SpecialCharTok{*}\NormalTok{ secondnumber}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 6
\end{verbatim}

We can store anything in a variable. A whole table. A list of numbers. A
single word. A whole book. All the books of the 18th century. Variables
are really powerful. We'll explore them at length.

A quick note about the console: After this brief introduction, we won't
spend much time in R Studio actually writing code directly into the
console. Instead, we'll write code in fancied-up text files --
interchangably called R Markdown or R Notebooks -- as will be explained
in the next chapter. But that code we write in those text files will
still \emph{execute} in the console, so it's good to know how it works.

\hypertarget{about-libraries}{%
\section{About libraries}\label{about-libraries}}

The real strength of any programming language is the external libraries
(often called ``packages'') that power it. The base language can do a
lot, but it's the external libraries that solve many specific problems
-- even making the base language easier to use.

With R, there are hundreds of free, useful libraries that make it easier
to do data journalism, created by a community of thousands of R users in
multiple fields who contribute to open-source coding projects.

For this class, we'll make use of several external libraries.

Most of them are part of a collection of libraries bundled into one
``metapackage'' called the
\href{https://www.tidyverse.org/packages/}{Tidyverse} that streamlines
tasks like:

\begin{itemize}
\tightlist
\item
  Loading data into R. (We'll use the
  \href{https://readr.tidyverse.org/}{readr} Tidyverse library)
\item
  Cleaning and reshaping the data before analysis. (We'll use the the
  \href{https://tidyr.tidyverse.org/index.html}{tidyr} and
  \href{https://dplyr.tidyverse.org/}{dplyr} Tidyverse libraries)
\item
  Data analysis. (We'll use the
  \href{https://dplyr.tidyverse.org/}{dplyr} Tidyverse library)
\item
  Data visualization (We'll use the
  \href{https://ggplot2.tidyverse.org/}{ggplot2} Tidyverse library)
\end{itemize}

To install packages, we use the function \texttt{install.packages()}.

You only need to install a library once, the first time you set up a new
computer to do data journalism work. You never need to install it again,
unless you want to update to a newer version of the package.

To install all of the Tidyverse libraries at once, the function is
\texttt{install.packages(\textquotesingle{}tidyverse\textquotesingle{})}.
You can type it directly in the console.

To use the R Markdown files mentioned earlier, we also need to install a
Tidyverse-related library that doesn't load as part of the core
Tidyverse package. The package is called, conveniently,
\href{https://rmarkdown.rstudio.com/docs/}{rmarkdown}. The code to
install that is
\texttt{install.packages(\textquotesingle{}rmarkdown\textquotesingle{})}

\bookmarksetup{startatroot}

\hypertarget{data-journalism-in-the-age-of-replication}{%
\chapter{Data journalism in the age of
replication}\label{data-journalism-in-the-age-of-replication}}

A single word in a single job ad for
\href{https://www.buzzfeednews.com/}{Buzzfeed News} posted in 2017
offered an indication of a profound shift in how data journalism is both
practiced and taught.

``We're looking for someone with a passion for news and a commitment to
using data to find amazing, important stories --- both quick hits and
deeper analyses that drive conversations,'' the posting seeking a data
journalist says. It goes on to list five things BuzzFeed is looking for:
Excellent collaborator, clear writer, deep statistical understanding,
knowledge of obtaining and restructuring data.

And then there's this:

\textbf{``You should have a strong command of at least one toolset that
(a) allows for filtering, joining, pivoting, and aggregating tabular
data, and (b) enables reproducible workflows.''}

The word you're seeing more and more of? Reproducible. And it started in
earnest in 2017 when data journalism crossed a major threshold in
American journalism: It got it's own section in the
\href{https://www.apstylebook.com/}{Associated Press Stylebook}.

``Data journalism has become a staple of reporting across beats and
platforms,'' the Data Journalism section of the Stylebook opens. ``The
ability to analyze quantitative information and present conclusions in
an engaging and accurate way is no longer the domain of specialists
alone.''

The AP's Data Journalism section discusses how to request data and in
what format, guidelines for scraping data from websites with automation,
the ethics of using leaked or hacked data and other topics long part of
data journalism conference talks.

But the third page of the section contains perhaps the most profound
commandment: \textbf{``As a general rule, all assertions in a story
based on data analysis should be reproducible. The methodology
description in the story or accompanying materials should provide a road
map to replicate the analysis.''}

Reproducible research -- replication -- is a cornerstone of scientific
inquiry. Researchers across a range of academic disciplines use methods
to find new knowledge and publish it in peer reviewed journals. And,
when it works, other researchers take that knowledge and try it with
their own samples in their own locations. Replication studies exist to
take something from an ``interesting finding'' to a ``theory'' and
beyond.

It doesn't always work.

Replication studies aren't funded at nearly the level as new research.
And, to the alarm of many, scores of studies can't be replicated by
others. Researchers across disciplines are finding that when their
original studies are replicated, flaws are found, or the effects found
aren't as strong as the original. Because of this, academics across a
number of disciplines have written about a replication crisis in their
respective fields, particularly psychology, social science and medical
research.

In Chapter 1 of the
\href{https://www.amazon.com/New-Precision-Journalism-Midland-Book/dp/0253206642}{New
Precision Journalism}, Phil Meyer wrote that ``we journalists would be
wrong less often if we adapted to our own use some of the research tools
of the social scientists.''

Meyer would go on to write about how computers pouring over datasets too
large to crunch by hand had changed social science from a discipline
with ``a few data and a lot of interpretation'' into a much more
meaningful and powerful area of study. If journalists could become
comfortable with data and some basic statistics, they too could harness
this power.

``It used to be said that journalism is history in a hurry,'' Meyer
wrote. ``The argument of this book is that to cope with the acceleration
of social change in today's world, journalism must become social science
in a hurry.''

He wrote that in 1971. It might as well have been yesterday.

Journalism doesn't have a history of replication, but the concerns about
credibility are substantially greater. Trust in media is at an all time
low and shows no signs of improving. While the politics of the day have
quite a bit to do with this mistrust of media, being more transparent
about what journalists do can't hurt.

The AP's commandment that ``Thou must replicate your findings'' could,
if taken seriously by the news business, have substantial impacts on how
data journalism gets done in newsrooms and how data journalism gets
taught, both at professional conferences and universities.

How? Two ways.

\begin{itemize}
\tightlist
\item
  The predominant way that data journalism gets done in a newsroom is
  through simple tools like Microsoft Excel or Google Sheets. Those
  simple tools, on their own, lack significant logging functions that
  automatically keep track of steps a data journalist took to reach a
  given conclusion. That means journalists using those tools have to
  maintain separate, detailed logs of what they did so any analysis can
  be replicated.
\item
  The predominant way that data journalism gets taught -- both in
  professional settings and at most universities -- doesn't deal with
  replication at all. The tools and the training stress ``getting things
  done'' -- an entirely logical focus for a deadline driven business.
  The choices of tools -- like spreadsheet programs -- are made to get
  from data to story as quick as possible, without frightening away math
  and tech phobic students.
\end{itemize}

If the AP's replication rules are to be followed, journalism needs to
become much more serious about the tools and techniques used to do data
journalism. The days of ``point and click'' tools to do ``quick and
dirty'' analysis that get published are dying. The days of formal
methods using documented steps are here.

\hypertarget{the-stylebook}{%
\section{The stylebook}\label{the-stylebook}}

Troy Thibodeaux, the editor of the AP's data journalism team, said the
stylebook entry started when the data team found themselves answering
the same questions over and over. With a grant from the Knight
Foundation, the team began to document their own standards and turn that
into a stylebook section.

From the beginning, they had a fairly clear idea of what they wanted to
do -- think through a project and ask what the frequently asked
questions are that came up. It was not going to be a soup-to-nuts guide
to how to do a data project.

When the section came out, eyebrows went up on the replication parts,
surprising Thibodeaux.

``From our perspective, this is a core value for us,'' he said. ``Just
for our own benefit, we need to be able to have someone give us a second
set of eyes. We benefit from that every day. We catch things for each
other.''

Thibodeaux said the AP data team has two audiences when it comes to
replication -- they have the readers of the work, and members of the
collective who may want to do their own work with the data.

``This is something that's essential to the way we work,'' he said.
``And it's important in terms of transparency and credibility going
forward. We thought it would be kind of unexceptionable.''

\hypertarget{replication}{%
\section{Replication}\label{replication}}

Meyer, now 86, said he's delighted to see replication up for discussion
now, but warned that we shouldn't take it too far.

``Making the analysis replicable was something I worried about from the
very beginning,'' he wrote in an email. So much so that in 1967, after
publishing stories from his landmark survey after the Detroit riots, he
shipped the data and backup materials about it to a social science data
repository at the University of North Carolina.

And, in doing so, he opened the door to others replicating his results.
One scholar attempted to find fault with Meyer's analysis by slicing the
data ever thinner until the differences weren't significant -- gaming
the analysis to criticize the stories.

Meyer believes replication is vitally important, but doesn't believe it
should take on the trappings of science replication, where newsrooms
take their own samples or re-survey a community. That would be
prohibitively expensive.

But journalists should be sharing their data and analysis steps. And it
doesn't need to be complicated, he said.

``Replication is a theoretical standard, not a requirement that every
investigator duplicate his or her own work for every project,'' he said.
``Giving enough information in the report to enable another investigator
to follow in your footsteps is enough. Just telling enough to make
replication possible will build confidence.''

But as simple as that sounds, it's not so simple. Ask social scientists.

Andrew Gelman, a professor of statistics and political science and
director of the Applied Statistics Center at Columbia University, wrote
in the journal CHANCE that difficulties with replication in empirical
research are pervasive.

``When an outsider requests data from a published paper, the authors
will typically not post or send their data files and code, but instead
will point to their sources, so replicators have to figure out exactly
what to do from there,'' Gelman wrote. ``End-to-end replicability is not
the norm, even among scholars who actively advocate for the principles
of open science.''

So goes science, so goes journalism.

Until a recent set of exceptions, journalists rarely shared data. The
``nerd box'' -- a sidebar story that explains how a news organization
did what they did -- is a term that first appeared on NICAR-L, a email
listserv of data journalists, in the 1990s.

It was a form born in print.

As newsrooms adapted to the internet, some news organizations began
linking to their data sources if they were online. Often, the data used
in stories were obtained through records requests. Sometimes, reporters
created the data themselves.

Journalism, more explicitly than science, is a competitive business.
There have been arguments that nerd boxes and downloadable links give
too much away to competitors.

Enter the AP Stylebook.

The AP Stylebook argues explicitly for both internal and external
replication. Externally, they argue that the \textbf{``methodology
description in the story or accompanying materials should provide a road
map to replicate the analysis''}, meaning someone else could do the
replication post publication.

Internally, the AP Stylebook says: \textbf{``If at all possible, an
editor or another reporter should attempt to reproduce the results of
the analysis and confirm all findings before publication.''}

There are two problems here.

First is that journalism, unlike science, has no history of replication.
There is no ``scientific method'' for stories. There is no standard
``research methods'' class taught at every journalism school, at least
not where it comes to writing stories. And, beyond that, journalism
school isn't a requirement to get into the news business. In other
words, journalism lacks the standards other disciplines have.

The second problem is, in many ways, worse: Except for the largest
newsrooms, most news organizations lack editors who could replicate the
analysis. Many don't have a second person who would know what to do.

Not having a second set of eyes in a newsroom is a problem, Thibodeaux
acknowledges. Having a data journalism team ``is an incredible luxury''
at the AP, he said, and their rule is nothing goes on the wire without a
second set of eyes.

Thibodeaux, for his part, wants to see fewer ``lone nerds in the
corner'' -- it's too much pressure. That person gets too much
credibility from people who don't understand what they do, and they get
too much blame when a mistake is made.

So what would replication look like in a newsroom? What does this mean
for how newsrooms do data journalism on deadline?

\hypertarget{goodbye-excel}{%
\section{Goodbye Excel?}\label{goodbye-excel}}

For decades, Excel has been the gateway drug for data journalists, the
Swiss Army knife of data tools, the ``One Tool You Can't Live Without.''
Investigative Reporters and Editors, an organization that trains
investigative journalists, have built large amounts of their curricula
around Excel. Of the journalism schools that teach data journalism, most
of them begin and end with spreadsheets.

The Stylebook says at a minimum, today's data journalists should keep a
log that details:

\begin{itemize}
\tightlist
\item
  The source of the data, making sure to work on a copy of the data and
  not the original file.
\item
  Data dictionaries or any other supporting documentation of the data.
\item
  \textbf{``Description of all steps required to transform the data and
  perform the analysis.''}
\end{itemize}

The trouble with Excel (or Google Sheets) is, unless you are keeping
meticulous notes on what steps you are taking, there's no way to keep
track. Many data journalists will copy and paste the values of a formula
over the formula itself to prevent Excel from fouling up cell references
when moving data around -- a practical step that also cuts off another
path to being able to replicate the results.

An increasing number of data journalists are switching to tools like
analysis notebooks, which use languages like Python and R, to document
their work. The notebooks, generally speaking, allow a data journalist
to mix code and explanation in the same document.

Combined with online sharing tools like GitHub, analysis notebooks seem
to solve the problem of replication. But the number using them is small
compared to those using spreadsheets. Recent examples of news
organizations using analysis notebooks include the
\href{https://github.com/datadesk}{Los Angeles Times}, the
\href{https://github.com/TheUpshot}{New York Times},
\href{https://github.com/fivethirtyeight/data}{FiveThirtyEight}, and
\href{https://github.com/BuzzFeedNews}{Buzzfeed}.

Peter Aldous, a data journalist at Buzzfeed recently published a story
about how the online news site used machine learning to
\href{https://www.buzzfeednews.com/article/peteraldhous/us-marshals-spy-plane-over-mexico\#.qqYnVj0B}{find
airplanes being used to spy on people in American cities}. Published
with the story is the
\href{https://github.com/BuzzFeedNews/2017-08-spy-plane-finder}{code
Aldous used to build his case}.

``I think of it this way: As a journalist, I don't like to simply trust
what people tell me. Sometimes sources lie. Sometimes they're just
mistaken. So I like to verify what I'm told,'' he wrote in an email.
``By the same token, why should someone reading one of my articles
believe my conclusions, if I don't provide the evidence that explains
how I reached them?''

The methodology document, associated code and source data took Aldous a
few hours to create. The story, from the initial data work through the
reporting required to make sense of it all, took a year. Aldous said
there wasn't a discussion about if the methodology would be published
because it was assumed -- ``it's written into our DNA at BuzzFeed
News.''

``My background is in science journalism, and before that (way back in
the 1980s) in science,'' Aldous said. ``In science, there's been a shift
from descriptive methods sections to publishing data and analysis code
for reproducible research. And I think we're seeing a similar shift in
data journalism. Simply saying what you've done is not as powerful as
providing the means for others to repeat and build on your work.''

Thibodeaux said that what Buzzfeed and others do with analysis notebooks
and code repositories that include their data is ``lovely.''

``That to me is the shining city on the hill,'' Thibodeaux said. ``We're
not going to get there, and I don't think we have to for every story and
every use case, and I don't think it's necessarily practical for every
person working with data to get to that point.''

There's a wide spectrum of approaches that still gets journalists to the
essence of what the stylebook is trying to do, Thibodeaux said. There
are many tools, many strategies, and the AP isn't going to advocate for
any single one of them, he said. They're just arguing for transparency
and replicability, even if that means doing more work.

``There's a certain burden that comes with transparency,'' he said.
``And I think we have to accept that burden.''

The question, Thibodeaux said, is what is sufficient? What's enough
transparency? What does someone need for replicability?

``Maybe we do have to set a higher standard -- the more critical the
analysis is to the story, and the more complex that analysis is, that's
going to push the bar on what is a sufficient methodology statement,''
he said. ``And it could end up being a whole code repo in order to just
say, this isn't black magic, here's how we got it if you're so
interested.''

\hypertarget{receptivity-is-high}{%
\section{``Receptivity \ldots{} is high''}\label{receptivity-is-high}}

Though written almost half a century ago, Meyer foresaw how data
journalism was going to arrive in the newsroom.

``For the new methods to gain currency in journalism, two things must
happen,'' he wrote. ``Editors must feel the need strongly enough to
develop the in-house capacity for systematic research \ldots{} The
second need, of course, is for the editors to be able to find the talent
to fill this need.''

Meyer optimistically wrote that journalism schools were prepared to
provide that talent -- they were not then, and only small handful are
now -- but students were unlikely to be drawn to these new skills if
they didn't see a chance to use those skills in their careers.

It's taken 45 years, but we are now at this point.

``The potential for receptivity, especially among the younger generation
of newspaper managers, is high,'' Meyer wrote.

\hypertarget{replication-in-notebooks}{%
\section{Replication in notebooks}\label{replication-in-notebooks}}

For our purposes in this book, replication requires two things from you,
the student: What and why. What is this piece of code doing, and why are
you doing that here and now? What lead you to this place? That you can
copy and paste code from this book or the internet is not impressive.
What is necessary for learning is that you know what a piece of code is
doing a thing and why you want to do that thing here.

How will we replicate? We'll make use of special text files -- R
Markdown, also known as R Notebooks -- that combine contextual text; the
code we use to load, clean, analyze and visualize data; and the output
of that code that allowed us to draw certain conclusions to use in
stories.

In an R Notebook, there are two blocks: A block that uses markdown,
which has no special notation, and a code block. The code blocks can run
mulitple languages inside R Studio. There's R, of course, but we could
also run Python, a general purpose scripting language; and SQL, or
Structured Query Language, the language of databases.

For the rest of the class, we're going to be working in notebooks.

In notebooks, you will both run your code and explain each step, much as
I am doing here in this online book. This entire book was produced with
R markdown files.

To start a notebook in R Studio, you click on the green plus in the top
left corner and go down to R Notebook.

\includegraphics{./images/verybasics2.png}

In our first lab, we'll go through the process of editing a markdown
notebook.

\bookmarksetup{startatroot}

\hypertarget{aggregates}{%
\chapter{Aggregates}\label{aggregates}}

\hypertarget{libraries}{%
\section{Libraries}\label{libraries}}

R is a statistical programming language that is purpose built for data
analysis.

Base R does a lot, but there are a mountain of external libraries that
do things to make R better/easier/more fully featured. We already
installed the tidyverse -- or you should have if you followed the
instructions for the last assignment -- which isn't exactly a library,
but a collection of libraries. Together, they make up the Tidyverse.
Individually, they are extraordinarily useful for what they do. We can
load them all at once using the tidyverse name, or we can load them
individually. Let's start with individually.

The two libraries we are going to need for this assignment are
\texttt{readr} and \texttt{dplyr}. The library \texttt{readr} reads
different types of data in. For this assignment, we're going to read in
csv data or Comma Separated Values data. That's data that has a comma
between each column of data.

Then we're going to use \texttt{dplyr} to analyze it.

To use a library, you need to import it. Good practice -- one I'm going
to insist on -- is that you put all your library steps at the top of
your notebooks.

That code looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\end{Highlighting}
\end{Shaded}

To load them both, you need to do this:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(readr)}
\FunctionTok{library}\NormalTok{(dplyr)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Attaching package: 'dplyr'
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:stats':

    filter, lag
\end{verbatim}

\begin{verbatim}
The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union
\end{verbatim}

But, because those two libraries -- and several others that we're going
to use over the course of this class -- are so commonly used, there's a
shortcut to loading all of the libraries we'll need:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

You can keep doing that for as many libraries as you need.

\hypertarget{importing-data}{%
\section{Importing data}\label{importing-data}}

The first thing we need to do is get some data to work with. We do that
by reading it in. In our case, we're going to read a datatable from an
``rds'' file, which is a format for storing data with R. Later in the
course, we'll more frequently work with a format called a CSV. A CSV is
a stripped down version of a spreadsheet you might open in a program
like Excel, in which each column is separated by a comma. RDS files are
less common when getting data from other people. But reading in CSVs is
less foolproof than reading in rds files, so for now we'll work with
rds.

The rds file we're going to read in contains information about classes
offered at the University of Maryland since 2017.

So step 1 is to import the data. The code to import the data looks like
this:

\texttt{umd\_courses\ \textless{}-\ read\_rds("umd\_courses.rds")}

Let's unpack that.

The first part -- \textbf{umd\_courses} -- is the name of a variable.

A \textbf{variable} is just a name that we'll use to refer to some more
complex thing. In this case, the more complex thing is the data we're
importing into R that will be stored as a \textbf{dataframe}, which is
one way R stores data.

We can call this variable whatever we want. The variable name doesn't
matter, technically. We could use any word. You could use your first
name, if you like. Generally, though, we want to give variables names
that are descriptive of the thing they refer to. Which is why we're
calling this one \textbf{umd\_courses}. Variable names, by convention
are one word all lower case (or two or more words connected by an
underscore). You can end a variable with a number, but you can't start
one with a number.

The \texttt{\textless{}-} bit, you'll recall from the basics, is the
\textbf{variable assignment operator}. It's how we know we're assigning
something to a word. Think of the arrow as saying ``Take everything on
the right of this arrow and stuff it into the thing on the left.'' So
we're creating an empty vessel called \textbf{umd\_courses} and stuffing
all this data into it.

\textbf{read\_rds()} is a function, one that only works when we've
loaded the tidyverse. A \textbf{function} is a little bit of computer
code that takes in information and follows a series of pre-determined
steps and spits it back out. A recipe to make pizza is a kind of
function. We might call it \textbf{make\_pizza()}.

The function does one thing. It takes a preset collection of ingredients
-- flour, water, oil, cheese, tomato, salt -- and passes them through
each step outlined in a recipe, in order. Things like: mix flour and
water and oil, knead, let it sit, roll it out, put tomato sauce and
cheese on it, bake it in an oven, then take it out.

The output of our \textbf{make pizza()} function is a finished pie.

We'll make use of a lot of pre-written functions from the tidyverse and
other packages, and even write some of our own. Back to this line of
code:

\texttt{umd\_courses\ \textless{}-\ read\_rds("umd\_courses.rds")}

Inside of the \textbf{read\_rds()} function, we've put the name of the
file we want to load. Things we put inside of function, to customize
what the function does, are called \textbf{arguments}.

The easiest thing to do, if you are confused about how to find your
data, is to put your data in the same folder as as your notebook (you'll
have to save that notebook first). If you do that, then you just need to
put the name of the file in there (maryland\_winred.rds). If you put
your data in a folder called ``data'' that sits next to your data
notebook, your function would instead look like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"data/umd\_courses.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In this data set, each row represents a course offered at UMD by a
department during a specific term, with some other information
optionally included, like seats in the class, the instructor(s) and a
description.

After loading the data, it's a good idea to get a sense of its shape.
What does it look like? There are several ways we can examine it.

By looking in the R Studio environment window, we can see the number of
rows (called ``obs.'', which is short for observations), and the number
of columns (called variables). We can double click on the dataframe name
in the environment window, and explore it like a spreadsheet.

There are several useful functions for getting a sense of the dataset
right in our markdown document.

If we run \texttt{glimpse(umd\_courses)}, it will give us a list of the
columns, the data type for each column and and the first few values for
each column.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(umd\_courses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 60,672
Columns: 8
$ id             <chr> "LING889", "LING200", "LING689", "LING499", "LING899", ~
$ title          <chr> "Directed Research", "Introductory Linguistics", "Indep~
$ description    <chr> NA, "Credit only granted for: HESP120 or LING200.\nAddi~
$ term           <dbl> 202112, 202112, 202112, 202112, 202112, 202112, 202112,~
$ department     <chr> "Linguistics", "Linguistics", "Linguistics", "Linguisti~
$ instructors    <chr> NA, "Michelle Morrison", NA, NA, NA, NA, NA, NA, NA, NA~
$ seats          <dbl> 0, 20, 0, 0, 0, 0, 0, 0, 0, 0, 30, 45, 5, 1, 34, 29, 6,~
$ syllabus_count <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,~
\end{verbatim}

If we type \texttt{head(umd\_courses)}, it will print out the columns
and the first six rows of data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(umd\_courses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 8
  id      title   description   term department instructors seats syllabus_count
  <chr>   <chr>   <chr>        <dbl> <chr>      <chr>       <dbl>          <dbl>
1 LING889 Direct~  <NA>       202112 Linguisti~ <NA>            0             NA
2 LING200 Introd~ "Credit on~ 202112 Linguisti~ Michelle M~    20             NA
3 LING689 Indepe~  <NA>       202112 Linguisti~ <NA>            0             NA
4 LING499 Direct~  <NA>       202112 Linguisti~ <NA>            0             NA
5 LING899 Doctor~  <NA>       202112 Linguisti~ <NA>            0             NA
6 LING698 Direct~  <NA>       202112 Linguisti~ <NA>            0             NA
\end{verbatim}

We can also click on the data name in the R Studio environment window to
explore it interactively.

\hypertarget{group-by-and-count}{%
\section{Group by and count}\label{group-by-and-count}}

So what if we wanted to know how many classes were offered in each term?

To do that by hand, we'd have to take each of the 60,672 individual rows
(or observations or records) and sort them into a pile. We'd put them in
groups -- one for each term -- and then count them.

\texttt{dplyr} has a group by function in it that does just this. A
massive amount of data analysis involves grouping like things together
and then doing simple things like counting them, or averaging them
together. So it's a good place to start.

So to do this, we'll take our dataset and we'll introduce a new
operator: \texttt{\%\textgreater{}\%}. The best way to read that
operator, in my opinion, is to interpret that as ``and then do this.''
The keyboard shortcut for this is cmd-shift-m on the Mac and
ctrl-shift-m on Windows. Don't like that character? R now has a new one
that does the same thing: \texttt{\textbar{}\textgreater{}}.

We're going to establish a pattern that will come up again and again
throughout this book: \texttt{data\ \%\textgreater{}\%\ function}. In
English: take your data set and then do this specific action to it.

The first step of every analysis starts with the data being used. Then
we apply functions to the data.

In our case, the pattern that you'll use many, many times is:
\texttt{data\ \%\textgreater{}\%\ group\_by(COLUMN\ NAME)\ \%\textgreater{}\%\ summarize(VARIABLE\ NAME\ =\ AGGREGATE\ FUNCTION(COLUMN\ NAME))}

In our dataset, the column with term information is called ``term''

Here's the code to count the number of courses in each term:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(term) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count\_classes =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 22 x 2
     term count_classes
    <dbl>         <int>
 1 201712           487
 2 201801          4479
 3 201805          1231
 4 201808          4465
 5 201905          1183
 6 201908          4537
 7 201912           516
 8 202001          4648
 9 202005          1357
10 202008          4508
# i 12 more rows
\end{verbatim}

So let's walk through that.

We start with our dataset -- \texttt{umd\_courses} -- and then we tell
it to group the data by a given field in the data. In this case, we
wanted to group together all the terms, signified by the field name
\texttt{term}, which you could get from using the glimpse() function.
After we group the data, we need to count them up.

In dplyr, we use the \texttt{summarize()} function,
\href{http://dplyr.tidyverse.org/reference/summarise.html}{which can do
alot more than just count things}.

Inside the parentheses in summarize, we set up the summaries we want. In
this case, we just want a count of the number of classes for each term
grouping. The line of code \texttt{count\_classes\ =\ n(),} says create
a new field, called \texttt{count\_classes} and set it equal to
\texttt{n()}. \texttt{n()} is a function that counts the number of rows
or records in each group. Why the letter n? The letter n is a common
symbol used to denote a count of something. The number of things (or
rows or observations or records) in a dataset? Statisticians call it
n.~There are n number of classes in this dataset.

When we run that, we get a list of terms with a count next to them. But
it's not in any order.

So we'll add another ``and then do this'' symbol -- \%\textgreater\% --
and use a new function called \texttt{arrange()}. Arrange does what you
think it does -- it arranges data in order. By default, it's in
ascending order -- smallest to largest. But if we want to know the term
with the most classes, we need to sort it in descending order. That
looks like this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(term) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count\_classes =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(count\_classes))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 22 x 2
     term count_classes
    <dbl>         <int>
 1 202208          4975
 2 202308          4785
 3 202001          4648
 4 202301          4578
 5 202108          4545
 6 201908          4537
 7 202008          4508
 8 202201          4493
 9 201801          4479
10 201808          4465
# i 12 more rows
\end{verbatim}

The term labeled \texttt{202208}, representing the Fall 2022 term, has
the most classes.

We can, if we want, group by more than one thing. The courses data
contains a column detailing the department.

We can group by ``term'' and ``department'' to see how many courses each
department offered in each term. We'll sort by department and term.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(term, department) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count\_classes =} \FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(term, department)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'term'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{verbatim}
# A tibble: 3,237 x 3
# Groups:   term [22]
     term department                                   count_classes
    <dbl> <chr>                                                <int>
 1 201712 African American Studies                                 6
 2 201712 Agricultural and Resource Economics                      1
 3 201712 American Studies                                         4
 4 201712 Animal Science                                           5
 5 201712 Anthropology                                            25
 6 201712 Applied Mathematics & Scientific Computation             3
 7 201712 Arabic                                                   1
 8 201712 Art History & Archaeology                                3
 9 201712 Art Studio                                              13
10 201712 Asian American Studies                                   1
# i 3,227 more rows
\end{verbatim}

\hypertarget{other-summarization-methods-summing-mean-median-min-and-max}{%
\section{Other summarization methods: summing, mean, median, min and
max}\label{other-summarization-methods-summing-mean-median-min-and-max}}

In the last example, we grouped like records together and counted them,
but there's so much more we can do to summarize each group.

Let's say we wanted to know the total number of seats offered in each
term? For that, we could use the \texttt{sum()} function to add up all
of the values in the column ``seats''. We put the column we want to
total -- ``seats'' -- inside the sum() function \texttt{sum(seats)}.
Note that we can simply add a new summarize function here, keeping our
count\_classes field in our output table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(term) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count\_classes =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{total\_seats =} \FunctionTok{sum}\NormalTok{(seats)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_seats))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 22 x 3
     term count_classes total_seats
    <dbl>         <int>       <dbl>
 1 202208          4975      230796
 2 202108          4545      223321
 3 202308          4785      221036
 4 202008          4508      217314
 5 201908          4537      217075
 6 201808          4465      208839
 7 202301          4578      207465
 8 202201          4493      206075
 9 202101          4400      204248
10 202001          4648      202688
# i 12 more rows
\end{verbatim}

We can also calculate the average number of seats for each department --
the mean -- and the number that sits at the midpoint of our data -- the
median.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(term) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count\_classes =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{total\_seats =} \FunctionTok{sum}\NormalTok{(seats),}
    \AttributeTok{mean\_seats =} \FunctionTok{mean}\NormalTok{(seats),}
    \AttributeTok{median\_seats =} \FunctionTok{median}\NormalTok{(seats)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_seats))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 22 x 5
     term count_classes total_seats mean_seats median_seats
    <dbl>         <int>       <dbl>      <dbl>        <dbl>
 1 202208          4975      230796       46.4           22
 2 202108          4545      223321       49.1           25
 3 202308          4785      221036       46.2           24
 4 202008          4508      217314       48.2           22
 5 201908          4537      217075       47.8           24
 6 201808          4465      208839       46.8           20
 7 202301          4578      207465       45.3           25
 8 202201          4493      206075       45.9           24
 9 202101          4400      204248       46.4           24
10 202001          4648      202688       43.6           20
# i 12 more rows
\end{verbatim}

We see something interesting here. The mean number of seats is higher
than the median number in most cases, but the difference isn't always
huge. In some cases the mean gets skewed by larger or lower amounts.
Examining both the median -- which is less sensitive to extreme values
-- and the mean -- which is more sensitive to extreme values -- gives
you a clearer picture of the composition of the data.

What about the highest and lowest number of seats for each department?
For that, we can use the \texttt{min()} and \texttt{max()} functions.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(department) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count\_classes =} \FunctionTok{n}\NormalTok{(),}
    \AttributeTok{total\_seats =} \FunctionTok{sum}\NormalTok{(seats),}
    \AttributeTok{mean\_seats =} \FunctionTok{mean}\NormalTok{(seats),}
    \AttributeTok{median\_seats =} \FunctionTok{median}\NormalTok{(seats),}
    \AttributeTok{min\_seats =} \FunctionTok{min}\NormalTok{(seats),}
    \AttributeTok{max\_seats =} \FunctionTok{max}\NormalTok{(seats)}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_seats))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 211 x 7
   department        count_classes total_seats mean_seats median_seats min_seats
   <chr>                     <int>       <dbl>      <dbl>        <dbl>     <dbl>
 1 Business and Man~          1900      162543       85.5           45         0
 2 Computer Science           1067      118136      111.            45         0
 3 Mathematics                1030      106235      103.            27         0
 4 English                    1617      102124       63.2           23         0
 5 Biological Scien~          1199       97653       81.4           31         0
 6 Chemistry                   669       85176      127.            38         0
 7 Information Stud~          1127       78820       69.9           40         0
 8 Engineering Scie~           609       78807      129.            45         0
 9 Economics                  1137       71420       62.8           35         0
10 Communication              1440       70165       48.7           30         0
# i 201 more rows
# i 1 more variable: max_seats <dbl>
\end{verbatim}

From this, we can see that every department has courses with 0 seats
listed. We'll need to account for those in any analysis, probably by
excluding them. The variation in mean and median seats is interesting,
too: look at Mathematics and English compared to Computer Science and
Engineering, for example. These patterns aren't random: departments make
choices about the courses they offer.

It would be interesting to see what the course with the largest number
of seats is. To do that, we could simply take our original data set and
sort it from highest to lowest on seats

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(seats))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 60,672 x 8
   id      title  description   term department instructors seats syllabus_count
   <chr>   <chr>  <chr>        <dbl> <chr>      <chr>       <dbl>          <dbl>
 1 UNIV100 The S~ "Credit on~ 201808 Universit~ Ciarra Joy~  2876             NA
 2 UNIV100 The S~ "Credit on~ 202108 Universit~ Raymond Na~  2685             NA
 3 UNIV100 The S~ "Credit on~ 202208 Universit~ Vickie Hil~  2575             NA
 4 ENES140 Disco~ "Additiona~ 202201 Engineeri~ James Green  2500             NA
 5 ENES140 Disco~ "Additiona~ 202101 Engineeri~ James Green  2500             NA
 6 ENES140 Disco~ "Additiona~ 202001 Engineeri~ James Green  2500             NA
 7 ENES140 Disco~ "Additiona~ 201801 Engineeri~ James Green  2500             NA
 8 ENES140 Disco~ "Additiona~ 202301 Engineeri~ James Green  2500              1
 9 UNIV100 The S~ "Credit on~ 202308 Universit~ Maiya Smit~  2409              4
10 UNIV100 The S~ "Credit on~ 202008 Universit~ Raymond Na~  2233             NA
# i 60,662 more rows
\end{verbatim}

UNIV100 and ENES140 have a LOT of seats. Are they virtual classes?
Hybrid? What else could we ask of this data?

\bookmarksetup{startatroot}

\hypertarget{filters-and-selections}{%
\chapter{Filters and selections}\label{filters-and-selections}}

More often than not, we have more data than we want. Sometimes we need
to be rid of that data. In \texttt{dplyr}, there's two ways to go about
this: filtering and selecting.

\textbf{Filtering creates a subset of the data based on criteria}. All
records where the amount is greater than 150,000. All records that match
``College Park''. Something like that. \textbf{Filtering works with rows
-- when we filter, we get fewer rows back than we start with.}

\textbf{Selecting simply returns only the fields named}. So if you only
want to see city and amount, you select those fields. When you look at
your data again, you'll have two columns. If you try to use one of your
columns that you had before you used select, you'll get an error.
\textbf{Selecting works with columns. You will have the same number of
records when you are done, but fewer columns of data to work with.}

Let's continue to work with the UMD course data we used in the previous
chapter. First, we need to load the tidyverse:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{umd\_courses }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"data/umd\_courses.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

If we want to see only those courses offered a particular department, we
can use the \texttt{filter} function to isolate just those records.
Filter works with something called a comparison operator. We need to
filter all records equal to ``Journalism''. The comparison operators in
R, like most programming languages, are == for equal to, != for not
equal to, \textgreater{} for greater than, \textgreater= for greater
than or equal to and so on.

\textbf{Be careful: \texttt{=} is not \texttt{==} and \texttt{=} is not
``equal to''. \texttt{=} is an assignment operator in most languages --
how things get named.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{journalism\_courses }\OtherTok{\textless{}{-}}\NormalTok{ umd\_courses }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(department }\SpecialCharTok{==} \StringTok{"Journalism"}\NormalTok{)}

\FunctionTok{head}\NormalTok{(journalism\_courses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 8
  id       title  description   term department instructors seats syllabus_count
  <chr>    <chr>  <chr>        <dbl> <chr>      <chr>       <dbl>          <dbl>
1 JOUR282  Beyon~ "Credit on~ 202112 Journalism Denitsa Yo~    35             NA
2 JOUR698  Speci~  <NA>       202112 Journalism <NA>            0             NA
3 JOUR175  Media~ "Additiona~ 202112 Journalism Susan Moel~    20             NA
4 JOUR199  Surve~  <NA>       202112 Journalism Karen Denny     5             NA
5 JOUR130  Self-~ "Credit on~ 202112 Journalism Amber Moore    25             NA
6 JOUR459W Speci~ "This cour~ 202112 Journalism Shannon Sc~    23             NA
\end{verbatim}

And just like that, we have just Journalism results, which we can verify
looking at the head, the first six rows.

We also have more data than we might want. For example, we may only want
to work with the course id and title.

To simplify our dataset, we can use select.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{selected\_journalism\_courses }\OtherTok{\textless{}{-}}\NormalTok{ journalism\_courses }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(id, title)}

\FunctionTok{head}\NormalTok{(selected\_journalism\_courses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  id       title                                                                
  <chr>    <chr>                                                                
1 JOUR282  Beyond Facebook: How Social Media are Transforming Society, Culture,~
2 JOUR698  Special Problems in Communication                                    
3 JOUR175  Media Literacy                                                       
4 JOUR199  Survey Apprenticeship                                                
5 JOUR130  Self-Presentation in the Age of YouTube                              
6 JOUR459W Special Topics in Journalism; Sports Media & Athlete Branding        
\end{verbatim}

And now we only have two columns of data for whatever analysis we might
want to do.

\hypertarget{combining-filters}{%
\section{Combining filters}\label{combining-filters}}

So let's say we wanted to see all the courses in the Theatre department
with at least 15 seats. We can do this a number of ways. The first is we
can chain together a whole lot of filters.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{theatre\_seats\_15 }\OtherTok{\textless{}{-}}\NormalTok{ umd\_courses }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(department }\SpecialCharTok{==} \StringTok{"Theatre"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(seats }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{)}

\FunctionTok{nrow}\NormalTok{(theatre\_seats\_15)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 231
\end{verbatim}

That gives us 231 records But that's repetitive, no? We can do better
using a single filter and boolean operators -- AND and OR. In this case,
AND is \texttt{\&} and OR is \texttt{\textbar{}}.

The difference? With AND, all conditions must be true to be included.
With OR, any of those conditions things can be true and it will be
included.

Here's the difference.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{and\_theatre\_seats\_15 }\OtherTok{\textless{}{-}}\NormalTok{ umd\_courses }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(department }\SpecialCharTok{==} \StringTok{"Theatre"} \SpecialCharTok{\&}\NormalTok{ seats }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{)}

\FunctionTok{nrow}\NormalTok{(and\_theatre\_seats\_15)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 231
\end{verbatim}

So AND gives us the same answer we got before. What does OR give us?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{and\_theatre\_seats\_15 }\OtherTok{\textless{}{-}}\NormalTok{ umd\_courses }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(department }\SpecialCharTok{==} \StringTok{"Theatre"} \SpecialCharTok{|}\NormalTok{ seats }\SpecialCharTok{\textgreater{}=} \DecValTok{15}\NormalTok{)}

\FunctionTok{nrow}\NormalTok{(and\_theatre\_seats\_15)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 41011
\end{verbatim}

So there's 41,011 rows that are EITHER Theatre classes OR have at least
15 seats. OR is additive; AND is restrictive.

A general tip about using filter: it's easier to work your way towards
the filter syntax you need rather than try and write it once and trust
the result. Each time you modify your filter, check the results to see
if they make sense. This adds a little time to your process but you'll
thank yourself for doing it because it helps avoid mistakes.

\bookmarksetup{startatroot}

\hypertarget{working-with-dates}{%
\chapter{Working with dates}\label{working-with-dates}}

One of the most frustrating things in data is working with dates.
Everyone has a different opinion on how to record them, and every
software package on the planet has to sort it out. Dealing with it can
be a little \ldots{} confusing. And every dataset has something new to
throw at you. So consider this an introduction.

First, there's the right way to display dates in data. Most of the rest
of the world knows how to do this, but Americans aren't taught it. The
correct way to display dates is the following format: YYYY-MM-DD, or
2022-09-15. Any date that looks different should be converted into that
format when you're using R.

Luckily, this problem is so common that the Tidyverse has an entire
library for dealing with it:
\href{https://lubridate.tidyverse.org/}{lubridate}.

We're going to do this two ways. First I'm going to show you how to use
base R to solve a tricky problem. And then we'll use a library called
\texttt{lubridate} to solve a more common and less tricky problem. And
then we'll use a new library to solve most of the common problems before
they start. If it's not already installed, just run
\texttt{install.packages(\textquotesingle{}lubridate\textquotesingle{})}

\hypertarget{making-dates-dates-again}{%
\section{Making dates dates again}\label{making-dates-dates-again}}

First, we'll import \texttt{tidyverse} like we always do and our
newly-installed lubridate.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

Let's start with a dataset of campaign expenses from Maryland political
committees:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/maryland\_expenses.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 97912 Columns: 14
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (12): expenditure_date, payee_name, address, payee_type, committee_name,...
dbl  (1): amount
lgl  (1): expense_toward

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(maryland\_expenses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 14
  expenditure_date payee_name           address payee_type amount committee_name
  <chr>            <chr>                <chr>   <chr>       <dbl> <chr>         
1 3/12/2021        <NA>                 <NA>    Reimburse     350 Salling   Joh~
2 3/29/2021        Dundalk Eagle Newsp~ PO Box~ Business/~    329 Salling   Joh~
3 4/29/2021        Dundalk Eagle Newsp~ PO Box~ Business/~    400 Salling   Joh~
4 5/18/2021        Dundalk Eagle Newsp~ PO Box~ Business/~    350 Salling   Joh~
5 6/9/2021         Dundalk Heritage Fa~ Dundal~ Business/~    200 Salling   Joh~
6 6/9/2021         Dundalk Heritage Fa~ Dundal~ Business/~    250 Salling   Joh~
# i 8 more variables: expense_category <chr>, expense_purpose <chr>,
#   expense_toward <lgl>, expense_method <chr>, vendor <chr>, fundtype <chr>,
#   comments <chr>, x14 <chr>
\end{verbatim}

Take a look at that first column, expenditure\_date. It \emph{looks}
like a date, but see the \texttt{\textless{}chr} right below the column
name? That means R thinks it's actually a character column. What we need
to do is make it into an actual date column, which lubridate is very
good at doing. It has a variety of functions that match the format of
the data you have. In this case, the current format is \texttt{m/d/y},
and the lubridate function is called \texttt{mdy} that we can use with
mutate:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses }\OtherTok{\textless{}{-}}\NormalTok{ maryland\_expenses }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{expenditure\_date=}\FunctionTok{mdy}\NormalTok{(expenditure\_date))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: There was 1 warning in `mutate()`.
i In argument: `expenditure_date = mdy(expenditure_date)`.
Caused by warning:
!  18 failed to parse.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(maryland\_expenses)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 14
  expenditure_date payee_name           address payee_type amount committee_name
  <date>           <chr>                <chr>   <chr>       <dbl> <chr>         
1 2021-03-12       <NA>                 <NA>    Reimburse     350 Salling   Joh~
2 2021-03-29       Dundalk Eagle Newsp~ PO Box~ Business/~    329 Salling   Joh~
3 2021-04-29       Dundalk Eagle Newsp~ PO Box~ Business/~    400 Salling   Joh~
4 2021-05-18       Dundalk Eagle Newsp~ PO Box~ Business/~    350 Salling   Joh~
5 2021-06-09       Dundalk Heritage Fa~ Dundal~ Business/~    200 Salling   Joh~
6 2021-06-09       Dundalk Heritage Fa~ Dundal~ Business/~    250 Salling   Joh~
# i 8 more variables: expense_category <chr>, expense_purpose <chr>,
#   expense_toward <lgl>, expense_method <chr>, vendor <chr>, fundtype <chr>,
#   comments <chr>, x14 <chr>
\end{verbatim}

Now look at the expenditure\_date column: R says it's a date column and
it looks like we want it to: YYYY-MM-DD. Accept no substitutes.

Lubridate has functions for basically any type of character date format:
mdy, ymd, even datetimes like ymd\_hms.

That's less code and less weirdness, so that's good.

But to get clean data, I've installed a library and created a new field
so I can now start to work with my dates. That seems like a lot, but
don't think your data will always be perfect and you won't have to do
these things.

Still, there's got to be a better way. And there is.

Fortunately, \texttt{readr} anticipates some date formatting and can
automatically handle many of these issues (indeed it uses lubridate
under the hood). When you are importing a CSV file, be sure to use
\texttt{read\_csv}, not \texttt{read.csv}.

But you're not done with lubridate yet. It has some interesting pieces
parts we'll use elsewhere.

For example, in spreadsheets you can extract portions of dates - a
month, day or year - with formulas. You can do the same in R with
lubridate. Let's say we wanted to add up the total amount spent in each
month in our Maryland expenses data.

We could use formatting to create a Month field but that would group all
the Aprils ever together. We could create a year and a month together,
but that would give us an invalid date object and that would create
problems later. Lubridate has something called a floor date that we can
use.

So to follow along here, we're going to use mutate to create a month
field, group by to lump them together, summarize to count them up and
arrange to order them. We're just chaining things together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{month =} \FunctionTok{floor\_date}\NormalTok{(expenditure\_date, }\StringTok{"month"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(month) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{total\_amount =} \FunctionTok{sum}\NormalTok{(amount)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_amount))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 2
   month      total_amount
   <date>            <dbl>
 1 2022-10-01    15827467.
 2 2022-09-01     6603431.
 3 2022-08-01     5892055.
 4 2022-11-01     4715694.
 5 2021-07-01     2242692.
 6 2021-09-01     2212083.
 7 2021-08-01     2086313.
 8 2021-06-01     1827400.
 9 2021-05-01     1341210 
10 2021-01-01      772923.
# i 15 more rows
\end{verbatim}

So the month of June 2022 had the most expenditures by far in this data.

\bookmarksetup{startatroot}

\hypertarget{mutating-data}{%
\chapter{Mutating data}\label{mutating-data}}

Often the data you have will prompt questions that it doesn't
immediately answer. Election results, for example, have raw vote totals
but we often don't use those to make comparisons between candidates
unless the numbers are small. We need percentages!

To do that in R, we can use \texttt{dplyr} and \texttt{mutate} to
calculate new metrics in a new field using existing fields of data.
That's the essence of \texttt{mutate} - using the data you have to
answer a new question.

So first we'll import the tidyverse so we can read in our data and begin
to work with it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

Now we'll import a dataset of county-level gubernatorial results from
Maryland's 2022 general election that is in the data folder in this
chapter's pre-lab directory. We'll use this to explore ways to create
new information from existing data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{general\_22 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{\textquotesingle{}data/md\_gov\_county.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 8
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): county
dbl (7): fips_code, cox, moore, lashar, wallace, harding, write_ins

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Let's add a column called \texttt{percent\_moore} for the percentage of
votes that went to Wes Moore, the Democratic candidate who won the
election, in each county. The code to calculate a percentage is pretty
simple. Remember, with \texttt{summarize}, we used \texttt{n()} to count
things. With \texttt{mutate}, we use very similar syntax to calculate a
new value -- a new column of data -- using other values in our dataset.

To calculate a percentage, we need both the number of votes for Moore
but also the total number of votes. We'll use mutate to create both
columns. The first will be total votes. The key here is to save the
dataframe to itself so that our changes stick.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{general\_22 }\OtherTok{\textless{}{-}}\NormalTok{ general\_22 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{total\_votes =}\NormalTok{ cox }\SpecialCharTok{+}\NormalTok{ moore }\SpecialCharTok{+}\NormalTok{ lashar }\SpecialCharTok{+}\NormalTok{ wallace }\SpecialCharTok{+}\NormalTok{ write\_ins,}
    \AttributeTok{pct\_moore =}\NormalTok{ moore}\SpecialCharTok{/}\NormalTok{total\_votes}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

But what do you see right away? Do those numbers look like we expect
them to? No.~They're a decimal expressed as a percentage. So let's fix
that by multiplying by 100. Since we're replacing the contents of our
new \texttt{pct\_moore} column, we can just update our previous code and
run it again:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{general\_22 }\OtherTok{\textless{}{-}}\NormalTok{ general\_22 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{pct\_moore =}\NormalTok{ (moore}\SpecialCharTok{/}\NormalTok{total\_votes)}\SpecialCharTok{*}\DecValTok{100}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

Now, does this ordering do anything for us? No.~Let's fix that with
arrange.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{general\_22 }\OtherTok{\textless{}{-}}\NormalTok{ general\_22 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{pct\_moore =}\NormalTok{ (moore}\SpecialCharTok{/}\NormalTok{total\_votes)}\SpecialCharTok{*}\DecValTok{100}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pct\_moore))}
\end{Highlighting}
\end{Shaded}

So now we have results ordered by \texttt{pct\_moore} with the highest
percentage first. To see the lowest percentage first, we can reverse
that \texttt{arrange} function - we don't need to recalculate the
column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{general\_22 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(pct\_moore)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 24 x 10
   fips_code county       cox moore lashar wallace harding write_ins total_votes
       <dbl> <chr>      <dbl> <dbl>  <dbl>   <dbl>   <dbl>     <dbl>       <dbl>
 1     24023 Garrett     8195  2402    194      52     138         0       10843
 2     24001 Allegany   13833  6390    279     143     267         0       20645
 3     24015 Cecil      18159  8691    472     155     311         0       27477
 4     24011 Caroline    6727  3276    176      56     140         0       10235
 5     24039 Somerset    3974  2254     67      46      81         0        6341
 6     24043 Washington 26943 15723    614     252     472         0       43532
 7     24013 Carroll    38969 25155   1515     436     561         0       66075
 8     24047 Worcester  13433  8550    273     121     161         0       22377
 9     24037 Saint Mar~ 20279 13291    661     253     346         0       34484
10     24035 Queen Ann~ 12840  8577    416     120     212         0       21953
# i 14 more rows
# i 1 more variable: pct_moore <dbl>
\end{verbatim}

Moore had his weakest performance in Garrett County, at the far western
edge of the state.

\hypertarget{another-use-of-mutate}{%
\section{Another use of mutate}\label{another-use-of-mutate}}

Mutate is also useful for standardizing data - for example, making
different spellings of, say, campaign spending recipients.

Let's load some Maryland state campaign expenditures into a
\texttt{maryland\_expenses} dataframe, and focus in particular on the
\texttt{payee\_name} column.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/maryland\_expenses.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 97912 Columns: 14
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (12): expenditure_date, payee_name, address, payee_type, committee_name,...
dbl  (1): amount
lgl  (1): expense_toward

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 97,912 x 14
   expenditure_date payee_name          address payee_type amount committee_name
   <chr>            <chr>               <chr>   <chr>       <dbl> <chr>         
 1 3/12/2021        <NA>                <NA>    Reimburse   350   Salling   Joh~
 2 3/29/2021        Dundalk Eagle News~ PO Box~ Business/~  329   Salling   Joh~
 3 4/29/2021        Dundalk Eagle News~ PO Box~ Business/~  400   Salling   Joh~
 4 5/18/2021        Dundalk Eagle News~ PO Box~ Business/~  350   Salling   Joh~
 5 6/9/2021         Dundalk Heritage F~ Dundal~ Business/~  200   Salling   Joh~
 6 6/9/2021         Dundalk Heritage F~ Dundal~ Business/~  250   Salling   Joh~
 7 6/1/2021         Neighborhood Signs  6655 a~ Business/~   77.4 Salling   Joh~
 8 4/16/2021        <NA>                <NA>    Reimburse   150   Salling   Joh~
 9 7/1/2021         MSP CUSTOM SOL      1000 P~ Business/~  238.  Salling   Joh~
10 7/2/2021         Squire's Restaurant 6723 H~ Business/~  260   Salling   Joh~
# i 97,902 more rows
# i 8 more variables: expense_category <chr>, expense_purpose <chr>,
#   expense_toward <lgl>, expense_method <chr>, vendor <chr>, fundtype <chr>,
#   comments <chr>, x14 <chr>
\end{verbatim}

You'll notice that there's a mix of styles: lower-case and upper-case
names like ``Anedot'' and ``ANEDOT'', for example. R will think those
are two different payees, and that will mean that any aggregates we
create based on payee\_name won't be accurate.

So how can we fix that? Mutate - it's not just for math! And a function
called \texttt{str\_to\_upper} that will convert a character column into
all uppercase.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{standardized\_maryland\_expenses }\OtherTok{\textless{}{-}}\NormalTok{ maryland\_expenses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{payee\_upper =} \FunctionTok{str\_to\_upper}\NormalTok{(payee\_name)}
\NormalTok{)}
\end{Highlighting}
\end{Shaded}

There are lots of potential uses for standardization - addresses, zip
codes, anything that can be misspelled or abbreviated.

\hypertarget{a-more-powerful-use}{%
\section{A more powerful use}\label{a-more-powerful-use}}

Mutate is even more useful when combined with some additional functions.
Let's keep rolling with our expenditure data. Take a look at the address
column: it contains a full address, including the state, spelled out. It
would be useful to have a separate \texttt{state} column with an
abbreviation. We can check to see if a state name is contained in that
column and then populate a new column with the value we want, using the
functions \texttt{str\_detect} and \texttt{case\_when}. We can identify
the state by the following pattern: a space, followed by the full name,
followed by another space. So, '' Maryland ``. The \texttt{case\_when}
function handles multiple variations, such as if the state is Maryland
or the state is Texas, etc. Crucially, we can tell R to populate the new
column with \texttt{NA} if it doesn't find a match.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses\_with\_state }\OtherTok{\textless{}{-}}\NormalTok{ maryland\_expenses }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}
    \AttributeTok{state =} \FunctionTok{case\_when}\NormalTok{(}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" Maryland "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"MD"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" California "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"CA"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" Washington "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"WA"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" Louisiana "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"LA"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" Florida "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"FL"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" North Carolina "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"NC"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" Massachusetts "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"MA"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" West Virginia "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"WV"}\NormalTok{,}
        \FunctionTok{str\_detect}\NormalTok{(address, }\StringTok{" Virginia "}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \StringTok{"VA"}\NormalTok{,}
        \AttributeTok{.default =} \ConstantTok{NA}
\NormalTok{      )}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

There's a lot going on here, so let's unpack it. It starts out as a
typical mutate statement, but \texttt{case\_when} introduces some new
things. Each line checks to see if the pattern is contained in the
address column, followed by \texttt{\textasciitilde{}} and then a value
for the new column for records that match that check. You can read it
like this: ``If we find ' Maryland ' in the address column, then put
`MD' in the state column'' for Maryland and then a handful of states,
and if we don't match any state we're looking for, make state
\texttt{NA}.

We can then use our new \texttt{state} column in group\_by statements to
make summarizing easier.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_expenses\_with\_state }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(state) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{total =} \FunctionTok{sum}\NormalTok{(amount)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 2
   state     total
   <chr>     <dbl>
 1 MD    77723146.
 2 WA    15552127.
 3 VA    10519646.
 4 CA     3370284.
 5 FL     1470592.
 6 MA     1264728 
 7 NC      691006.
 8 LA      255522.
 9 WV       41088.
10 <NA>        NA 
\end{verbatim}

Most expenditures seem to have occurred in Maryland, which makes sense,
although we haven't assigned a state for every transaction.

Mutate is there to make your data more useful and to make it easier for
you to ask more and better questions of it.

\bookmarksetup{startatroot}

\hypertarget{data-cleaning-part-i-data-smells}{%
\chapter{Data Cleaning Part I: Data
smells}\label{data-cleaning-part-i-data-smells}}

Any time you are given a dataset from anyone, you should immediately be
suspicious. Is this data what I think it is? Does it include what I
expect? Is there anything I need to know about it? Will it produce the
information I expect?

One of the first things you should do is give it the smell test.

Failure to give data the smell test
\href{https://source.opennews.org/en-US/learning/handling-data-about-race-and-ethnicity/}{can
lead you to miss stories and get your butt kicked on a competitive
story}.

With data smells, we're trying to find common mistakes in data.
\href{https://github.com/nikeiubel/data-smells/wiki/Ensuring-Accuracy-in-Data-Journalism}{For
more on data smells, read the GitHub wiki post that started it all}.
Some common data smells are:

\begin{itemize}
\tightlist
\item
  Missing data or missing values
\item
  Gaps in data
\item
  Wrong type of data
\item
  Outliers
\item
  Sharp curves
\item
  Conflicting information within a dataset
\item
  Conflicting information across datasets
\item
  Wrongly derived data
\item
  Internal inconsistency
\item
  External inconsistency
\item
  Wrong spatial data
\item
  Unusable data, including non-standard abbreviations, ambiguous data,
  extraneous data, inconsistent data
\end{itemize}

Not all of these data smells are detectable in code. You may have to ask
people about the data. You may have to compare it to another dataset
yourself. Does the agency that uses the data produce reports from the
data? Does your analysis match those reports? That will expose wrongly
derived data, or wrong units, or mistakes you made with inclusion or
exclusion.

But with several of these data smells, we can do them first, before we
do anything else.

We're going to examine several here as they apply to some
\href{https://opendata.maryland.gov/Budget/State-of-Maryland-Payments-Data-FY2008-to-FY2024/7syw-q4cy}{Maryland
state government payments data} and
\href{https://opendata.maryland.gov/Budget/State-of-Maryland-Grant-and-Loan-Data-FY2009-to-FY/absk-avps}{Maryland
state government grant \& loan data}.

\hypertarget{wrong-type}{%
\section{Wrong Type}\label{wrong-type}}

First, let's look at \textbf{Wrong Type Of Data}.

We can sniff that out by looking at the output of \texttt{readr}.

Let's load the tidyverse.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove scientific notation}
\FunctionTok{options}\NormalTok{(}\AttributeTok{scipen=}\DecValTok{999}\NormalTok{)}
\CommentTok{\# Load the tidyverse}
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

This time, we're going to load the data in a CSV format, which stands
for comma separated values and is essentially a fancy structured text
file. Each column in the csv is separated -- ``delimited'' -- by a comma
from the next column.

We're also going to introduce a new argument to our function that reads
in the data, read\_csv(), called ``guess\_max''. As R reads in the csv
file, it will attempt to make some calls on what ``data type'' to assign
to each field: number, character, date, and so on. The ``guess\_max''
argument says: look at the values in the whatever number of rows we
specify before deciding which data type to assign. In this case, we'll
pick 10.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load the data}
\NormalTok{payments }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/State\_of\_Maryland\_Payments\_Data\_\_FY2008\_to\_FY2024.csv"}\NormalTok{, }\AttributeTok{guess\_max=}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: One or more parsing issues, see `problems()` for details
\end{verbatim}

\begin{verbatim}
Rows: 373564 Columns: 8
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (4): Agency Name, Vendor Name, Date, Category
dbl (4): Fiscal Year, Vendor Zip, Amount, Fiscal Period

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Pay attention to the red warning that signals ``one or more parsing
issues.'' It advises us to run the problems() function to see what went
wrong. Let's do that.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{problems}\NormalTok{(payments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 350 x 5
     row   col expected actual file                                             
   <int> <int> <chr>    <chr>  <chr>                                            
 1   342     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 2   624     4 a double V5A    /Users/dpwillis/code/datajournalismbook-maryland~
 3  2643     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 4  5019     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 5  7930     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 6  7931     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 7 14866     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 8 17624     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
 9 17846     4 a double XXXXX  /Users/dpwillis/code/datajournalismbook-maryland~
10 19581     4 a double V5A    /Users/dpwillis/code/datajournalismbook-maryland~
# i 340 more rows
\end{verbatim}

It produces a table of all the parsing problems. It has 280 rows, which
means we have that some problems but not a huge number considering we
have 322,000 rows. In almost every case here, the \texttt{readr} library
has guessed that a given column was of a ``double'' data type -- a
number. It did it based on very limited information -- only 10 rows. So,
when it hit a value that looked like a date, or a character string, it
didn't know what to do. So it just didn't read in that value correctly.

The easy way to fix this is to set the guess\_max argument higher. It
will take a little longer to load, but we'll use every single row in the
data set to guess the column type -- all 322,138 of them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{payments }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/State\_of\_Maryland\_Payments\_Data\_\_FY2008\_to\_FY2024.csv"}\NormalTok{, }\AttributeTok{guess\_max=}\DecValTok{373564}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 373564 Columns: 8
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (5): Agency Name, Vendor Name, Vendor Zip, Date, Category
dbl (3): Fiscal Year, Amount, Fiscal Period

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

No parsing errors this time! You can see what the columns are using the
\texttt{glimpse} function:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(payments)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 373,564
Columns: 8
$ `Fiscal Year`   <dbl> 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, 2023, ~
$ `Agency Name`   <chr> "STATE DEPARTMENT OF EDUCATION", "COMPTROLLER OF MARYL~
$ `Vendor Name`   <chr> "KATHIE A DUBAY", "CHONKO MICHAEL", "MICHAEL T. GUILBA~
$ `Vendor Zip`    <chr> "21157", "21502", "21045", "21703", "21791", "21136", ~
$ Amount          <dbl> 25103.00, 40655.17, 36900.00, 42831.80, 33212.27, 700.~
$ `Fiscal Period` <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ~
$ Date            <chr> "01/01/2024 12:00:00 AM", "01/01/2024 12:00:00 AM", "0~
$ Category        <chr> "R00", "E00", "C80", "R00", "615", "D13", "S00", "J03"~
\end{verbatim}

Things that should be characters -- like agency name, vendor name -- are
characters (chr). Things that should be numbers (dbl) -- like amount and
fiscal year -- are numbers. We've seen before that sometimes dates
aren't defined as date datatypes by R - we can fix that using
\texttt{lubridate}.

\hypertarget{wrong-spatial-data}{%
\section{Wrong Spatial Data}\label{wrong-spatial-data}}

The second smell we can find in code is \textbf{wrong spatial data}.
Spatial data means data that refers to some geography; in this dataset
the only geographical element is the vendor's zip code. Zip codes should
be, at a minimum, five characters long (although composed of numbers,
zip codes aren't used as numbers).

We can check to see if any of the zip codes are less than five
characters by using
\href{https://stringr.tidyverse.org/reference/str_length.html}{a
function called \texttt{str\_length}} inside a filter:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{payments }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Vendor Zip}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_length}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Vendor Zip}\StringTok{\textasciigrave{}}\NormalTok{) }\SpecialCharTok{\textless{}} \DecValTok{5}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count=}\FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(count))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 549 x 2
   `Vendor Zip` count
   <chr>        <int>
 1 2241          2044
 2 8873           434
 3 2284           356
 4 7921           296
 5 4915           197
 6 8650           195
 7 7188           194
 8 7101           191
 9 1441           133
10 7102           112
# i 539 more rows
\end{verbatim}

So, yes, we definitely have some zip codes that are less than 5
characters long, which is not good, particularly because we don't have
any other geographical information (such as a state) that would tell us
whether we're missing a leading zero or some other character.

\hypertarget{gaps-in-data-missing-data}{%
\section{Gaps in data \& Missing data}\label{gaps-in-data-missing-data}}

Let's now look at \textbf{gaps in data}. These often occur when you have
a date or time element in your data, but there are other potential gaps,
too. To illustrate those, we're going to introduce some Maryland state
grant and loan data from 2009 forward. Let's load it and take a look:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_grants\_loans }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/State\_of\_Maryland\_Grant\_and\_Loan\_Data\_\_FY2009\_to\_FY2022.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 17765 Columns: 9
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (6): Grantor, Grantee, Zip Code, Description, Category, Date
dbl (3): Fiscal Year, Amount, Fiscal Period

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Each row represents a recipient of state grant or loan, along with
information about their location and the state agency that provided the
money. When we talk about gaps, often they indicate the administrative
rules. Here's an example: let's count the number of payments in each
category (Grant or Loan) by year in this dataset:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Fiscal Year}\StringTok{\textasciigrave{}}\NormalTok{, Category) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{count =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\StringTok{\textasciigrave{}}\AttributeTok{Fiscal Year}\StringTok{\textasciigrave{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'Fiscal Year'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{verbatim}
# A tibble: 33 x 3
# Groups:   Fiscal Year [14]
   `Fiscal Year` Category count
           <dbl> <chr>    <int>
 1          2009 Grant      819
 2          2010 Grant      612
 3          2010 Loan        76
 4          2010 <NA>         1
 5          2011 Grant      990
 6          2011 Loan        45
 7          2011 <NA>         1
 8          2012 Grant     1100
 9          2012 Loan        58
10          2013 Grant      986
# i 23 more rows
\end{verbatim}

We can see a couple of issues here: first, there is no loan data for FY
2009. That's mentioned in
\href{https://opendata.maryland.gov/Budget/State-of-Maryland-Grant-and-Loan-Data-FY2009-to-FY/absk-avps}{the
source page for the data}. It's good to be aware of all gaps in data,
but they don't always represent a problem. Second, and more problematic,
there are a few records where the Category is NA - that data is missing.
There also are some inconsistent values - there are 50 records in FY2013
with the category of ``L'' (probably loans) and one in FY 2017 that is
listed as ``Contract''.

\hypertarget{unusual-outliers}{%
\section{Unusual Outliers}\label{unusual-outliers}}

Any time you are going to focus on a column for analysis, you should
check for unusual values. Are there any unusually large values or
unusually small values? Are there any values that raise immediate
questions about the data? Let's look at the smallest amounts in the
grants and loan data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(Amount)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,765 x 9
   Grantor          Grantee `Zip Code` `Fiscal Year` Amount Description Category
   <chr>            <chr>   <chr>              <dbl>  <dbl> <chr>       <chr>   
 1 Department of N~ Land P~ 21285               2009   60   Catherine ~ Grant   
 2 Maryland Depart~ UNITED~ 21031               2012   96.0 FOOD SERVI~ Grant   
 3 Conservation Re~ Easter~ 21601               2018  186.  Rolling Vi~ Grant   
 4 Maryland Depart~ FAMILY~ 20877               2013  304   RTTT - EAR~ Grant   
 5 Maryland Depart~ Washin~ 21740               2017  361   Hold Harml~ Grant   
 6 Maryland Depart~ THE CH~ 21215-3211          2012  362.  FOOD SERVI~ Grant   
 7 Maryland Depart~ ARCHDI~ 21227               2012  379.  MARYLAND M~ Grant   
 8 Maryland Depart~ HUMAN ~ 21158               2013  387   CASH FOR C~ Grant   
 9 Department of N~ The Co~ 22209               2012  402.  R Creighto~ Grant   
10 Governor's Offi~ Court ~ 21204               2018  411.  Children's~ Grant   
# i 17,755 more rows
# i 2 more variables: `Fiscal Period` <dbl>, Date <chr>
\end{verbatim}

There are two grants for less than \$100, which might not be problematic
at all, but given that just two of 17,000 are for very small amounts you
might wonder if there are suggested amounts for applicants and how tiny
ones get evaluated compared to very large requests. As journalists, we
should be skeptical of information put in front of us and ask why or
what it says about the data itself.

\bookmarksetup{startatroot}

\hypertarget{data-cleaning-part-ii-janitor}{%
\chapter{Data Cleaning Part II:
Janitor}\label{data-cleaning-part-ii-janitor}}

The necessary bane of every data journalist's existence is data
cleaning.

Every developer, every data system, every agency, they all have opinions
about how data gets collected. Some decisions make sense from the
outside. Some decisions are based entirely on internal politics: who is
creating the data, how they are creating it, why they are creating it.
Is it automated? Is it manual? Are data normalized? Are there free form
fields where users can just type into or does the system restrict them
to choices?

Your journalistic questions -- what you want the data to tell you -- are
almost never part of that equation.

So cleaning data is the process of fixing issues in your data so you can
answer the questions you want to answer. Data cleaning is a critical
step that you can't skip. A standard metric is that 80 percent of the
time working with data will be spent cleaning and verifying data, and 20
percent the more exciting parts like analysis and visualization.

The tidyverse has a lot of built-in tools for data cleaning. We're also
going to make use of a new library, called \texttt{janitor} that has a
bunch of great functions for cleaning data. Let's load those now.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

Let's continue with our Maryland grants and loans data that we worked
with in the previous chapter.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_grants\_loans }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/State\_of\_Maryland\_Grant\_and\_Loan\_Data\_\_FY2009\_to\_FY2022.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 17765 Columns: 9
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (6): Grantor, Grantee, Zip Code, Description, Category, Date
dbl (3): Fiscal Year, Amount, Fiscal Period

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

There are a number of issues with this data set that might get in the
way of asking questions and receiving accurate answers. They are:

\begin{itemize}
\tightlist
\item
  The column names have spaces in them. This isn't a deal-breaker, as we
  used this dataframe previously. But it does require that you do some
  things differently when writing code, and ideally you don't want
  spaces in your column names.
\item
  Inconsistent capitalization across multiple columns. Sometimes the
  grantee is capitalized, and other times not. Portions of the grantor
  name are sometimes capitalized. This issue will ruin your ability to
  count and add things using those columns.
\item
  The zip field mixes five digit ZIP codes and nine digit ZIP codes, and
  some of the records include spaces. If we wanted to group and count
  the number of loans in a given ZIP code, this inconsistency would not
  let us do that correctly.
\item
  The category column is inconsistent and has some missing values.
\end{itemize}

Let's get cleaning. Our goal will be to build up one block of code that
does all the necessary cleaning in order to answer this question: which
zip code has gotten the most amount of money from the Maryland Tourism
Board?

\hypertarget{cleaning-headers}{%
\section{Cleaning headers}\label{cleaning-headers}}

One of the first places we can start with cleaning data is cleaning the
column names (or headers).

Every system has their own way of recording headers, and every developer
has their own thoughts of what a good idea is within it. R is most happy
when headers are lower case, without special characters.

If column headers start with a number, or have a space in between two
words, you have to set them off with backticks when using them in a
function. Generally speaking, we want one word (or words separated by an
underscore), all lowercase, that don't start with numbers.

The \texttt{janitor} library makes fixing headers trivially simple with
the function \texttt{clean\_names()}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{()}

\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,765 x 9
   grantor              grantee zip_code fiscal_year amount description category
   <chr>                <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>   
 1 Commerce/Maryland T~ PRINCE~ 20772           2017 1.28e5 Maryland T~ Grant   
 2 Department of Healt~ Associ~ 21201           2010 9.34e4 Minority O~ Grant   
 3 Maryland Department~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant   
 4 Department of Healt~ Maryla~ 21075           2010 2.14e5 Babies Bor~ Grant   
 5 Department of Natur~ Anacos~ 20710           2017 1.74e5 Payments m~ Grant   
 6 Department of Busin~ Washin~ 21740           2009 5.59e4 grant fund~ Grant   
 7 Boards and Commissi~ Domest~ 21045           2014 1.72e5 Domestic V~ Grant   
 8 MD Small Business D~ Dacore~ 20601           2018 1.04e5 Maryland S~ Loan    
 9 Maryland Higher Edu~ Mount ~ 21727           2015 1.75e6 Sellinger ~ Grant   
10 Department of Busin~ Olney ~ 20830           2010 2.16e5 Grant fund~ Grant   
# i 17,755 more rows
# i 2 more variables: fiscal_period <dbl>, date <chr>
\end{verbatim}

This function changed \texttt{Zip\ Code} to \texttt{zip\_code} and
generally got rid of capital letters and replaced spaces with
underscores. If we wanted to rename a column, we can use a tidyverse
function \texttt{rename()} to do that. Let's change \texttt{grantor} to
\texttt{source} as an example. NOTE: when using \texttt{rename()}, the
\emph{new} name comes first.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{source =}\NormalTok{ grantor)}

\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,765 x 9
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 Comme~ PRINCE~ 20772           2017 1.28e5 Maryland T~ Grant                1
 2 Depar~ Associ~ 21201           2010 9.34e4 Minority O~ Grant                1
 3 Maryl~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant                1
 4 Depar~ Maryla~ 21075           2010 2.14e5 Babies Bor~ Grant                1
 5 Depar~ Anacos~ 20710           2017 1.74e5 Payments m~ Grant                1
 6 Depar~ Washin~ 21740           2009 5.59e4 grant fund~ Grant                1
 7 Board~ Domest~ 21045           2014 1.72e5 Domestic V~ Grant                1
 8 MD Sm~ Dacore~ 20601           2018 1.04e5 Maryland S~ Loan                 1
 9 Maryl~ Mount ~ 21727           2015 1.75e6 Sellinger ~ Grant                1
10 Depar~ Olney ~ 20830           2010 2.16e5 Grant fund~ Grant                1
# i 17,755 more rows
# i 1 more variable: date <chr>
\end{verbatim}

\hypertarget{changing-capitalization}{%
\section{Changing capitalization}\label{changing-capitalization}}

Right now the \texttt{source}, \texttt{grantee} and \texttt{description}
columns have inconsistent capitalization. We can fix that using a mutate
statement and a function that changes the case of text called
\texttt{str\_to\_upper()}. We'll use the same columns, overwriting
what's in there since all we're doing is changing case.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{source =}\NormalTok{ grantor) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{source =} \FunctionTok{str\_to\_upper}\NormalTok{(source), }\AttributeTok{grantee =} \FunctionTok{str\_to\_upper}\NormalTok{(grantee), }\AttributeTok{description =} \FunctionTok{str\_to\_upper}\NormalTok{(description))}

\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,765 x 9
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 COMME~ PRINCE~ 20772           2017 1.28e5 MARYLAND T~ Grant                1
 2 DEPAR~ ASSOCI~ 21201           2010 9.34e4 MINORITY O~ Grant                1
 3 MARYL~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant                1
 4 DEPAR~ MARYLA~ 21075           2010 2.14e5 BABIES BOR~ Grant                1
 5 DEPAR~ ANACOS~ 20710           2017 1.74e5 PAYMENTS M~ Grant                1
 6 DEPAR~ WASHIN~ 21740           2009 5.59e4 GRANT FUND~ Grant                1
 7 BOARD~ DOMEST~ 21045           2014 1.72e5 DOMESTIC V~ Grant                1
 8 MD SM~ DACORE~ 20601           2018 1.04e5 MARYLAND S~ Loan                 1
 9 MARYL~ MOUNT ~ 21727           2015 1.75e6 SELLINGER ~ Grant                1
10 DEPAR~ OLNEY ~ 20830           2010 2.16e5 GRANT FUND~ Grant                1
# i 17,755 more rows
# i 1 more variable: date <chr>
\end{verbatim}

What this does is make it so that using \texttt{group\_by} will result
in fewer rows due to inconsistent capitalization. It won't fix
misspellings, but working off a single case style definitely helps.

\hypertarget{duplicates}{%
\section{Duplicates}\label{duplicates}}

One of the most difficult problems to fix in data is duplicate records
in the data. They can creep in with bad joins, bad data entry practices,
mistakes -- all kinds of reasons. A duplicated record isn't always there
because of an error, but you need to know if it's there before making
that determination.

So the question is, do we have any records repeated?

Here we'll use a function called \texttt{get\_dupes} from the janitor
library to check for fully repeated records in our cleaned data set.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{get\_dupes}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
No variable names specified - using all columns.
\end{verbatim}

\begin{verbatim}
# A tibble: 50 x 10
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 BOARD~ CASA C~ 21740           2009 6.62e4 SERVICES T~ Grant                1
 2 BOARD~ CASA C~ 21740           2009 6.62e4 SERVICES T~ Grant                1
 3 BOARD~ FAMILY~ 21218           2009 2.18e5 PRE-ADJUDI~ Grant                1
 4 BOARD~ FAMILY~ 21218           2009 2.18e5 PRE-ADJUDI~ Grant                1
 5 BOARD~ HEARTL~ 21705           2009 5.32e4 UNDERSERVE~ Grant                1
 6 BOARD~ HEARTL~ 21705           2009 5.32e4 UNDERSERVE~ Grant                1
 7 BOARD~ MARYLA~ 21012           2009 9.72e4 CAPACITY B~ Grant                1
 8 BOARD~ MARYLA~ 21012           2009 9.72e4 CAPACITY B~ Grant                1
 9 BOARD~ MARYLA~ 21012           2009 9.72e4 SEXUAL ASS~ Grant                1
10 BOARD~ MARYLA~ 21012           2009 9.72e4 SEXUAL ASS~ Grant                1
# i 40 more rows
# i 2 more variables: date <chr>, dupe_count <int>
\end{verbatim}

And the answer is \ldots{} maybe? Because the original dataset doesn't
have a unique identifier for each grant, it's \emph{possible} that we
have duplicates here, as many as 50. If we could confirm that these
actually are duplicates, we can fix this by adding the function
\texttt{distinct()} to our cleaning script. This will keep only one copy
of each unique record in our table. But we'd need to confirm that first.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{source =}\NormalTok{ grantor) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{source =} \FunctionTok{str\_to\_upper}\NormalTok{(source), }\AttributeTok{grantee =} \FunctionTok{str\_to\_upper}\NormalTok{(grantee), }\AttributeTok{description =} \FunctionTok{str\_to\_upper}\NormalTok{(description)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{()}

\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,740 x 9
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 COMME~ PRINCE~ 20772           2017 1.28e5 MARYLAND T~ Grant                1
 2 DEPAR~ ASSOCI~ 21201           2010 9.34e4 MINORITY O~ Grant                1
 3 MARYL~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant                1
 4 DEPAR~ MARYLA~ 21075           2010 2.14e5 BABIES BOR~ Grant                1
 5 DEPAR~ ANACOS~ 20710           2017 1.74e5 PAYMENTS M~ Grant                1
 6 DEPAR~ WASHIN~ 21740           2009 5.59e4 GRANT FUND~ Grant                1
 7 BOARD~ DOMEST~ 21045           2014 1.72e5 DOMESTIC V~ Grant                1
 8 MD SM~ DACORE~ 20601           2018 1.04e5 MARYLAND S~ Loan                 1
 9 MARYL~ MOUNT ~ 21727           2015 1.75e6 SELLINGER ~ Grant                1
10 DEPAR~ OLNEY ~ 20830           2010 2.16e5 GRANT FUND~ Grant                1
# i 17,730 more rows
# i 1 more variable: date <chr>
\end{verbatim}

\hypertarget{cleaning-strings}{%
\section{Cleaning strings}\label{cleaning-strings}}

The rest of the problems with this data set all have to do with
inconsistent format of values in a few of the columns. To fix these
problems, we're going to make use of mutate() in concert with ``string
functions'' -- special functions that allow us to clean up columns
stored as character strings. The tidyverse package \texttt{stringr} has
lots of useful string functions, more than we'll learn in this chapter.

Let's start by cleaning up the zip field. Remember, some of the rows had
a five-digit ZIP code, while others had a nine-digit ZIP code, separated
by a hyphen or not.

We're going to write code that tells R to make a new column for our
zips, keeping the first five digits on the left, and get rid of anything
after that by using \texttt{mutate()} in concert with
\texttt{str\_sub()}, from the \texttt{stringr} package.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{source =}\NormalTok{ grantor) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{source =} \FunctionTok{str\_to\_upper}\NormalTok{(source), }\AttributeTok{grantee =} \FunctionTok{str\_to\_upper}\NormalTok{(grantee), }\AttributeTok{description =} \FunctionTok{str\_to\_upper}\NormalTok{(description)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zip5 =} \FunctionTok{str\_sub}\NormalTok{(zip\_code, }\AttributeTok{start=}\NormalTok{1L, }\AttributeTok{end=}\NormalTok{5L))}


\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,740 x 10
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 COMME~ PRINCE~ 20772           2017 1.28e5 MARYLAND T~ Grant                1
 2 DEPAR~ ASSOCI~ 21201           2010 9.34e4 MINORITY O~ Grant                1
 3 MARYL~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant                1
 4 DEPAR~ MARYLA~ 21075           2010 2.14e5 BABIES BOR~ Grant                1
 5 DEPAR~ ANACOS~ 20710           2017 1.74e5 PAYMENTS M~ Grant                1
 6 DEPAR~ WASHIN~ 21740           2009 5.59e4 GRANT FUND~ Grant                1
 7 BOARD~ DOMEST~ 21045           2014 1.72e5 DOMESTIC V~ Grant                1
 8 MD SM~ DACORE~ 20601           2018 1.04e5 MARYLAND S~ Loan                 1
 9 MARYL~ MOUNT ~ 21727           2015 1.75e6 SELLINGER ~ Grant                1
10 DEPAR~ OLNEY ~ 20830           2010 2.16e5 GRANT FUND~ Grant                1
# i 17,730 more rows
# i 2 more variables: date <chr>, zip5 <chr>
\end{verbatim}

Let's break down that last line of code. It says: take the value in each
zip column and extract the first character on the left (1L) through the
fifth character on the left (5L), and then use that five-digit zip to
populate a new zip5 column.

If we arrange the zip5 column we can see that there are some non-digits
in there, so let's make those NA. For that, we're going to use
\texttt{case\_when()}, a function that let's us say if a value meets a
certain condition, then change it, and if it doesn't, don't change it.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{source =}\NormalTok{ grantor) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{source =} \FunctionTok{str\_to\_upper}\NormalTok{(source), }\AttributeTok{grantee =} \FunctionTok{str\_to\_upper}\NormalTok{(grantee), }\AttributeTok{description =} \FunctionTok{str\_to\_upper}\NormalTok{(description)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zip5 =} \FunctionTok{str\_sub}\NormalTok{(zip\_code, }\AttributeTok{start=}\NormalTok{1L, }\AttributeTok{end=}\NormalTok{5L)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zip5 =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    zip5 }\SpecialCharTok{==} \StringTok{"Vario"} \SpecialCharTok{\textasciitilde{}} \ConstantTok{NA}\NormalTok{,}
\NormalTok{    zip5 }\SpecialCharTok{==} \StringTok{"UB7 O"} \SpecialCharTok{\textasciitilde{}} \ConstantTok{NA}\NormalTok{,}
\NormalTok{    zip5 }\SpecialCharTok{==} \StringTok{"UB7 "} \SpecialCharTok{\textasciitilde{}} \ConstantTok{NA}\NormalTok{,}
    \AttributeTok{.default =}\NormalTok{ zip5}
\NormalTok{  ))}

\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,740 x 10
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 COMME~ PRINCE~ 20772           2017 1.28e5 MARYLAND T~ Grant                1
 2 DEPAR~ ASSOCI~ 21201           2010 9.34e4 MINORITY O~ Grant                1
 3 MARYL~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant                1
 4 DEPAR~ MARYLA~ 21075           2010 2.14e5 BABIES BOR~ Grant                1
 5 DEPAR~ ANACOS~ 20710           2017 1.74e5 PAYMENTS M~ Grant                1
 6 DEPAR~ WASHIN~ 21740           2009 5.59e4 GRANT FUND~ Grant                1
 7 BOARD~ DOMEST~ 21045           2014 1.72e5 DOMESTIC V~ Grant                1
 8 MD SM~ DACORE~ 20601           2018 1.04e5 MARYLAND S~ Loan                 1
 9 MARYL~ MOUNT ~ 21727           2015 1.75e6 SELLINGER ~ Grant                1
10 DEPAR~ OLNEY ~ 20830           2010 2.16e5 GRANT FUND~ Grant                1
# i 17,730 more rows
# i 2 more variables: date <chr>, zip5 <chr>
\end{verbatim}

That last bit is a little complex, so let's break it down.

What the code above says, in English, is this: Look at all the values in
the zip5 column. If the value is ``Vario'', then (that's what the
``\textasciitilde{}'' means, then) replace it with NA. Same for the
other variations. If it's anything other than that (that's what ``TRUE''
means, otherwise), then keep the existing value in that column.

Instead of specifying the exact value, we can also solve the problem by
using something more generalizable, using a function called
str\_detect(), which allows us to search parts of words.

The second line of our case\_when() function below now says, in English:
look in the city column. If you find that one of the values starts with
``UB7'' (the ``\^{}'' symbol means ``starts with''), then (the tilde
\textasciitilde{} means then) change it to NA.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# cleaning function}
\NormalTok{cleaned\_md\_grants\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{source =}\NormalTok{ grantor) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{source =} \FunctionTok{str\_to\_upper}\NormalTok{(source), }\AttributeTok{grantee =} \FunctionTok{str\_to\_upper}\NormalTok{(grantee), }\AttributeTok{description =} \FunctionTok{str\_to\_upper}\NormalTok{(description)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{distinct}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zip5 =} \FunctionTok{str\_sub}\NormalTok{(zip\_code, }\AttributeTok{start=}\NormalTok{1L, }\AttributeTok{end=}\NormalTok{5L)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zip5 =} \FunctionTok{case\_when}\NormalTok{(}
\NormalTok{    zip5 }\SpecialCharTok{==} \StringTok{"Vario"} \SpecialCharTok{\textasciitilde{}} \ConstantTok{NA}\NormalTok{,}
    \FunctionTok{str\_detect}\NormalTok{(zip5, }\StringTok{"\^{}UB7"}\NormalTok{) }\SpecialCharTok{\textasciitilde{}} \ConstantTok{NA}\NormalTok{,}
    \AttributeTok{.default =}\NormalTok{ zip5}
\NormalTok{  ))}

\CommentTok{\# display the cleaned dataset}
\NormalTok{cleaned\_md\_grants\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,740 x 10
   source grantee zip_code fiscal_year amount description category fiscal_period
   <chr>  <chr>   <chr>          <dbl>  <dbl> <chr>       <chr>            <dbl>
 1 COMME~ PRINCE~ 20772           2017 1.28e5 MARYLAND T~ Grant                1
 2 DEPAR~ ASSOCI~ 21201           2010 9.34e4 MINORITY O~ Grant                1
 3 MARYL~ WESTED~ 94107-1~        2014 1.61e6 GRANTS FOR~ Grant                1
 4 DEPAR~ MARYLA~ 21075           2010 2.14e5 BABIES BOR~ Grant                1
 5 DEPAR~ ANACOS~ 20710           2017 1.74e5 PAYMENTS M~ Grant                1
 6 DEPAR~ WASHIN~ 21740           2009 5.59e4 GRANT FUND~ Grant                1
 7 BOARD~ DOMEST~ 21045           2014 1.72e5 DOMESTIC V~ Grant                1
 8 MD SM~ DACORE~ 20601           2018 1.04e5 MARYLAND S~ Loan                 1
 9 MARYL~ MOUNT ~ 21727           2015 1.75e6 SELLINGER ~ Grant                1
10 DEPAR~ OLNEY ~ 20830           2010 2.16e5 GRANT FUND~ Grant                1
# i 17,730 more rows
# i 2 more variables: date <chr>, zip5 <chr>
\end{verbatim}

We've gotten the source and zip code data as clean as we can, and now we
can answer our question: which zip code has gotten the most amount of
money from the Maryland Tourism Board? A good rule of thumb is that you
should only spend time cleaning fields that are critical to the specific
analysis you want to do.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_md\_grants\_loans }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(source }\SpecialCharTok{==} \StringTok{\textquotesingle{}COMMERCE/MARYLAND TOURISM BOARD\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(zip5) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{total\_amount =} \FunctionTok{sum}\NormalTok{(amount)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_amount))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 8 x 2
  zip5  total_amount
  <chr>        <dbl>
1 21202       648069
2 21601       400000
3 20005       250000
4 21401       190570
5 20772       128041
6 21701       116708
7 21740        63070
8 21014        52490
\end{verbatim}

Why, it's
\href{https://censusreporter.org/profiles/86000US21202-21202/}{downtown
Baltimore, including the Inner Harbor area}.

\bookmarksetup{startatroot}

\hypertarget{data-cleaning-part-iii-open-refine}{%
\chapter{Data Cleaning Part III: Open
Refine}\label{data-cleaning-part-iii-open-refine}}

Gather 'round kids and let me tell you a tale about one of your authors.
In college, this author (Matt Waite) got involved in a project where he
mapped crime in Lincoln, Nebraska, looking specifically in the
neighborhoods surrounding campus. This was in the mid 1990s. Computers
were under powered. Tools were pretty primitive. I was given a database
of nearly 50,000 calls for service.

And then I learned that addresses were not stored in a standard way.
However the officer wrote it down, that's how it was recorded.

What did that mean?

It meant the Lincoln Police Department came up with dozens of ways to
say a single place. And since the mapping software needed the addressed
to be in a specific form, I had to fix them. For example, I will go to
my grave knowing that Lincoln High School's street address is 2229 J
Street. Police officers wrote down LHS, L.H.S., Lincoln HS, Lincoln
H.S., LHS (J Street), 2229 J, 2229 J ST, St., Street and on and on and
on. That one was relatively easy. The local convenience store chain,
with 8 locations around the city, was harder. I had to use the patrol
district to locate them.

It took me four months to clean up more than 30,000 unique addresses and
map them.

I tell you this because if I had Open Refine, it would have taken me a
week, not four months.

Every time I talk about Open Refine, I remember this, and I get mad.

Fortunately (unfortunately?) several columns in the PPP loan data we're
working with are flawed in exactly the same way. There are dozens of
variations on just ``Baltimore''.

We're going to explore two ways into Open Refine: Through R, and through
Open Refine itself.

\hypertarget{refinr-open-refine-in-r}{%
\section{Refinr, Open Refine in R}\label{refinr-open-refine-in-r}}

What is Open Refine?

Open Refine is a software program that has tools -- algorithms -- that
find small differences in text and helps you fix them quickly. How Open
Refine finds those small differences is through something called
clustering. The algorithms behind clustering are not exclusive to Open
Refine, so they can be used elsewhere.

Enter \texttt{refinr}, a package that contains the same clustering
algorithms as Open Refine but all within R. Go ahead and install it if
you haven't already by opening the console and running
\texttt{install.packages("refinr")}. Then we can load libraries as we
do.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(refinr)}
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

Let's load that Maryland state government grants and loan data that
we've been working with, and to make our standardization work easier
we'll change all the grantees to upper-case.

Now let's try and group and count the number of grants by recipient. To
make it a bit more manageable, let's use another string function from
\texttt{stringr} and filter for recipients that start with the uppercase
``W'' or lowercase ``w'' using the function \texttt{str\_detect()} with
a regular expression.

The filter function in the codeblock below says: look in the city
column, and pluck out any value that starts with (the ``\^{}'' symbol
means ``starts with'') a lowercase ``w'' OR (the vertical
``\textbar{}'', called a pipe, means OR) an uppercase ``W''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_grant\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Grantee) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count=}\FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Grantee, }\StringTok{\textquotesingle{}\^{}w|\^{}W\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Grantee)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 323 x 2
   Grantee                          count
   <chr>                            <int>
 1 W. F. DELAUTER & SON, INC.           1
 2 W.R. GRACE & CO.                     1
 3 WABASH MARKETS OF MARYLAND, INC.     1
 4 WAGABOUT                             1
 5 WAH,LLC                              1
 6 WALBROOK MILL APARTMENTS             1
 7 WALDEN SIERRA                        1
 8 WALDEN SIERRA INC                    1
 9 WALDEN SIERRA, INC.                 31
10 WALDORF ARETE, LLC                   1
# i 313 more rows
\end{verbatim}

There are several problems in this data that will prevent proper
grouping and summarizing - you can see multiple versions of ``Walden
Sierra'', for example. We've learned several functions to fix this
manually, but that could take awhile.

By using the Open Refine package for R, \texttt{refinr}, our hope is
that it can identify and standardize the data with a little more ease.

The first merging technique that's part of the \texttt{refinr} package
we'll try is the \texttt{key\_collision\_merge}.

The key collision merge function takes each string and extracts the key
parts of it. It then puts every key in a bin based on the keys matching.

One rule you should follow when using this is: \textbf{do not overwrite
your original fields}. Always work on a copy. If you overwrite your
original field, how will you know if it did the right thing? How can you
compare it to your original data? To follow this, I'm going to mutate a
new field called clean\_city and put the results of key collision merge
there.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_md\_grant\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grant\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{grantee\_clean=}\FunctionTok{key\_collision\_merge}\NormalTok{(Grantee)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Grantee, grantee\_clean, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{cleaned\_md\_grant\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,765 x 10
   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description
   <chr>       <chr>         <chr>   <chr>              <dbl>  <dbl> <chr>      
 1 PRINCE GEO~ PRINCE GEORG~ Commer~ 20772               2017 1.28e5 Maryland T~
 2 ASSOCIATED~ ASSOCIATED B~ Depart~ 21201               2010 9.34e4 Minority O~
 3 WESTED/PUB~ WESTED/PUBLI~ Maryla~ 94107-1242          2014 1.61e6 GRANTS FOR~
 4 MARYLAND P~ MARYLAND PAT~ Depart~ 21075               2010 2.14e5 Babies Bor~
 5 ANACOSTIA ~ ANACOSTIA WA~ Depart~ 20710               2017 1.74e5 Payments m~
 6 WASHINGTON~ WASHINGTON C~ Depart~ 21740               2009 5.59e4 grant fund~
 7 DOMESTIC V~ DOMESTIC VIO~ Boards~ 21045               2014 1.72e5 Domestic V~
 8 DACORE INV~ DACORE INVES~ MD Sma~ 20601               2018 1.04e5 Maryland S~
 9 MOUNT ST M~ MOUNT ST MAR~ Maryla~ 21727               2015 1.75e6 Sellinger ~
10 OLNEY THEA~ OLNEY THEATR~ Depart~ 20830               2010 2.16e5 Grant fund~
# i 17,755 more rows
# i 3 more variables: Category <chr>, `Fiscal Period` <dbl>, Date <chr>
\end{verbatim}

To examine changes \texttt{refinr} made, let's examine the changes it
made to cities that start with the letter ``W''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_md\_grant\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Grantee, grantee\_clean) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count=}\FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Grantee, }\StringTok{\textquotesingle{}\^{}w|\^{}W\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Grantee)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'Grantee'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{verbatim}
# A tibble: 323 x 3
# Groups:   Grantee [323]
   Grantee                          grantee_clean                    count
   <chr>                            <chr>                            <int>
 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1
 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1
 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1
 4 WAGABOUT                         WAGABOUT                             1
 5 WAH,LLC                          WAH,LLC                              1
 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1
 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1
 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1
 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31
10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1
# i 313 more rows
\end{verbatim}

You can see several changes on the first page of results, including that
refinr consolidated all the Walden Sierra entries into a single one in
\texttt{grantee\_clean}, which is pretty smart. Other potential changes,
grouping together ``WALTER'S ART MUSEUM'' and ``THE WALTERS ART
MUSEUM'', didn't happen. Key collision will do well with different
cases, but all of our records are upper case.

There's another merging algorithim that's part of refinr that works a
bit differently, called \texttt{n\_gram\_merge()}. Let's try applying
that one.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_md\_grant\_loans }\OtherTok{\textless{}{-}}\NormalTok{ md\_grant\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{grantee\_clean=}\FunctionTok{n\_gram\_merge}\NormalTok{(Grantee)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(Grantee, grantee\_clean, }\FunctionTok{everything}\NormalTok{())}

\NormalTok{cleaned\_md\_grant\_loans}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 17,765 x 10
   Grantee     grantee_clean Grantor `Zip Code` `Fiscal Year` Amount Description
   <chr>       <chr>         <chr>   <chr>              <dbl>  <dbl> <chr>      
 1 PRINCE GEO~ PRINCE GEORG~ Commer~ 20772               2017 1.28e5 Maryland T~
 2 ASSOCIATED~ ASSOCIATED B~ Depart~ 21201               2010 9.34e4 Minority O~
 3 WESTED/PUB~ WESTED/PUBLI~ Maryla~ 94107-1242          2014 1.61e6 GRANTS FOR~
 4 MARYLAND P~ MARYLAND PAT~ Depart~ 21075               2010 2.14e5 Babies Bor~
 5 ANACOSTIA ~ ANACOSTIA WA~ Depart~ 20710               2017 1.74e5 Payments m~
 6 WASHINGTON~ WASHINGTON C~ Depart~ 21740               2009 5.59e4 grant fund~
 7 DOMESTIC V~ DOMESTIC VIO~ Boards~ 21045               2014 1.72e5 Domestic V~
 8 DACORE INV~ DACORE INVES~ MD Sma~ 20601               2018 1.04e5 Maryland S~
 9 MOUNT ST M~ MOUNT ST MAR~ Maryla~ 21727               2015 1.75e6 Sellinger ~
10 OLNEY THEA~ OLNEY THEATR~ Depart~ 20830               2010 2.16e5 Grant fund~
# i 17,755 more rows
# i 3 more variables: Category <chr>, `Fiscal Period` <dbl>, Date <chr>
\end{verbatim}

To examine changes \texttt{refinr} made with this algorithm, let's again
look at recipients that start with the letter ``W''. We see there wasn't
a substantial change from the previous method, and it even missed a few
the first method got.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{cleaned\_md\_grant\_loans }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(Grantee, grantee\_clean) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{count=}\FunctionTok{n}\NormalTok{()}
\NormalTok{  ) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\FunctionTok{str\_detect}\NormalTok{(Grantee, }\StringTok{\textquotesingle{}\^{}w|\^{}W\textquotesingle{}}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(Grantee)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`summarise()` has grouped output by 'Grantee'. You can override using the
`.groups` argument.
\end{verbatim}

\begin{verbatim}
# A tibble: 323 x 3
# Groups:   Grantee [323]
   Grantee                          grantee_clean                    count
   <chr>                            <chr>                            <int>
 1 W. F. DELAUTER & SON, INC.       W. F. DELAUTER & SON, INC.           1
 2 W.R. GRACE & CO.                 W.R. GRACE & CO.                     1
 3 WABASH MARKETS OF MARYLAND, INC. WABASH MARKETS OF MARYLAND, INC.     1
 4 WAGABOUT                         WAGABOUT                             1
 5 WAH,LLC                          WAH,LLC                              1
 6 WALBROOK MILL APARTMENTS         WALBROOK MILL APARTMENTS             1
 7 WALDEN SIERRA                    WALDEN SIERRA, INC.                  1
 8 WALDEN SIERRA INC                WALDEN SIERRA, INC.                  1
 9 WALDEN SIERRA, INC.              WALDEN SIERRA, INC.                 31
10 WALDORF ARETE, LLC               WALDORF ARETE, LLC                   1
# i 313 more rows
\end{verbatim}

This method also made some good changes, but not in every case. No
single method will be perfect and often a combination is necessary.

That's how you use the Open Refine r package, refinr.

Now let's upload the data to the interactive version of OpenRefine,
which really shines at this task.

\hypertarget{manually-cleaning-data-with-open-refine}{%
\section{Manually cleaning data with Open
Refine}\label{manually-cleaning-data-with-open-refine}}

Open Refine is free software. \href{https://openrefine.org/}{You should
download and install it}; the most recent version is 3.6.0. Refinr is
great for quick things on smaller datasets that you can check to make
sure it's not up to any mischief.

For bigger datasets, Open Refine is the way to go. And it has a lot more
tools than refinr does (by design).

After you install it, run it. (If you are on a Mac it might tell you
that it can't run the program. Go to System Preferences -\textgreater{}
Security \& Privacy -\textgreater{} General and click ``Open Anyway''.)
Open Refine works in the browser, and the app spins up a small web
server visible only on your computer to interact with it. A browser will
pop up automatically.

You first have to import your data into a project. Click the choose
files button and upload a csv of the Maryland state grants and loans.

\includegraphics[width=2.91in,height=\textheight]{./images/open1.png}

After your data is loaded into the app, you'll get a screen to look over
what the data looks like. On the top right corner, you'll see a button
to create the project. Click that.

\includegraphics[width=3.77in,height=\textheight]{./images/open2.png}

Open Refine has many, many tools. We're going to use one piece of it, as
a tool for data cleaning. To learn how to use it, we're going to clean
the ``Grantee'' field.

First, let's make a copy of the original Grantee column so that we can
preserve the original data while cleaning the new one.

Click the dropdown arrow next to the Grantee column, choose ``edit
column'' \textgreater{} ``Add column based on this column'':

\includegraphics[width=2.51in,height=\textheight]{./images/open2a.png}

On the window that pops up, type ``grantee\_clean'' in the ``new column
name'' field. Then hit the OK button. We'll work on that new column.

\includegraphics[width=2.37in,height=\textheight]{./images/open2b.png}

Now, let's get to work cleaning the grantee\_clean column.

Next to the grantee\_clean field name, click the down arrow, then facet,
then text facet.

\includegraphics[width=1.65in,height=\textheight]{./images/open3.png}

After that, a new box will appear on the left. It tells us how many
unique recipient\_names there are: 8,956 (you may need to . And, there's
a button on the right of the box that says Cluster.

\includegraphics[width=0.99in,height=\textheight]{./images/open4.png}

Click the cluster button. A new window will pop up, a tool to help us
identify things that need to be cleaned, and quickly clean them.

\includegraphics[width=2.99in,height=\textheight]{./images/open5.png}

The default ``method'' used is a clustering algorithim called ``key
collision'', using the fingerprint function. This is the same method we
used with the refinr package above.

At the top, you'll see which method was used, and how many clusters that
algorithm identified. There are several different methods, each of which
work slightly differently and produce different results.

\includegraphics[width=2.99in,height=\textheight]{./images/open6.png}

Then, below that, you can see what those clusters are. Right away, we
can see how useful this program is. It identified 23 rows that have some
variation on ``University of Maryland - Baltimore'' in the
grantee\_clean field. It proposed changing them all to ``UNIVERSITY OF
MARYLAND BALTIMORE''.

Using human judgement, you can say if you agree with the cluster. If you
do, click the ``merge'' checkbox. When it merges, the new result will be
what it says in New Cell Value. Most often, that's the row with the most
common result. You also can manually edit the ``New Cell Value'' if you
want it to be something else:

Now begins the fun part: You have to look at all the clusters found and
decide if they are indeed valid. The key collision method is very good,
and very conservative. You'll find that most of them are usually valid.

Be careful! If you merge two things that aren't supposed to be together,
it will change your data in a way that could lead to inaccurate results.

When you're done, click Merge Selected and Re-Cluster.

If any new clusters come up, evaluate them. Repeat until either no
clusters come up or the clusters that do come up are ones you reject.

Now. Try a new method, maybe the ``nearest neighbor levenshtein''
method. Notice that it finds even more clusters - using a slightly
different approach.

Rinse and repeat.

You'll keep doing this, and if the dataset is reasonably clean, you'll
find the end.

When you're finished cleaning, click ``Merge Selected \& Close''.

Then, export the data as a csv so you can load it back into R.

\includegraphics[width=3.86in,height=\textheight]{./images/open7.png}

A question for all data analysts -- if the dataset is bad enough, can it
ever be cleaned?

There's no good answer. You have to find it yourself.

\bookmarksetup{startatroot}

\hypertarget{cleaning-data-part-iv-pdfs}{%
\chapter{Cleaning Data Part IV: PDFs}\label{cleaning-data-part-iv-pdfs}}

The next circle of Hell on the Dante's Inferno of Data Journalism is the
PDF. Governments everywhere love the PDF and publish all kinds of
records in a PDF. The problem is a PDF isn't a data format -- it's a
middle finger, saying I've Got Your Accountability Right Here, Pal.

It's so ridiculous that there's a constellation of tools that do nothing
more than try to harvest tables out of PDFs. There are online services
like \href{https://www.cometdocs.com/}{CometDocs} where you can upload
your PDF and point and click your way into an Excel file. There are
mobile device apps that take a picture of a table and convert it into a
spreadsheet. But one of the best is a tool called
\href{https://tabula.technology/}{Tabula}. It was build by journalists
for journalists.

There is a version of Tabula that will run inside of R -- a library
called Tabulizer -- but the truth is I'm having the hardest time
installing it on my machine, which leads me to believe that trying to
install it across a classroom of various machines would be disastrous.
The standalone version works just fine, and it provides a useful way for
you to see what's actually going on.

Unfortunately, harvesting tables from PDFs with Tabula is an exercise in
getting your hopes up, only to have them dashed. We'll start with an
example. First, let's load the tidyverse and janitor.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

\hypertarget{easy-does-it}{%
\section{Easy does it}\label{easy-does-it}}

Tabula works best when tables in PDFs are clearly defined and have
nicely-formatted information. Here's a perfect example:
\href{https://www.elections.maryland.gov/press_room/2022_stats/GG22/Eligible\%20Active\%20Voters\%20by\%20County\%20-\%20GG22.pdf}{active
voters by county in Maryland}.

\href{https://tabula.technology/}{Download and install Tabula}. Tabula
works much the same way as Open Refine does -- it works in the browser
by spinning up a small webserver in your computer.

When Tabula opens, you click browse to find the PDF on your computer
somewhere, and then click import. After it imports, click autodetect
tables. You'll see red boxes appear around what Tabula believes are the
tables. You'll see it does a pretty good job at this.

\includegraphics[width=1.89in,height=\textheight]{./images/md_voters.png}

Now you can hit the green ``Preview \& Export Extracted Data'' button on
the top right. You should see something very like this:

\includegraphics[width=5.4in,height=\textheight]{./images/md_voters2.png}

You can now export that extracted table to a CSV file using the
``Export'' button. And then we can read it into R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{voters\_by\_county }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/tabula{-}Eligible Active Voters by County {-} GG22.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 25 Columns: 10
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): County
dbl (1): BAR

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{voters\_by\_county}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 10
   County              DEM    REP   BAR   GRN   LIB   WCP   OTH    UNA  TOTAL
   <chr>             <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>
 1 Allegany          11793  22732     0    76   223    73   382   8337  43616
 2 Anne Arundel     173922 129893     0   632  2257   434  2951  96403 406492
 3 Baltimore City   303620  28211     0   908  1080   666  3984  56665 395134
 4 Baltimore County 309297 137378     0   898  2493   687  5921 106789 563463
 5 Calvert           23779  27912     0    94   410    69   548  15169  67981
 6 Caroline           6250  10539     0    34   108    40   160   4454  21585
 7 Carroll           33572  63771     0   191   797   102  1033  28139 127605
 8 Cecil             20666  31961     0   111   447   104   678  16360  70327
 9 Charles           74373  23334     0   129   425   143   864  21819 121087
10 Dorchester         9608   8965     0    22   110    33   191   3745  22674
# i 15 more rows
\end{verbatim}

Boom - we're good to go.

\hypertarget{when-it-looks-good-but-needs-a-little-fixing}{%
\section{When it looks good, but needs a little
fixing}\label{when-it-looks-good-but-needs-a-little-fixing}}

Here's a slightly more involved PDF, from
\href{https://health.maryland.gov/vsa/Documents/Overdose/Annual_2020_Drug_Intox_Report.pdf}{Maryland's
2020 annual report on unintentional drug and alcohol-related
intoxication deaths}. Specifically, we're looking at Table 7 on page 67
of the report which lists the number of fentanyl-related deaths by
jurisdiction:

\includegraphics[width=3.2in,height=\textheight]{./images/md_fentanyl_deaths_1.png}

Looks like a spreadsheet, right? Save that PDF file to your computer in
a place where you'll remember it (like a Downloads folder).

Now let's repeat the steps we did to import the PDF into Tabula, go to
page 67. It should look like this:

\includegraphics[width=2.88in,height=\textheight]{./images/md_fentanyl_deaths_2.png}

Let's draw a box around what we want, but there's a catch: the headers
aren't on a single line. If you draw your box around the whole table and
preview, you'll see that there's a problem. To fix that, we'll need to
limit our box to just the data. Using your cursor, click and drag a box
across the table so it looks like this:

\includegraphics[width=2.29in,height=\textheight]{./images/md_fentanyl_deaths_3.png}

Now you can hit the green ``Preview \& Export Extracted Data'' button on
the top right. Using the ``Stream'' method, you should see something
very like this:

\includegraphics[width=2.87in,height=\textheight]{./images/md_fentanyl_deaths_4.png}

You can now export that extracted table to a CSV file using the
``Export'' button. And then we can read it into R and clean up the
column names and some other things:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/tabula{-}Annual\_2020\_Drug\_Intox\_Report.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{clean\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 29 Columns: 12
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): MARYLAND ......................................
dbl (6): 26, 29, 58, 186, 340, 1,119

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 29 x 12
   maryland       x26   x29   x58  x186  x340 x1_119 x1_594 x1_888 x1_927 x2_342
   <chr>        <dbl> <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>
 1 NORTHWEST A~     6     3     7     8    32    109    119    166    146    200
 2 GARRETT ...~     1     0     0     0     2      0      2      2      5      5
 3 ALLEGANY ..~     1     1     1     1     5     29     29     29     19     44
 4 WASHINGTON ~     1     1     4     1    14     31     39     70     70     95
 5 FREDERICK .~     3     1     2     6    11     49     49     65     52     56
 6 BALTIMORE M~    10    16    35   142   248    792   1118   1415   1395   1605
 7 BALTIMORE C~     2     4    12    72   120    419    573    758    810    920
 8 BALTIMORE C~     4     5    11    36    65    182    244    308    285    328
 9 ANNE ARUNDE~     2     3     6    23    29     98    152    184    164    209
10 CARROLL ...~     0     1     2     4    11     20     40     55     47     37
# i 19 more rows
# i 1 more variable: x9_509 <dbl>
\end{verbatim}

\hypertarget{cleaning-up-the-data-in-r}{%
\section{Cleaning up the data in R}\label{cleaning-up-the-data-in-r}}

The good news is that we have data we don't have to retype. The bad news
is, we have a few things to fix, starting with the fact that the headers
shouldn't be headers. Let's start by re-importing it and specifying that
the first row doesn't have column headers:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/tabula{-}Annual\_2020\_Drug\_Intox\_Report.csv"}\NormalTok{, }\AttributeTok{col\_names =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{clean\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30 Columns: 12
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): X1
dbl (5): X2, X3, X4, X5, X6

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 12
   x1             x2    x3    x4    x5    x6    x7    x8    x9   x10   x11   x12
   <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
 1 MARYLAND .~    26    29    58   186   340  1119  1594  1888  1927  2342  9509
 2 NORTHWEST ~     6     3     7     8    32   109   119   166   146   200   796
 3 GARRETT ..~     1     0     0     0     2     0     2     2     5     5    17
 4 ALLEGANY .~     1     1     1     1     5    29    29    29    19    44   159
 5 WASHINGTON~     1     1     4     1    14    31    39    70    70    95   326
 6 FREDERICK ~     3     1     2     6    11    49    49    65    52    56   294
 7 BALTIMORE ~    10    16    35   142   248   792  1118  1415  1395  1605  6776
 8 BALTIMORE ~     2     4    12    72   120   419   573   758   810   920  3690
 9 BALTIMORE ~     4     5    11    36    65   182   244   308   285   328  1468
10 ANNE ARUND~     2     3     6    23    29    98   152   184   164   209   870
# i 20 more rows
\end{verbatim}

Ok, now we have all the data. But we need actual headers. Let's add
those using \texttt{rename()}, keeping in mind that the new name comes
\emph{first}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/tabula{-}Annual\_2020\_Drug\_Intox\_Report.csv"}\NormalTok{, }\AttributeTok{col\_names =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{jurisdiction =}\NormalTok{ x1, }\AttributeTok{deaths\_2011 =}\NormalTok{ x2, }\AttributeTok{deaths\_2012 =}\NormalTok{ x3, }\AttributeTok{deaths\_2013 =}\NormalTok{ x4, }\AttributeTok{deaths\_2014 =}\NormalTok{ x5, }\AttributeTok{deaths\_2015 =}\NormalTok{ x6, }\AttributeTok{deaths\_2016 =}\NormalTok{ x7, }\AttributeTok{deaths\_2017 =}\NormalTok{ x8, }
         \AttributeTok{deaths\_2018 =}\NormalTok{ x9, }\AttributeTok{deaths\_2019 =}\NormalTok{ x10, }\AttributeTok{deaths\_2020 =}\NormalTok{ x11, }\AttributeTok{deaths\_total =}\NormalTok{ x12)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30 Columns: 12
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): X1
dbl (5): X2, X3, X4, X5, X6

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 12
   jurisdiction      deaths_2011 deaths_2012 deaths_2013 deaths_2014 deaths_2015
   <chr>                   <dbl>       <dbl>       <dbl>       <dbl>       <dbl>
 1 MARYLAND .......~          26          29          58         186         340
 2 NORTHWEST AREA .~           6           3           7           8          32
 3 GARRETT ........~           1           0           0           0           2
 4 ALLEGANY .......~           1           1           1           1           5
 5 WASHINGTON .....~           1           1           4           1          14
 6 FREDERICK ......~           3           1           2           6          11
 7 BALTIMORE METRO ~          10          16          35         142         248
 8 BALTIMORE CITY .~           2           4          12          72         120
 9 BALTIMORE COUNTY~           4           5          11          36          65
10 ANNE ARUNDEL ...~           2           3           6          23          29
# i 20 more rows
# i 6 more variables: deaths_2016 <dbl>, deaths_2017 <dbl>, deaths_2018 <dbl>,
#   deaths_2019 <dbl>, deaths_2020 <dbl>, deaths_total <dbl>
\end{verbatim}

We could stop here, but there are a bunch of periods in the jurisdiction
column and it's better to remove them - it will make filtering easier.
Let's use \texttt{str\_replace\_all()} to do that:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/tabula{-}Annual\_2020\_Drug\_Intox\_Report.csv"}\NormalTok{, }\AttributeTok{col\_names =} \ConstantTok{FALSE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{jurisdiction =}\NormalTok{ x1, }\AttributeTok{deaths\_2011 =}\NormalTok{ x2, }\AttributeTok{deaths\_2012 =}\NormalTok{ x3, }\AttributeTok{deaths\_2013 =}\NormalTok{ x4, }\AttributeTok{deaths\_2014 =}\NormalTok{ x5, }\AttributeTok{deaths\_2015 =}\NormalTok{ x6, }\AttributeTok{deaths\_2016 =}\NormalTok{ x7, }\AttributeTok{deaths\_2017 =}\NormalTok{ x8, }
         \AttributeTok{deaths\_2018 =}\NormalTok{ x9, }\AttributeTok{deaths\_2019 =}\NormalTok{ x10, }\AttributeTok{deaths\_2020 =}\NormalTok{ x11, }\AttributeTok{deaths\_total =}\NormalTok{ x12) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{jurisdiction =} \FunctionTok{str\_squish}\NormalTok{(}\FunctionTok{str\_replace\_all}\NormalTok{(jurisdiction,}\StringTok{\textquotesingle{}}\SpecialCharTok{\textbackslash{}\textbackslash{}}\StringTok{.\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}\textquotesingle{}}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 30 Columns: 12
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): X1
dbl (5): X2, X3, X4, X5, X6

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fentanyl\_deaths}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 30 x 12
   jurisdiction      deaths_2011 deaths_2012 deaths_2013 deaths_2014 deaths_2015
   <chr>                   <dbl>       <dbl>       <dbl>       <dbl>       <dbl>
 1 MARYLAND                   26          29          58         186         340
 2 NORTHWEST AREA              6           3           7           8          32
 3 GARRETT                     1           0           0           0           2
 4 ALLEGANY                    1           1           1           1           5
 5 WASHINGTON                  1           1           4           1          14
 6 FREDERICK                   3           1           2           6          11
 7 BALTIMORE METRO ~          10          16          35         142         248
 8 BALTIMORE CITY              2           4          12          72         120
 9 BALTIMORE COUNTY            4           5          11          36          65
10 ANNE ARUNDEL                2           3           6          23          29
# i 20 more rows
# i 6 more variables: deaths_2016 <dbl>, deaths_2017 <dbl>, deaths_2018 <dbl>,
#   deaths_2019 <dbl>, deaths_2020 <dbl>, deaths_total <dbl>
\end{verbatim}

There are a few important things to explain here:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Because we're replacing a literal period (.), we need to make sure
  that R knows that. Hence the `\textbackslash.' Why? Because `.' is a
  valid expression meaning ``any character'', so if we didn't have the
  backslashes the above code would make the entire column blank (try
  it!)
\item
  The \texttt{str\_squish} function cleans up any excess spaces, at the
  beginning, middle or end of a character column. If we then use filter,
  we can do so with confidence.
\item
  I put ``deaths\_'' in front of each yearly column because R likes it
  when columns don't begin with a number. You can have a column called
  \texttt{2011}, but you literally have to use the backticks
  (\texttt{2011}) to refer to it in code.
\end{enumerate}

All things considered, that was pretty easy. Many - most? - electronic
PDFs aren't so easy to parse. Sometimes you'll need to open the exported
CSV file and clean things up before importing into R. Other times you'll
be able to do that cleaning in R itself.

Here's the sad truth: THIS IS PRETTY GOOD. It sure beats typing it out.
And since many government processes don't change all that much, you can
save the code to process subsequent versions of PDFs.

\bookmarksetup{startatroot}

\hypertarget{combining-and-joining}{%
\chapter{Combining and joining}\label{combining-and-joining}}

Often, as data journalists, we're looking at data across time or at data
stored in multiple tables. And to do that, we need to often need to
merge that data together.

Depending on what we have, we may just need to stack data on top of each
other to make new data. If we have 2019 data and 2018 data and we want
that to be one file, we stack them. If we have a dataset of cows in
counties and a dataset of populations in county, we're going to join
those two together on the county -- the common element.

Let's explore.

\hypertarget{combining-data-stacking}{%
\section{Combining data (stacking)}\label{combining-data-stacking}}

Let's say that we have Maryland county voter registration data from four
different elections in four different files. They have the same record
layout and the same number of counties (plus Baltimore City). We can
combine them into a single dataframe.

Let's do what we need to import them properly. I've merged it all into
one step for each of the three datasets.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_voters\_2016 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/county\_voters\_2016.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 9
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): COUNTY
dbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, TOTAL

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_voters\_2018 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/county\_voters\_2018.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 9
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): COUNTY
dbl (8): YEAR, DEM, REP, LIB, GRN, UNA, OTH, TOTAL

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_voters\_2020 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/county\_voters\_2020.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 9
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): COUNTY
dbl (8): YEAR, DEM, REP, GRN, LIB, OTH, UNA, TOTAL

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_voters\_2022 }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/county\_voters\_2022.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 9
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): COUNTY
dbl (8): YEAR, DEM, REP, GRN, LIB, UNA, OTH, TOTAL

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

All three of these datasets have the same number of columns, all with
the same names, so if we want to merge them together to compare them
over time, we need to stack them together. The verb here, in R, is
\texttt{bind\_rows}. You tell the function what you want to combine and
it does it, assuming that you've got column names in common containing
identically formatted data.

Since we have three dataframes, we're going to need to pass them as a
list, meaning they'll be enclosed inside the \texttt{list} function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_voters\_combined }\OtherTok{\textless{}{-}} \FunctionTok{bind\_rows}\NormalTok{(}\FunctionTok{list}\NormalTok{(county\_voters\_2016, county\_voters\_2018, county\_voters\_2020, county\_voters\_2022))}
\end{Highlighting}
\end{Shaded}

And boom, like that, we have 96 rows of data together instead of four
dataframes. There are plenty of uses for \texttt{bind\_rows}: any
regularly updated data that comes in the same format like crime reports
or award recipients or player game statistics. Or election results.

\hypertarget{joining-data}{%
\section{Joining data}\label{joining-data}}

More complicated is when you have two separate tables that are connected
by a common element or elements. But there's a verb for that, too:
\texttt{join}.

Let's start by reading in some Maryland 2020 county population data:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{\textquotesingle{}data/maryland\_population\_2020.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 2
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (1): COUNTY
dbl (1): POP2020

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

One of the columns we have is called \texttt{county}, which is what we
have in our county\_voters\_2020 dataframe.

To put the Maryland population data and voter registration data
together, we need to use something called a join. There are different
kinds of joins. It's better if you think of two tables sitting next to
each other. A \texttt{left\_join} takes all the records from the left
table and only the records that match in the right one. A
\texttt{right\_join} does the same thing. An \texttt{inner\_join} takes
only the records where they are equal. There's one other join -- a
\texttt{full\_join} which returns all rows of both, regardless of if
there's a match -- but I've never once had a use for a full join.

In the best-case scenario, the two tables we want to join share a common
column. In this case, both of our tables have a column called
\texttt{county} that has the same characteristics: values in both look
identical, including how they distinguish Baltimore City from Baltimore
County. This is important, because joins work on \emph{exact matches}.

We can do this join multiple ways and get a similar result. We can put
the population file on the left and the registration data on the right
and use a left join to get them all together. And we use \texttt{by=} to
join by the correct column. I'm going to count the rows at the end. The
reason I'm doing this is important: \textbf{Rule 1 in joining data is
having an idea of what you are expecting to get}. So with a left join
with population on the left, I have 24 rows, so I expect to get 24 rows
when I'm done.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(county\_voters\_2020, }\FunctionTok{join\_by}\NormalTok{(}\StringTok{"COUNTY"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{nrow}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 24
\end{verbatim}

Remove the nrow and run it again for yourself. By default,
\texttt{dplyr} will do a ``natural'' join, where it'll match all the
matching columns in both tables. So if we take out the
\texttt{join\_by()}, it'll use all the common columns between the
tables. That may not be right in every instance but let's try it. If it
works, we should get 24 rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(county\_voters\_2020)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(COUNTY)`
\end{verbatim}

\begin{verbatim}
# A tibble: 24 x 10
   COUNTY           POP2020  YEAR    DEM    REP   GRN   LIB   OTH    UNA  TOTAL
   <chr>              <dbl> <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>  <dbl>  <dbl>
 1 Allegany           68106  2020  12820  22530    74   204   434   7674  43736
 2 Anne Arundel      588261  2020 174494 135457   564  1922  3017  90162 405616
 3 Baltimore City    585708  2020 311610  30163   802   951  3709  52450 399685
 4 Baltimore County  854535  2020 313870 142534   898  2227  6303 100576 566408
 5 Calvert            92783  2020  24587  28181    89   332   617  14178  67984
 6 Caroline           33293  2020   6629  10039    33    86   182   4208  21177
 7 Carroll           172891  2020  33662  63967   155   670  1137  25770 125361
 8 Cecil             103725  2020  21601  30880   103   341   784  15110  68819
 9 Charles           166617  2020  72416  24711   112   349   865  19849 118302
10 Dorchester         32531  2020   9848   8730    19    78   164   3348  22187
# i 14 more rows
\end{verbatim}

Since we only have one column in common between the two tables, the join
only used that column. And we got the same answer. If we had more
columns in common, you could see in your results columns with .X after
them - that's a sign of duplicative columns between two tables, and you
may decide you don't need both moving forward.

Let's save our joined data to a new dataframe, but this time let's
remove the select function so we don't limit the columns to just three.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population\_with\_voters }\OtherTok{\textless{}{-}}\NormalTok{ maryland\_population }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(county\_voters\_2020)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(COUNTY)`
\end{verbatim}

Now, with our joined data, we can answer questions in a more useful way.
But joins can do even more than just bring data together; they can
include additional data to enable you to ask more sophisticated
questions. Right now we have registered voters and total population. But
we can do more.

Let's try adding more Maryland demographic data to the mix. Using a file
describing the 18-and-over population (from which eligible voters come)
from
\href{https://planning.maryland.gov/MSDC/Pages/census/Census2020/pL_2020redistricting.aspx}{the
state's data catalog}, we can read it into R:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_demographics }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{\textquotesingle{}data/maryland\_demographics.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 24 Columns: 11
-- Column specification --------------------------------------------------------
Delimiter: ","
chr  (1): NAME
dbl (10): GEOCODE, pop_18_over, pop_one_race, pop_white, pop_black, pop_nati...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

Again, we can use a \texttt{left\_join} to make our demographic data
available. This time we'll need to specify the two fields to join
because they do not have identical names. We'll use \texttt{COUNTY} from
our population data and \texttt{NAME} from the demographic data, and the
order matters - the first column is from the dataframe you name
\emph{first}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population\_with\_voters\_and\_demographics }\OtherTok{\textless{}{-}}\NormalTok{ maryland\_population\_with\_voters }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(maryland\_demographics, }\FunctionTok{join\_by}\NormalTok{(COUNTY }\SpecialCharTok{==}\NormalTok{ NAME))}
\end{Highlighting}
\end{Shaded}

Now we've got population data and demographic data by county. That means
we can draw from both datasets in asking our questions. For example, we
could see the counties with the highest 18+ Black population as a
percentage of all population 18 and over and also the percentage of
Democrats in that county.

We can get this by using mutate and arrange:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population\_with\_voters\_and\_demographics }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pct\_black\_18\_plus =}\NormalTok{ (pop\_black}\SpecialCharTok{/}\NormalTok{pop\_18\_over)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{, }\AttributeTok{pct\_dems =}\NormalTok{ (DEM}\SpecialCharTok{/}\NormalTok{TOTAL)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pct\_black\_18\_plus)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(COUNTY, pct\_black\_18\_plus, pct\_dems)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 24 x 3
   COUNTY           pct_black_18_plus pct_dems
   <chr>                        <dbl>    <dbl>
 1 Prince George's               60.9     78.3
 2 Baltimore City                56.3     78.0
 3 Charles                       48.2     61.2
 4 Somerset                      39.0     41.8
 5 Baltimore County              28.8     55.4
 6 Dorchester                    26.2     44.4
 7 Wicomico                      25.6     42.3
 8 Howard                        18.7     52.4
 9 Montgomery                    18.1     61.0
10 Anne Arundel                  17.4     43.0
# i 14 more rows
\end{verbatim}

If you know Maryland political demographics, this result isn't too
surprising, but Somerset County - the state's 2nd smallest in terms of
population - stands out for its Black population, which is a greater
percentage than Baltimore County and Montgomery County.

Let's change that to look at Asian population:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_population\_with\_voters\_and\_demographics }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pct\_asian\_18\_plus =}\NormalTok{ (pop\_asian}\SpecialCharTok{/}\NormalTok{pop\_18\_over)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{, }\AttributeTok{pct\_dems =}\NormalTok{ (DEM}\SpecialCharTok{/}\NormalTok{TOTAL)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(pct\_asian\_18\_plus)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(COUNTY, pct\_asian\_18\_plus, pct\_dems)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 24 x 3
   COUNTY           pct_asian_18_plus pct_dems
   <chr>                        <dbl>    <dbl>
 1 Howard                       19.4      52.4
 2 Montgomery                   16.0      61.0
 3 Baltimore County              6.34     55.4
 4 Frederick                     4.88     38.9
 5 Prince George's               4.68     78.3
 6 Anne Arundel                  4.52     43.0
 7 Baltimore City                4.17     78.0
 8 Charles                       3.55     61.2
 9 Harford                       3.15     35.4
10 St. Mary's                    3.13     35.7
# i 14 more rows
\end{verbatim}

Here, Howard and Montgomery County stand out in terms of the percentage
of Asian population 18 and over. The jurisdictions with the highest
percentage of Democrats - Prince George's and Baltimore City - have
small Asian populations.

Sometimes joins look like they should work but don't. Often this is due
to the two columns you're joining on having different data types:
joining a column to a column, for example. Let's walk through an example
of that using some demographic data by zip code.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_zcta }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{\textquotesingle{}data/maryland\_zcta.csv\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 468 Columns: 40
-- Column specification --------------------------------------------------------
Delimiter: ","
chr  (5): FIRST_CLAS, FIRST_MTFC, FIRST_FUNC, REPORT_2_P, REPORT_9_P
dbl (35): OBJECTID_1, ZCTA5CE10, FIRST_STAT, FIRST_GEOI, ZCTA5N, STATE, AREA...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(maryland\_zcta)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 468
Columns: 40
$ OBJECTID_1 <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ~
$ ZCTA5CE10  <dbl> 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206~
$ FIRST_STAT <dbl> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,~
$ FIRST_GEOI <dbl> 2420601, 2420602, 2420603, 2420606, 2420607, 2420608, 24206~
$ FIRST_CLAS <chr> "B5", "B5", "B5", "B5", "B5", "B5", "B5", "B5", "B5", "B5",~
$ FIRST_MTFC <chr> "G6350", "G6350", "G6350", "G6350", "G6350", "G6350", "G635~
$ FIRST_FUNC <chr> "S", "S", "S", "S", "S", "S", "S", "S", "S", "S", "S", "S",~
$ ZCTA5N     <dbl> 20601, 20602, 20603, 20606, 20607, 20608, 20609, 20611, 206~
$ STATE      <dbl> 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24,~
$ AREALAND   <dbl> 115635266, 35830723, 44239637, 7501011, 54357590, 45583064,~
$ AREAWATR   <dbl> 387684, 352762, 219356, 1248760, 448221, 5330329, 6602735, ~
$ POP100     <dbl> 24156, 24955, 28967, 431, 9802, 919, 1120, 1078, 261, 11860~
$ HU100      <dbl> 8722, 9736, 10317, 230, 3504, 426, 554, 413, 142, 4424, 204~
$ NHW        <dbl> 9785, 8466, 9625, 377, 2165, 438, 1009, 798, 245, 4044, 352~
$ NHB        <dbl> 11146, 13054, 15025, 45, 6321, 453, 82, 215, 12, 6786, 32, ~
$ NHAI       <dbl> 155, 116, 98, 1, 33, 5, 2, 5, 0, 106, 2, 32, 3, 4, 38, 8, 1~
$ NHA        <dbl> 880, 731, 1446, 4, 560, 2, 1, 10, 0, 186, 3, 165, 5, 1, 402~
$ NHNH       <dbl> 11, 15, 24, 0, 3, 0, 1, 0, 0, 4, 1, 2, 0, 0, 4, 1, 0, 3, 1,~
$ NHO        <dbl> 48, 58, 65, 0, 6, 0, 0, 0, 0, 8, 0, 1, 0, 3, 5, 8, 0, 5, 10~
$ NHT        <dbl> 849, 999, 1091, 0, 234, 9, 15, 33, 1, 321, 13, 213, 14, 4, ~
$ HISP       <dbl> 1282, 1516, 1593, 4, 480, 12, 10, 17, 3, 405, 2, 244, 9, 7,~
$ PNHW       <dbl> 40.5, 33.9, 33.2, 87.5, 22.1, 47.7, 90.1, 74.0, 93.9, 34.1,~
$ PNHB       <dbl> 46.1, 52.3, 51.9, 10.4, 64.5, 49.3, 7.3, 19.9, 4.6, 57.2, 7~
$ PNHAI      <dbl> 0.6, 0.5, 0.3, 0.2, 0.3, 0.5, 0.2, 0.5, 0.0, 0.9, 0.5, 0.5,~
$ PNHA       <dbl> 3.6, 2.9, 5.0, 0.9, 5.7, 0.2, 0.1, 0.9, 0.0, 1.6, 0.7, 2.8,~
$ PNHNH      <dbl> 0.0, 0.1, 0.1, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0, 0.0, 0.2, 0.0,~
$ PNHO       <dbl> 0.2, 0.2, 0.2, 0.0, 0.1, 0.0, 0.0, 0.0, 0.0, 0.1, 0.0, 0.0,~
$ PNHT       <dbl> 3.5, 4.0, 3.8, 0.0, 2.4, 1.0, 1.3, 3.1, 0.4, 2.7, 3.2, 3.6,~
$ PHISP      <dbl> 5.3, 6.1, 5.5, 0.9, 4.9, 1.3, 0.9, 1.6, 1.1, 3.4, 0.5, 4.2,~
$ POP65_     <dbl> 1922, 1964, 1400, 108, 847, 173, 271, 129, 54, 1372, 73, 55~
$ PCTPOP65_  <dbl> 8.0, 7.9, 4.8, 25.1, 8.6, 18.8, 24.2, 12.0, 20.7, 11.6, 18.~
$ MEDAGE     <dbl> 37.3, 32.6, 34.5, 49.1, 40.9, 46.6, 47.6, 44.3, 47.3, 40.8,~
$ VACNS      <dbl> 376, 769, 531, 15, 172, 39, 32, 22, 14, 249, 18, 158, 8, 18~
$ PVACNS     <dbl> 4.3, 7.9, 5.1, 6.5, 4.9, 9.2, 5.8, 5.3, 9.9, 5.6, 8.8, 7.2,~
$ PHOWN      <dbl> 71.1, 59.7, 73.8, 49.7, 83.1, 60.4, 44.8, 63.8, 38.3, 73.9,~
$ PWOMORT    <dbl> 11.2, 9.0, 4.7, 39.3, 10.3, 28.2, 38.7, 21.8, 43.9, 17.4, 2~
$ PRENT      <dbl> 19.9, 34.4, 22.6, 18.1, 7.4, 15.9, 27.0, 18.3, 31.7, 10.5, ~
$ PLT18SP    <dbl> 30.4, 43.6, 29.9, 31.2, 22.1, 14.1, 28.9, 24.5, 43.9, 26.7,~
$ REPORT_2_P <chr> "http://mdpgis.mdp.state.md.us/Census2010/PDF/00_SF1DP_2Pro~
$ REPORT_9_P <chr> "http://mdpgis.mdp.state.md.us/census2010/PDF/00_SF1_9PROFI~
\end{verbatim}

You can see that ZCTA5N, the column representing the Zip Code Tabulation
Area, is a numeric column. But should it be? Do we ever want to know the
average zip code in Maryland? Zip codes and ZCTAs look like numbers but
really are character columns. Let's change that so that we can be sure
to join them correctly with other data where the zip codes are not
numbers. We'll use \texttt{mutate}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{maryland\_zcta }\OtherTok{\textless{}{-}}\NormalTok{ maryland\_zcta }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{ZCTA5N =} \FunctionTok{as.character}\NormalTok{(ZCTA5N))}
\end{Highlighting}
\end{Shaded}

What's happening here is that we're telling R to take all of the values
in the ZCTA5N column and make them ``as.character''. If we wanted to
change a column to numeric, we'd do ``as.numeric''. When you join two
dataframes, the join columns \emph{must} be the same datatype.

Joining datasets allows you to expand the range and sophistication of
questions you're able to ask. It is one of the most powerful tools in a
journalist's toolkit.

\bookmarksetup{startatroot}

\hypertarget{intro-to-apis-the-census}{%
\chapter{Intro to APIs: The Census}\label{intro-to-apis-the-census}}

There is truly an astonishing amount of data collected by the US Census
Bureau. First, there's the Census that most people know -- the every 10
year census. That's the one mandated by the Constitution where the
government attempts to count every person in the US. It's a
mind-boggling feat to even try, and billions get spent on it. That data
is used first for determining how many representatives each state gets
in Congress. From there, the Census gets used to divide up billions of
dollars of federal spending.

To answer the questions the government needs to do that, a ton of data
gets collected. That, unfortunately, means the Census is exceedingly
complicated to work with. The good news is, the Census has an API -- an
application programming interface. What that means is we can get data
directly through the Census Bureau via calls over the internet.

Let's demonstrate.

We're going to use a library called \texttt{tidycensus} which makes
calls to the Census API in a very tidy way, and gives you back tidy
data. That means we don't have to go through the process of importing
the data from a file. I can't tell you how amazing this is, speaking
from experience. The documentation for this library is
\href{https://walker-data.com/tidycensus/}{here}. Another R library for
working with Census APIs (there is more than one) is
\href{https://github.com/hrecht/censusapi}{this one} from Hannah Recht,
a journalist with Kaiser Health News.

First we need to install \texttt{tidycensus} using the console:
\texttt{install.packages("tidycensus",\ dependencies\ =\ TRUE)}. You
also should install the \texttt{sf} and \texttt{rgdal} packages.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidycensus)}
\end{Highlighting}
\end{Shaded}

To use the API, you need an API key. To get that, you need to
\href{https://api.census.gov/data/key_signup.html}{apply for an API key
with the Census Bureau}. It takes a few minutes and you need to activate
your key via email. Once you have your key, you need to set that for
this session. Just FYI: Your key is your key. Do not share it around.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{census\_api\_key}\NormalTok{(}\StringTok{"YOUR KEY HERE"}\NormalTok{, }\AttributeTok{install=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The two main functions in tidycensus are \texttt{get\_decennial}, which
retrieves data from the 2000 and 2010 Censuses (and soon the 2020
Census), and \texttt{get\_acs}, which pulls data from the American
Community Survey, a between-Censuses annual survey that provides
estimates, not hard counts, but asks more detailed questions. If you're
new to Census data, there's
\href{http://walker-data.com/umich-workshop/census-data-in-r/slides/\#1}{a
very good set of slides from Kyle Walker}, the creator of tidycensus,
and he's working on a
\href{https://walker-data.com/census-r/index.html}{book} that you can
read for free online.

It's important to keep in mind that Census data represents people - you,
your neighbors and total strangers. It also requires some level of
definitions, especially about race \& ethnicity, that may or may not
match how you define yourself or how others define themselves.

So to give you some idea of how complicated the data is, let's pull up
just one file from the decennial Census. We'll use Summary File 1, or
SF1. That has the major population and housing stuff.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sf1 }\OtherTok{\textless{}{-}} \FunctionTok{load\_variables}\NormalTok{(}\DecValTok{2010}\NormalTok{, }\StringTok{"sf1"}\NormalTok{, }\AttributeTok{cache =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{sf1}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 8,959 x 3
   name    label                                concept         
   <chr>   <chr>                                <chr>           
 1 H001001 Total                                HOUSING UNITS   
 2 H002001 Total                                URBAN AND RURAL 
 3 H002002 Total!!Urban                         URBAN AND RURAL 
 4 H002003 Total!!Urban!!Inside urbanized areas URBAN AND RURAL 
 5 H002004 Total!!Urban!!Inside urban clusters  URBAN AND RURAL 
 6 H002005 Total!!Rural                         URBAN AND RURAL 
 7 H002006 Total!!Not defined for this file     URBAN AND RURAL 
 8 H003001 Total                                OCCUPANCY STATUS
 9 H003002 Total!!Occupied                      OCCUPANCY STATUS
10 H003003 Total!!Vacant                        OCCUPANCY STATUS
# i 8,949 more rows
\end{verbatim}

Note: There are thousands of variables in SF1. That's not a typo. Open
it in your environment by double clicking. As you scroll down, you'll
get an idea of what you've got to choose from.

If you think that's crazy, try the SF3 file from 2000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sf3 }\OtherTok{\textless{}{-}} \FunctionTok{load\_variables}\NormalTok{(}\DecValTok{2000}\NormalTok{, }\StringTok{"sf3"}\NormalTok{, }\AttributeTok{cache =} \ConstantTok{TRUE}\NormalTok{)}

\NormalTok{sf3}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 16,520 x 3
   name    label                                       concept                  
   <chr>   <chr>                                       <chr>                    
 1 H001001 Total                                       HOUSING UNITS [1]        
 2 H002001 Total                                       UNWEIGHTED SAMPLE HOUSIN~
 3 H002002 Total!!Occupied                             UNWEIGHTED SAMPLE HOUSIN~
 4 H002003 Total!!Vacant                               UNWEIGHTED SAMPLE HOUSIN~
 5 H003001 Total                                       100-PERCENT COUNT OF HOU~
 6 H004001 Percent of occupied housing units in sample PERCENT OF HOUSING UNITS~
 7 H004002 Percent of vacant housing units in sample   PERCENT OF HOUSING UNITS~
 8 H005001 Total                                       URBAN AND RURAL [7]      
 9 H005002 Total!!Urban                                URBAN AND RURAL [7]      
10 H005003 Total!!Urban!!Inside urbanized areas        URBAN AND RURAL [7]      
# i 16,510 more rows
\end{verbatim}

Yes. That's more than 16,000 variables to choose from. I told you.
Astonishing.

So let's try to answer a question using the Census. What is the fastest
growing state since 2000?

To answer this, we need to pull the total population by state in each of
the decennial census. Here's 2000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p00 }\OtherTok{\textless{}{-}} \FunctionTok{get\_decennial}\NormalTok{(}\AttributeTok{geography =} \StringTok{"state"}\NormalTok{, }\AttributeTok{variables =} \StringTok{"P001001"}\NormalTok{, }\AttributeTok{year =} \DecValTok{2000}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now 2010.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p10 }\OtherTok{\textless{}{-}} \FunctionTok{get\_decennial}\NormalTok{(}\AttributeTok{geography =} \StringTok{"state"}\NormalTok{, }\AttributeTok{variables =} \StringTok{"P001001"}\NormalTok{, }\AttributeTok{year =} \DecValTok{2010}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Let's take a peek at 2010.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p10}
\end{Highlighting}
\end{Shaded}

As you can see, we have a GEOID, NAME, then variable and value. Variable
and value are going to be the same. Because those are named the same
thing, to merge them together, we need to rename them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p10 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(GEOID, NAME, value) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{Population2010=}\NormalTok{value) }\OtherTok{{-}\textgreater{}}\NormalTok{ p2010}

\NormalTok{p00 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(GEOID, NAME, value) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{rename}\NormalTok{(}\AttributeTok{Population2000=}\NormalTok{value) }\OtherTok{{-}\textgreater{}}\NormalTok{ p2000}
\end{Highlighting}
\end{Shaded}

Now we join the data together.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alldata }\OtherTok{\textless{}{-}}\NormalTok{ p2000 }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{inner\_join}\NormalTok{(p2010)}
\end{Highlighting}
\end{Shaded}

And now we calculate the percent change.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{alldata }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{change =}\NormalTok{ ((Population2010}\SpecialCharTok{{-}}\NormalTok{Population2000)}\SpecialCharTok{/}\NormalTok{Population2000)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(change))}
\end{Highlighting}
\end{Shaded}

And just like that: Nevada.

You may be asking: hey, wasn't there a 2020 Census? Where's that data?
The answer is that it's coming, slowly - the Census Bureau has a
\href{https://www.census.gov/programs-surveys/popest/about/schedule.html}{schedule
of releases}.

\hypertarget{the-acs}{%
\section{The ACS}\label{the-acs}}

In 2010, the Census Bureau replaced SF3 with the American Community
Survey. The Good News is that the data would be updated on a rolling
basis. The bad news is that it's more complicated because it's more like
survey data with a large sample. That means there's margins of error and
confidence intervals to worry about. By default, using \texttt{get\_acs}
fetches data from the 5-year estimates (currently 2016-2020), but you
can specify 1-year estimates for jurisdictions with at least 65,000
people (many counties and cities).

Here's an example using the 5-year ACS estimates:

What is Maryland's richest county?

We can measure this by median household income. That variable is
\texttt{B19013\_001}, so we can get that data like this (I'm narrowing
it to the top 20 for simplicity):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography =} \StringTok{"county"}\NormalTok{,}
              \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\AttributeTok{medincome =} \StringTok{"B19013\_001"}\NormalTok{),}
              \AttributeTok{state =} \StringTok{"MD"}\NormalTok{,}
              \AttributeTok{year =} \DecValTok{2020}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2016-2020 5-year ACS
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md }\OtherTok{\textless{}{-}}\NormalTok{ md }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(estimate)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{top\_n}\NormalTok{(}\DecValTok{20}\NormalTok{, estimate)}

\NormalTok{md}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 20 x 5
   GEOID NAME                             variable  estimate   moe
   <chr> <chr>                            <chr>        <dbl> <dbl>
 1 24027 Howard County, Maryland          medincome   124042  3448
 2 24009 Calvert County, Maryland         medincome   112696  3287
 3 24031 Montgomery County, Maryland      medincome   111812  1361
 4 24017 Charles County, Maryland         medincome   103678  1654
 5 24003 Anne Arundel County, Maryland    medincome   103225  1817
 6 24021 Frederick County, Maryland       medincome   100685  1927
 7 24013 Carroll County, Maryland         medincome    99569  3051
 8 24035 Queen Anne's County, Maryland    medincome    96467  4785
 9 24037 St. Mary's County, Maryland      medincome    95864  3872
10 24025 Harford County, Maryland         medincome    94003  2398
11 24033 Prince George's County, Maryland medincome    86994   865
12 24015 Cecil County, Maryland           medincome    79415  3479
13 24005 Baltimore County, Maryland       medincome    78724  1641
14 24041 Talbot County, Maryland          medincome    73102  4031
15 24047 Worcester County, Maryland       medincome    65396  3856
16 24043 Washington County, Maryland      medincome    63510  1930
17 24045 Wicomico County, Maryland        medincome    60366  2437
18 24029 Kent County, Maryland            medincome    60208  5185
19 24011 Caroline County, Maryland        medincome    59042  4215
20 24023 Garrett County, Maryland         medincome    54542  3487
\end{verbatim}

Howard, Calvert, Montgomery, Anne Arundel, Charles. What do they all
have in common? Lots of suburban flight from DC and Baltimore. But do
the margins of error let us say one county is richer than the other. We
can find this out visually using error bars. Don't worry much about the
code here -- we'll cover that soon enough.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{NAME =} \FunctionTok{gsub}\NormalTok{(}\StringTok{" County, Maryland"}\NormalTok{, }\StringTok{""}\NormalTok{, NAME)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ estimate, }\AttributeTok{y =} \FunctionTok{reorder}\NormalTok{(NAME, estimate))) }\SpecialCharTok{+}
  \FunctionTok{geom\_errorbarh}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xmin =}\NormalTok{ estimate }\SpecialCharTok{{-}}\NormalTok{ moe, }\AttributeTok{xmax =}\NormalTok{ estimate }\SpecialCharTok{+}\NormalTok{ moe)) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\AttributeTok{color =} \StringTok{"red"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Household income by county in Maryland"}\NormalTok{,}
       \AttributeTok{subtitle =} \StringTok{"2016{-}2020 American Community Survey"}\NormalTok{,}
       \AttributeTok{y =} \StringTok{""}\NormalTok{,}
       \AttributeTok{x =} \StringTok{"ACS estimate (bars represent margin of error)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./census_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

As you can see, some of the error bars are quite wide. Some are narrow.
But if the bars overlap, it means the difference between the two
counties is within the margin of error, and the differences aren't
statistically significant. So is the difference between Calvert and
Montgomery significant? Nope. Is the difference between Howard and
everyone else significant? Yes it is.

Let's ask another question of the ACS -- did any counties lose income
from the time of the global financial crisis to the current 5-year
window?

Let's re-label our first household income data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md20 }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography =} \StringTok{"county"}\NormalTok{,}
              \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\AttributeTok{medincome =} \StringTok{"B19013\_001"}\NormalTok{),}
              \AttributeTok{state =} \StringTok{"MD"}\NormalTok{,}
              \AttributeTok{year =} \DecValTok{2020}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2016-2020 5-year ACS
\end{verbatim}

And now we grab the 2010 median household income.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md10 }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography =} \StringTok{"county"}\NormalTok{,}
              \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\AttributeTok{medincome =} \StringTok{"B19013\_001"}\NormalTok{),}
              \AttributeTok{state =} \StringTok{"MD"}\NormalTok{,}
              \AttributeTok{year =} \DecValTok{2010}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2006-2010 5-year ACS
\end{verbatim}

What I'm going to do next is a lot, but each step is simple. I'm going
to join the data together, so each county has one line of data. Then I'm
going to rename some fields that repeat. Then I'm going to calculate the
minimium and maximum value of the estimate using the margin of error.
That'll help me later. After that, I'm going to calculate a perent
change and sort it by that change.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md10 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(md20, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{"GEOID"}\NormalTok{, }\StringTok{"NAME"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{estimate2010=}\NormalTok{estimate.x, }\AttributeTok{estimate2020=}\NormalTok{estimate.y) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{min2010 =}\NormalTok{ estimate2010}\SpecialCharTok{{-}}\NormalTok{moe.x, }\AttributeTok{max2010 =}\NormalTok{ estimate2010}\SpecialCharTok{+}\NormalTok{moe.x, }\AttributeTok{min2020 =}\NormalTok{ estimate2020}\SpecialCharTok{{-}}\NormalTok{moe.y, }\AttributeTok{max2020 =}\NormalTok{ estimate2020}\SpecialCharTok{+}\NormalTok{moe.y) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{variable.x, }\SpecialCharTok{{-}}\NormalTok{variable.y, }\SpecialCharTok{{-}}\NormalTok{moe.x, }\SpecialCharTok{{-}}\NormalTok{moe.y) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{change =}\NormalTok{ ((estimate2020}\SpecialCharTok{{-}}\NormalTok{estimate2010)}\SpecialCharTok{/}\NormalTok{estimate2010)}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(change)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 24 x 9
   GEOID NAME   estimate2010 estimate2020 min2010 max2010 min2020 max2020 change
   <chr> <chr>         <dbl>        <dbl>   <dbl>   <dbl>   <dbl>   <dbl>  <dbl>
 1 24011 Carol~        58799        59042   56740   60858   54827   63257  0.413
 2 24039 Somer~        42443        44980   39092   45794   40432   49528  5.98 
 3 24041 Talbo~        63017        73102   60081   65953   69071   77133 16.0  
 4 24017 Charl~        88825       103678   87268   90382  102024  105332 16.7  
 5 24019 Dorch~        45151        52799   43470   46832   49020   56578 16.9  
 6 24047 Worce~        55487        65396   52749   58225   61540   69252 17.9  
 7 24045 Wicom~        50752        60366   49313   52191   57929   62803 18.9  
 8 24035 Queen~        81096        96467   78068   84124   91682  101252 19.0  
 9 24023 Garre~        45760        54542   43729   47791   51055   58029 19.2  
10 24031 Montg~        93373       111812   92535   94211  110451  113173 19.7  
# i 14 more rows
\end{verbatim}

So according to this, Somerset and Caroline counties had the smallest
change between 2010 and 2020, while all other jurisdictions saw
double-digit percentage increases.

But did they?

Look at the min and max values for both. Is the change statistically
significant?

The ACS data has lots of variables, just like the decennial Census does.
To browse them, you can do this:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{v20 }\OtherTok{\textless{}{-}} \FunctionTok{load\_variables}\NormalTok{(}\DecValTok{2020}\NormalTok{, }\StringTok{"acs5"}\NormalTok{, }\AttributeTok{cache=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

And then view \texttt{v20} to see what kinds of variables are available
via the API.

\hypertarget{wide-results}{%
\section{``Wide'' Results}\label{wide-results}}

Although one of the chief strengths of tidycensus is that it offers a,
well, tidy display of Census data, it also has the ability to view
multiple variables spread across columns. This can be useful for
creating percentages and comparing multiple variables.

\hypertarget{sorting-results}{%
\section{Sorting Results}\label{sorting-results}}

You'll notice that we've used \texttt{arrange} to sort the results of
tidycensus functions, although that's done after we create a new
variable to hold the data. There's another way to use \texttt{arrange}
that you should know about, one that you can use for exploratory
analysis. An example using median household income from 2020:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md20 }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography =} \StringTok{"county"}\NormalTok{,}
              \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\AttributeTok{medincome =} \StringTok{"B19013\_001"}\NormalTok{),}
              \AttributeTok{state =} \StringTok{"MD"}\NormalTok{,}
              \AttributeTok{year =} \DecValTok{2020}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2016-2020 5-year ACS
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{arrange}\NormalTok{(md20, }\FunctionTok{desc}\NormalTok{(estimate))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 24 x 5
   GEOID NAME                          variable  estimate   moe
   <chr> <chr>                         <chr>        <dbl> <dbl>
 1 24027 Howard County, Maryland       medincome   124042  3448
 2 24009 Calvert County, Maryland      medincome   112696  3287
 3 24031 Montgomery County, Maryland   medincome   111812  1361
 4 24017 Charles County, Maryland      medincome   103678  1654
 5 24003 Anne Arundel County, Maryland medincome   103225  1817
 6 24021 Frederick County, Maryland    medincome   100685  1927
 7 24013 Carroll County, Maryland      medincome    99569  3051
 8 24035 Queen Anne's County, Maryland medincome    96467  4785
 9 24037 St. Mary's County, Maryland   medincome    95864  3872
10 24025 Harford County, Maryland      medincome    94003  2398
# i 14 more rows
\end{verbatim}

In this case we don't save the sorted results to a variable, we can just
see the output in the console.

\hypertarget{combining-with-other-data}{%
\section{Combining with Other Data}\label{combining-with-other-data}}

Combining Census data with other types of data really unlocks its power
for journalism. In this case, we'll use some 2019-20 data from Maryland
high schools, including the number of graduates and the number of
students who enrolled in college the next fall semester, plus the
percentage. We also have the zip code of the high school, which will
enable us to bring in Census data. Let's load the CSV file and make sure
the zip code is a character column, because that's what we'll need to
join.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_high\_schools }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/md\_high\_schools.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{zip =} \FunctionTok{as.character}\NormalTok{(zip))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 201 Columns: 7
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (3): cohort_year, local_school_system, school
dbl (4): zip, high_school_graduates, enrollment, percent_of_high_school_grad...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

The Census Bureau has Zip Code Tabulation Areas - they are mostly
identical to what we know as zip codes, with some small differences. In
most cases we can use them interchangeably. Let's get ZCTA household
income data from the ACS using tidycensus for each zip code in
\texttt{md\_high\_schools}. Run the following codeblock to retrieve
household income data for all Maryland ZCTAs first, then join them to
\texttt{md\_high\_schools} using the GEOID column. We'll also create min
and max values and drop any rows where we don't have income estimates.
What's one question we could ask of this data now that we have
enrollment percentages and household income for the high school's zip
code? What might make the answer harder to interpret?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_zipcodes }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography=}\StringTok{"zcta"}\NormalTok{, }\AttributeTok{variables =} \StringTok{"B19013\_001"}\NormalTok{, }\AttributeTok{state=}\StringTok{\textquotesingle{}MD\textquotesingle{}}\NormalTok{, }\AttributeTok{year=}\DecValTok{2019}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2015-2019 5-year ACS
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_high\_schools\_with\_acs }\OtherTok{\textless{}{-}}\NormalTok{ md\_high\_schools }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(md\_zipcodes, }\AttributeTok{by=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}zip\textquotesingle{}}\OtherTok{=}\StringTok{\textquotesingle{}GEOID\textquotesingle{}}\NormalTok{))}
\NormalTok{md\_high\_schools\_with\_acs }\OtherTok{\textless{}{-}}\NormalTok{ md\_high\_schools\_with\_acs }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{min=}\NormalTok{(estimate}\SpecialCharTok{{-}}\NormalTok{moe), }\AttributeTok{max=}\NormalTok{(estimate}\SpecialCharTok{+}\NormalTok{moe)) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{drop\_na}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now let's look at the average household income for high schools that
have 60\% or more graduates immediately enrolling in college, and then
look at those with less than 40\%. What do you see here?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_high\_schools\_with\_acs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(percent\_of\_high\_school\_graduates\_enrolled }\SpecialCharTok{\textgreater{}=} \DecValTok{60}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{avg\_income =} \FunctionTok{mean}\NormalTok{(estimate), }\AttributeTok{count =}\FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  avg_income count
       <dbl> <int>
1    111448.    50
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_high\_schools\_with\_acs }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(percent\_of\_high\_school\_graduates\_enrolled }\SpecialCharTok{\textless{}=} \DecValTok{40}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{avg\_income =} \FunctionTok{mean}\NormalTok{(estimate), }\AttributeTok{count =} \FunctionTok{n}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 1 x 2
  avg_income count
       <dbl> <int>
1     69252.    76
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{visualizing-your-data-for-reporting}{%
\chapter{Visualizing your data for
reporting}\label{visualizing-your-data-for-reporting}}

Visualizing data is becoming a much greater part of journalism. Large
news organizations are creating graphics desks that create complex
visuals with data to inform the public about important events.

To do it well is a course on its own. And not every story needs a feat
of programming and art. Sometimes, you can help yourself and your story
by just creating a quick chart, which helps you see patterns in the data
that wouldn't otherwise surface.

Good news: one of the best libraries for visualizing data is in the
tidyverse and it's pretty simple to make simple charts quickly with just
a little bit of code. It's called
\href{https://ggplot2.tidyverse.org/}{ggplot2}.

Let's revisit some data we've used in the past and turn it into charts.
First, let's load libraries. When we load the tidyverse, we get ggplot2,
and we'll need lubridate, too.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

The dataset we'll use is 911 overdose calls from Baltimore County. Let's
load it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/baltco\_911\_calls.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 3822 Columns: 4
-- Column specification --------------------------------------------------------
Delimiter: ","
chr  (1): address
dbl  (1): callnumber
date (1): date
time (1): time

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\hypertarget{bar-charts}{%
\section{Bar charts}\label{bar-charts}}

The first kind of chart we'll create is a simple bar chart.

It's a chart designed to show differences between things -- the
magnitude of one thing, compared to the next thing, and the next, and
the next.

So if we have thing, like a county, or a state, or a group name, and
then a count of that group, we can make a bar chart.

So what does the chart of the top months with the most 911 overdose
calls look like?

First, we'll add a month column to our dataframe using lubridate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\OtherTok{\textless{}{-}}\NormalTok{ baltco\_911\_calls }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{month =} \FunctionTok{month}\NormalTok{(date, }\AttributeTok{label=}\ConstantTok{TRUE}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{group\_by}\NormalTok{(month) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{summarize}\NormalTok{(}\AttributeTok{total\_calls =} \FunctionTok{n}\NormalTok{()) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(total\_calls))}

\NormalTok{baltco\_911\_calls\_by\_month}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 12 x 2
   month total_calls
   <ord>       <int>
 1 Aug           366
 2 May           365
 3 Mar           364
 4 Apr           357
 5 Jul           339
 6 Oct           334
 7 Jun           314
 8 Jan           293
 9 Nov           293
10 Sep           289
11 Dec           271
12 Feb           237
\end{verbatim}

Now let's create a bar chart using ggplot.

With ggplot, the first thing we'll always do is draw a blank canvas that
will house our chart. We start with our dataframe name, and then
(\%\textgreater\%) we invoke the ggplot() function to make that blank
canvas. All this does is make a gray box, the blank canvas that will
hold our chart.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

Next we need to tell ggplot what kind of chart to make.

In ggplot, we work with two key concepts called geometries (abbreviated
frequently as geom) and aesthetics (abbreviated as aes).

Geometries are the shape that the data will take; think of line charts,
bar charts, scatterplots, histograms, pie charts and other common
graphics forms.

Asesthetics help ggplot know what component of our data to visualize --
why we'll visualize values from one column instead of another.

In a bar chart, we first pass in the data to the geometry, then set the
aesthetic.

In the codeblock below, we've added a new function, geom\_bar().

Using geom\_bar() -- as opposed to geom\_line() -- says we're making a
bar chart.

Inside of that function, the asthetic, aes, says which columns to use in
drawing the chart.

We're setting the values on the x axis (horizontal) to be the name of
the county. We set weight to total loans, and it uses that value to
``weight'' or set the height of each bar.

One quirk here with ggplot.

After we've invoked the ggplot() function, you'll notice we're using a +
symbol. It means the same thing as \%\textgreater\% -- ``and then do
this''. It's just a quirk of ggplot() that after you invoke the ggplot()
function, you use + instead of \%\textgreater\%. It makes no sense to me
either, just something to live with.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{month, }\AttributeTok{weight=}\NormalTok{total\_calls))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-5-1.pdf}

}

\end{figure}

This is a very basic chart. But it's hard to derive much meaning from
this chart, because the counties aren't ordered from highest to lowest
by total\_loans. We can fix that by using the reorder() function to do
just that:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(month,total\_calls), }\AttributeTok{weight=}\NormalTok{total\_calls))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-6-1.pdf}

}

\end{figure}

This is a little more useful. But the bottom is kind of a mess, with
overlapping names. We can fix that by flipping it from a vertical bar
chart (also called a column chart) to a horizontal one. coord\_flip()
does that for you.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(month,total\_calls), }\AttributeTok{weight=}\NormalTok{total\_calls)) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

Is this art? No.~Does it quickly tell you something meaningful? It does.

We're mainly going to use these charts to help us in reporting, so style
isn't that important.

But it's worth mentioning that we can pretty up these charts for
publication, if we wanted to, with some more code. To style the chart,
we can change or even modify the ``theme'', a kind of skin that makes
the chart look better.

It's kind of like applying CSS to html. Here I'm changing the theme
slightly to remove the gray background with one of ggplot's built in
themes, theme\_minimal()

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(month,total\_calls), }\AttributeTok{weight=}\NormalTok{total\_calls)) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

The ggplot universe is pretty big, and lots of people have made and
released cool themes for you to use. Want to make your graphics look
kind of like \href{https://www.economist.com/}{The Economist's}
graphics? There's a theme for that.

First, you have to install and load a package that contains lots of
extra themes, called
\href{https://yutannihilation.github.io/allYourFigureAreBelongToUs/ggthemes/}{ggthemes}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#install.packages(\textquotesingle{}ggthemes\textquotesingle{})}
\FunctionTok{library}\NormalTok{(ggthemes)}
\end{Highlighting}
\end{Shaded}

And now we'll apply the economist theme from that package with
theme\_economist()

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(month,total\_calls), }\AttributeTok{weight=}\NormalTok{total\_calls)) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_economist}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

Those axis titles are kind of a mess. Let's change ``count'' on the x
axis to ``net change'' and change ``reorder(County,TOTAL\_DIFF)'' to
``county''. And while we're at it, let's add a basic title and a source
as a caption. We'll use a new function, labs(), which is short for
labels.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_month }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_bar}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\FunctionTok{reorder}\NormalTok{(month,total\_calls), }\AttributeTok{weight=}\NormalTok{total\_calls)) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme\_economist}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{labs}\NormalTok{(}
    \AttributeTok{title=}\StringTok{"More 911 Overdose Calls in Warmer Months"}\NormalTok{,}
    \AttributeTok{x =} \StringTok{"month"}\NormalTok{,}
    \AttributeTok{y =} \StringTok{"total calls"}\NormalTok{,}
    \AttributeTok{caption =} \StringTok{"source: Baltimore County"}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

Viola. Not super pretty, but good enough to show an editor to help them
understand the conclusions you reached with your data analysis.

\hypertarget{line-charts}{%
\section{Line charts}\label{line-charts}}

Let's look at how to make another common chart type that will help you
understand patterns in your data.

Line charts can show change over time. It works much the same as a bar
chart, code wise, but instead of a weight, it uses a y.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_date }\OtherTok{\textless{}{-}}\NormalTok{ baltco\_911\_calls }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(date) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{total\_calls=}\FunctionTok{n}\NormalTok{()}
\NormalTok{  )}

\NormalTok{baltco\_911\_calls\_by\_date}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 366 x 2
   date       total_calls
   <date>           <int>
 1 2022-02-06           8
 2 2022-02-07          15
 3 2022-02-08           7
 4 2022-02-09           5
 5 2022-02-10           5
 6 2022-02-11           8
 7 2022-02-12           9
 8 2022-02-13           7
 9 2022-02-14          11
10 2022-02-15           7
# i 356 more rows
\end{verbatim}

And now let's make a line chart to look for patterns in this data.

We'll put the date on the x axis and total calls on the y axis.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_date }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{total\_calls))}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-13-1.pdf}

}

\end{figure}

It's not super pretty, but there's a bit of a pattern here: the number
of calls fluctuates between 5 and 20 a day for most of this period, and
then jumps way up at certain points during the year. In particular there
are spikes in July, early October and January.

Right now, it's kind of hard to see specifics, though. When did some of
those smaller spikes and troughs happen?

We can't really tell. So let's modify the x axis to have one tick mark
and label per month. We can do that with a function called
scale\_x\_date().

We'll set the date\_breaks to appear for every week; if we wanted every
month, we'd say date\_breaks = ``1 month''. We can set the date to
appear as month abbreviated name (\%b) and day (\%d).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_date }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{total\_calls)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{date\_breaks =} \StringTok{"1 week"}\NormalTok{, }\AttributeTok{date\_labels =} \StringTok{"\%b \%d"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-14-1.pdf}

}

\end{figure}

Those are a little hard to read, so we can turn them 45 degrees to
remove the overlap using the theme() function for styling. With
``axis.text.x = element\_text(angle = 45, hjust=1)'' we're saying, turn
the date labels 45 degrees.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{baltco\_911\_calls\_by\_date }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{date, }\AttributeTok{y=}\NormalTok{total\_calls)) }\SpecialCharTok{+} 
  \FunctionTok{scale\_x\_date}\NormalTok{(}\AttributeTok{date\_breaks =} \StringTok{"1 week"}\NormalTok{, }\AttributeTok{date\_labels =} \StringTok{"\%b \%d"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme}\NormalTok{(}
    \AttributeTok{axis.text.x =} \FunctionTok{element\_text}\NormalTok{(}\AttributeTok{angle =} \DecValTok{45}\NormalTok{,  }\AttributeTok{hjust=}\DecValTok{1}\NormalTok{)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./visualizing-for-reporting_files/figure-pdf/unnamed-chunk-15-1.pdf}

}

\end{figure}

Again, this isn't as pretty as we could make it. But by charting this,
we can quickly see a pattern that can help guide our reporting.

We're just scratching the surface of what ggplot can do, and chart
types. There's so much more you can do, so many other chart types you
can make. But the basics we've shown here will get you started.

\bookmarksetup{startatroot}

\hypertarget{visualizing-your-data-for-publication}{%
\chapter{Visualizing your data for
publication}\label{visualizing-your-data-for-publication}}

Doing data visualization well, and at professional level, takes time,
skill and practice to perfect. Understanding it and doing it at a
complex level is an entire class on it's own. It uses some of the same
skills here -- grouping, filtering, calculating -- but then takes that
data and turns it into data pictures.

But simple stuff -- and even some slightly complicated stuff -- can be
done with tools made for people who aren't data viz pros.

The tool we're going to use is called
\href{https://www.datawrapper.de/}{Datawrapper}.

First, let's get some data and work with it. Let's use a simple CSV of
total votes cast for four Maryland Republicans who ran for statewide
office in the 2022 general election. Let's look at it.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_gop\_cands }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/md\_gop\_cands.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(md\_gop\_cands)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 4 x 5
  Candidate        `Early Voting` `Election Day`  Mail  Votes
  <chr>                     <dbl>          <dbl> <dbl>  <dbl>
1 Michael Peroutka         131551         451685 24669 606557
2 Dan Cox                  123287         424837 22112 569450
3 Barry Glassman           142726         484892 28656 655379
4 Chris Chaffee            130458         447330 23561 600543
\end{verbatim}

\hypertarget{datawrapper}{%
\section{Datawrapper}\label{datawrapper}}

Making charts in Datawrapper is preposterously simple, which is the
point. There are dozens of chart types, and dozens of options. To get
from a csv to a chart to publication is very, very easy.

First, go to \href{https://www.datawrapper.de/}{datawrapper.de} and sign
up for an account. It's free.

Once logged in, you'll click on New Chart.

\includegraphics[width=4.51in,height=\textheight]{./images/datawrapper1.png}

The first thing we'll do is upload our CSV. Click on XLS/CSV and upload
the file.

\includegraphics[width=6.75in,height=\textheight]{./images/datawrapper2.png}

Next up is to check and see what Datawrappper did with our data when we
uploaded it. As you can see from the text on the left, if it's blue,
it's a number. If it's green, it's a date. If it's black, it's text. Red
means there's a problem. This data is very clean, so it imports cleanly.
Click on the ``Proceed'' button.

\includegraphics[width=3.41in,height=\textheight]{./images/datawrapper3.png}

Now we make a chart. Bar chart comes up by default, which is good,
because with totals, that's what we have.

Click on Refine. The first option we want to change is column we're
using for the bars, which defaults to ``Early Voting''. Let's make it
``Votes. Let's also choose to sort the bars so that the largest value
(and bar appears first). We do that by clicking on the''Sort bars''
button.

\includegraphics[width=1.92in,height=\textheight]{./images/datawrapper4.png}

Now we need to annotate our charts. Every chart needs a title, a source
line and a credit line. Most need chatter (called description here).
Click on the ``Annotate'' tab to get started.

Really think about the title and description: the title is like a
headline and the description is provides some additional context.
Another way to think about it: the title is the most important lesson
from the graphic, and the description could be the next most important
lesson or could provide more context to the title.

\includegraphics[width=2.86in,height=\textheight]{./images/datawrapper5.png}

To publish, we click the ``Publish \& Embed'' tab. Some publication
systems allow for the embedding of HTML into a post or a story. Some
don't. The only way to know is to ask someone at your publication. Every
publication system on the planet, though, can publish an image. So
there's always a way to export your chart as a PNG file, which you can
upload like any photo.

\includegraphics[width=4.04in,height=\textheight]{./images/datawrapper6.png}

\hypertarget{making-a-map}{%
\subsection{Making a Map}\label{making-a-map}}

Let's create a choropleth map - one that shows variations between the
percentage of votes received by Wes Moore across Maryland counties.
We'll read that in from the data folder.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_gov\_county }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/md\_gov\_county.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In order to make a map, we need to be able to tell Datawrapper that a
certain column contains geographic information (besides the name of the
county). The easiest way to do that for U.S. maps is to use something
called a
\href{https://www.census.gov/programs-surveys/geography/guidance/geo-identifiers.html}{FIPS
Code}. You should read about them so you understand what they are, and
think of them as a unique identifier for some geographical entity like a
state or county. Our md\_gov\_county dataframe has a FIPS code for each
county, but if you ever need one for a county, this is a solved problem
thanks to the Tigris library that we used in pre\_lab 9.

We'll need to write code to add columns showing the total number of
votes for each county and the percentage of votes received by Wes Moore
in each county, then replace the CSV file in the data folder with it

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_gov\_county }\OtherTok{\textless{}{-}}\NormalTok{ md\_gov\_county }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{total =}\NormalTok{ cox }\SpecialCharTok{+}\NormalTok{ moore }\SpecialCharTok{+}\NormalTok{ lashar }\SpecialCharTok{+}\NormalTok{ wallace }\SpecialCharTok{+}\NormalTok{ harding }\SpecialCharTok{+}\NormalTok{ write\_ins) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{pct\_moore =}\NormalTok{ moore}\SpecialCharTok{/}\NormalTok{total }\SpecialCharTok{*} \DecValTok{100}\NormalTok{)}

\FunctionTok{write\_csv}\NormalTok{(md\_gov\_county, }\StringTok{"data/md\_gov\_county\_with\_percent.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Go back to Datawrapper and click on ``New Map''. Click on ``Choropleth
map'' and then choose ``USA \textgreater\textgreater{} Counties (2022)''
for the map base and click the Proceed button.

Now we can upload the \texttt{md\_gov\_county\_with\_percent.csv} file
we just saved using the Upload File button. It should look like the
following image:

\includegraphics[width=4.5in,height=\textheight]{./images/datawrapper7.png}

We'll need to make sure that Datawrapper understands what the data is
and where the FIPS code is. Click on the ``Match'' tab and make sure
that yours looks like the image below:

\includegraphics[width=1.83in,height=\textheight]{./images/datawrapper8.png}

Click the ``Proceed'' button (you should have to click it twice, since
the first time it will tell you that there's no data for 3,197 counties
- the rest of the U.S.). That will take you to the Visualize tab.

You'll see that the map currently is of the whole nation, and we only
have Maryland data. Let's fix that.

Look for ``Hide regions without data'' under Appearance, and click the
slider icon to enable that feature. You should see a map zoomed into
Maryland with some counties in various colors.

But it's a little rough visually, so let's clean that up.

Look for the ``Show color legend'' label and add a caption for the
legend, which is the horizontal bar under the title. Then click on the
``Annotate'' tab to add a title, description, data source and byline.
The title should represent the headline, while the description should be
a longer phrase that tells people what they are looking at.

That's better, but check out the tooltip by hovering over a county. It's
not super helpful. Let's change the tooltip behavior to show the county
name and a better-formatted number.

Click the ``Customize tooltips'' button so it expands down. Change \{\{
fips\_code \}\} to \{\{ county \}\} and \{\{ pctmoore \}\} to \{\{
FORMAT(pctmoore, ``00.0\%'')\}\}

Experiment with the ``Show labels'' options to see if you can add county
labels to your map.

Ok, that looks better. Let's publish!

Click the ``Proceed'' button until you get to the ``Publish \& Embed''
tab, then click ``Publish Now''.

\bookmarksetup{startatroot}

\hypertarget{geographic-data-basics}{%
\chapter{Geographic data basics}\label{geographic-data-basics}}

Up to now, we've been looking at patterns in data for what is more than
this, or what's the middle look like. We've calculated metrics like
percentages, or looked at how data changes over time.

Another way we can look at the data is geographically. Is there a
spatial pattern to our data? Can we learn anything by using distance as
a metric? What if we merge non-geographic data into geographic data?

The bad news is that there isn't a One Library To Rule Them All when it
comes to geo queries in R. But there's one emerging, called Simple
Features, that is very good.

Go to the console and install it with \texttt{install.packages("sf")}

To understand geographic queries, you have to get a few things in your
head first:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Your query is using planar space. Usually that's some kind of
  projection of the world. If you're lucky, your data is projected, and
  the software will handle projection differences under the hood without
  you knowing anything about it.
\item
  Projections are cartographers making opinionated decisions about what
  the world should look like when you take a spheroid -- the earth isn't
  perfectly round -- and flatten it. Believe it or not, every state in
  the US has their own geographic projection. There's dozens upon dozens
  of them.
\item
  Geographic queries work in layers. In most geographic applications,
  you'll have multiple layers. You'll have a boundary file, and a river
  file, and a road file, and a flood file and combined together they
  make the map. But you have to think in layers.
\item
  See 1. With layers, they're all joined together by the planar space.
  So you don't need to join one to the other like we did earlier -- the
  space has done that. So you can query how many X are within the
  boundaries on layer Y. And it's the plane that holds them together.
\end{enumerate}

\includegraphics[width=3.41in,height=\textheight]{./images/geolayers.jpg}

\hypertarget{importing-and-viewing-data}{%
\section{Importing and viewing data}\label{importing-and-viewing-data}}

Let's start with the absolute basics of geographic data: loading and
viewing. Load libraries as usual.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

First: an aside on geographic data. There are many formats for
geographic data, but data type you'll see the most is called the
shapefile. It comes from a company named ERSI, which created the most
widely used GIS software in the world. For years, they were the only
game in town, really, and the shapefile became ubiquitous, especially so
in government and utilities.

More often than not, you'll be dealing with a shapefile. But a shapefile
isn't just a single file -- it's a collection of files that combined
make up all the data that allow you to use it. There's a .shp file --
that's the main file that pulls it all together -- but it's important to
note if your shapefiles has a .prj file, which indicates that the
projection is specified. You also might be working with a GeoDatabase,
or a .gdb file. That's a slightly different, more compact version of a
Shapefile.

The data we're going to be working with is a Shapefile of Maryland zip
codes from the
\href{https://data.imap.maryland.gov/datasets/38f3f8bc61bb4261b59d71b3642e3cd6_3/explore?location=38.810313\%2C-77.268250\%2C8.83}{state's
GIS data catalog}.

Similar to \texttt{readr}, the \texttt{sf} library has functions to read
geographic data. In this case, we're going to use \texttt{st\_read} to
read in our hospitals data. And then glimpse it to look at the columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_zips }\OtherTok{\textless{}{-}} \FunctionTok{st\_read}\NormalTok{(}\StringTok{"data/md\_zips/BNDY\_ZIPCodes11Digit\_MDP.shp"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Reading layer `BNDY_ZIPCodes11Digit_MDP' from data source 
  `/Users/dpwillis/code/datajournalismbook-maryland/data/md_zips/BNDY_ZIPCodes11Digit_MDP.shp' 
  using driver `ESRI Shapefile'
Simple feature collection with 526 features and 7 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -8848487 ymin: 4567000 xmax: -8354398 ymax: 4825836
Projected CRS: WGS 84 / Pseudo-Mercator
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{glimpse}\NormalTok{(md\_zips)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 526
Columns: 8
$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, ~
$ ZIPCODE2   <chr> "02400121502", "02400121521", "02400121530", "02400121532",~
$ ZIPNAME    <chr> "Cumberland", "Barton", "Flintstone", "Frostburg", "Lonacon~
$ EXISTING   <chr> "MDPV2017_18", "MDPV2017_18", "MDPV2017_18", "MDPV2017_18",~
$ ZIPCODE1   <chr> "21502", "21521", "21530", "21532", "21539", "21540", "2154~
$ Shape_Leng <dbl> 161775.468, 29199.174, 93603.136, 58275.951, 35065.607, 397~
$ Shape_Area <dbl> 441770403, 38842463, 267606761, 188918096, 65669461, 108639~
$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((-8755679 48..., MULTIPOLYGON (~
\end{verbatim}

This looks like a normal dataframe, and mostly it is. We have one row
per zipcode, and each column is some feature of that zip code: the
fipscode, name and more. What sets this data apart from other dataframes
we've used is the last column, ``geometry'', which is of a new data
type. It's not a character or a number, it's a ``Multipolygon'', which
is composed of multiple longitude and latitude values. When we plot
these on a grid of latitude and longitude, it will draw those shapes on
a map.

Let's look at these zip codes We have 526 of them, according to this
data.

But where in Maryland are these places? We can simply plot them on a
longitude-latitude grid using ggplot and geom\_sf.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_zips }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographic-basics_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

Each shape is a zip code, with the boundaries plotted according to its
degrees of longitude and latitude.

If you know anything about Maryland, you can pick out the geographic
context here. You can basically see where Baltimore is and where the
borders of the District of Columbia touch Maryland. But this map is not
exactly ideal. It would help to have a county map layered underneath of
it, to help make sense of the spatial nature of this data.

This is where layering becomes more clear. First, we want to go out and
get another shapefile, this one showing Maryland county outlines.

Instead of loading it from our local machine, like we did above, we're
going to use a package to directly download it from the U.S. Census. The
package is called \texttt{tigris} and it's developed by the same person
who made \texttt{tidycensus}.

In the console, install tigris with
\texttt{install.packages(\textquotesingle{}tigris\textquotesingle{})}

Then load it:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tigris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
To enable caching of data, set `options(tigris_use_cache = TRUE)`
in your R script or .Rprofile.
\end{verbatim}

Now, let's use the counties() function from tigris to pull down a
shapefile of all U.S. counties.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{counties }\OtherTok{\textless{}{-}} \FunctionTok{counties}\NormalTok{()}

\FunctionTok{glimpse}\NormalTok{(counties)}
\end{Highlighting}
\end{Shaded}

This looks pretty similar to our Census blocks shapefile, in that it
looked mostly like a normal dataframe with the exception of the new
geometry column.

This county shapefile has all 3233 U.S. counties. We only want the
Maryland counties, so we're going to filter the data to only keep
Maryland counties. There is no STATE column, but there is a STATEFP
column, with each number representing a state. Maryland's FP number is
24.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_counties }\OtherTok{\textless{}{-}}\NormalTok{ counties }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(STATEFP }\SpecialCharTok{==} \StringTok{"24"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

To see what this looks like, let's plot it out with ggplot. We can
pretty clearly see the shapes of Maryland counties.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_counties }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographic-basics_files/figure-pdf/unnamed-chunk-8-1.pdf}

}

\end{figure}

With this county map, we can layer our places data.

Something to note: The layers are rendered in the order they appear. So
the first geom\_sf is rendered first. The second geom\_sf is rendered ON
TOP OF the first one.

We're also going to change things up a bit to put the datasets we want
to display INSIDE of the geom\_sf() function, instead of starting with a
dataframe. We have two to plot now, so it's easier this way.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data=}\NormalTok{md\_counties) }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data=}\NormalTok{md\_zips) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographic-basics_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

Notice the subtle differences at the boundaries?

Let's dive back into zip codes and this time bring along data on notices
of intent to foreclosure by zip code (found at
https://opendata.maryland.gov/Housing/Maryland-Notices-of-Intent-to-Foreclose-by-Zip-Cod/ftsr-vapt).
Where are these foreclosure notices appearing?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foreclosure\_zip }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/Maryland\_Notices\_of\_Intent\_to\_Foreclose\_by\_Zip\_Code.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{clean\_names}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 547 Columns: 22
-- Column specification --------------------------------------------------------
Delimiter: ","
chr  (1): Zip
dbl (21): July 2021, August 2021, September 2021, October 2021, November 202...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{foreclosure\_zip\_march\_2023 }\OtherTok{\textless{}{-}}\NormalTok{ foreclosure\_zip }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{select}\NormalTok{(zip, march\_2023)}

\FunctionTok{View}\NormalTok{(foreclosure\_zip\_march\_2023)}
\end{Highlighting}
\end{Shaded}

Now we can join the zip codes to the foreclosure data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{zip\_codes\_with\_foreclosures }\OtherTok{\textless{}{-}}\NormalTok{ md\_zips }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{left\_join}\NormalTok{(foreclosure\_zip\_march\_2023, }\FunctionTok{join\_by}\NormalTok{(ZIPCODE1}\SpecialCharTok{==}\NormalTok{zip))}
\end{Highlighting}
\end{Shaded}

Now we can use color to distinguish zip codes from each other. Let's use
the number of foreclosure notices to start with:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_centroids }\OtherTok{\textless{}{-}} \FunctionTok{st\_centroid}\NormalTok{(zip\_codes\_with\_foreclosures)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in st_centroid.sf(zip_codes_with_foreclosures): st_centroid assumes
attributes are constant over geometries of x
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data=}\NormalTok{zip\_codes\_with\_foreclosures, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{march\_2023)) }\SpecialCharTok{+}
  \FunctionTok{scale\_colour\_viridis\_b}\NormalTok{(}\AttributeTok{option=}\StringTok{"magma"}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographic-basics_files/figure-pdf/unnamed-chunk-12-1.pdf}

}

\end{figure}

With these changes, what can we make out here? First, you can pretty
easily spot zip codes with a high number of foreclosures - in Prince
George's County in particular, along with zip codes around Baltimore and
in places in the northeast and northwest parts of the state.

\bookmarksetup{startatroot}

\hypertarget{geographic-analysis}{%
\chapter{Geographic analysis}\label{geographic-analysis}}

In the previous chapter, we looked at election precincts in Prince
George's County to show a bit of a pattern regarding concentration of
the precincts with the most and 0 eligible voters. Let's go little
further and look at voters statewide.

First, let's load the libraries we'll need, including tigris. We're also
going to load tidycensus and set an API key for tidycensus.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(sf)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(tigris)}
\FunctionTok{library}\NormalTok{(tidycensus)}
\FunctionTok{census\_api\_key}\NormalTok{(}\StringTok{"549950d36c22ff16455fe196bbbd01d63cfbe6cf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the previous chapter, we looked at foreclosure notices by zip codes
to find patterns in Maryland. Let's go further and, instead of using raw
numbers, use percentages based on the number of owner-occupied housing
units in each zip code.

For the rest of this chapter, we're going to work on building a map that
will help us gain insight into geographic patterns in foreclosure
notices by county in Maryland. What geographic patterns can we identify?

First, we'll go out and get the county foreclosure notices and
population data for each county using tidycensus. The variable for total
population is B01001\_001

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_notices }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/Maryland\_Foreclosure\_Data\_by\_County.csv"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{slice}\NormalTok{(}\DecValTok{1}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{pivot\_longer}\NormalTok{(}\AttributeTok{cols=}\SpecialCharTok{{-}}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}Date\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}Type\textquotesingle{}}\NormalTok{), }\AttributeTok{names\_to=}\StringTok{\textquotesingle{}county\textquotesingle{}}\NormalTok{, }\AttributeTok{values\_to =} \StringTok{\textquotesingle{}notices\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 63 Columns: 27
-- Column specification --------------------------------------------------------
Delimiter: ","
chr  (2): Date, Type
dbl (25): Allegany County, Anne Arundel County, Baltimore City, Baltimore Co...

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_population }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography =} \StringTok{"county"}\NormalTok{,}
              \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\AttributeTok{population =} \StringTok{"B01001\_001"}\NormalTok{),}
              \AttributeTok{year =} \DecValTok{2021}\NormalTok{,}
              \AttributeTok{state =} \StringTok{"MD"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2017-2021 5-year ACS
\end{verbatim}

Ultimately, we're going to join this county table with population with
foreclosure notices by county, and then calculate a percentage. But
remember, we then want to visualize this data by drawing a zip code map
that helps us pick out trends. Thinking ahead, we know we'll need a zip
code map shapefile. Fortunately, we can pull this geometry information
right from tidycensus at the same time that we pull in the population
data by adding ``geometry = TRUE'' to our get\_acs function.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_population }\OtherTok{\textless{}{-}} \FunctionTok{get\_acs}\NormalTok{(}\AttributeTok{geography =} \StringTok{"county"}\NormalTok{,}
              \AttributeTok{variables =} \FunctionTok{c}\NormalTok{(}\AttributeTok{population =} \StringTok{"B01001\_001"}\NormalTok{),}
              \AttributeTok{year =} \DecValTok{2021}\NormalTok{,}
              \AttributeTok{state =} \StringTok{"MD"}\NormalTok{,}
              \AttributeTok{geometry =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Getting data from the 2017-2021 5-year ACS
\end{verbatim}

\begin{verbatim}
Downloading feature geometry from the Census website.  To cache shapefiles for use in future sessions, set `options(tigris_use_cache = TRUE)`.
\end{verbatim}

\begin{verbatim}

  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |                                                                      |   1%
  |                                                                            
  |=                                                                     |   1%
  |                                                                            
  |=                                                                     |   2%
  |                                                                            
  |==                                                                    |   2%
  |                                                                            
  |==                                                                    |   3%
  |                                                                            
  |===                                                                   |   4%
  |                                                                            
  |===                                                                   |   5%
  |                                                                            
  |====                                                                  |   5%
  |                                                                            
  |====                                                                  |   6%
  |                                                                            
  |=====                                                                 |   6%
  |                                                                            
  |=====                                                                 |   7%
  |                                                                            
  |=====                                                                 |   8%
  |                                                                            
  |======                                                                |   8%
  |                                                                            
  |======                                                                |   9%
  |                                                                            
  |=======                                                               |   9%
  |                                                                            
  |=======                                                               |  10%
  |                                                                            
  |========                                                              |  11%
  |                                                                            
  |========                                                              |  12%
  |                                                                            
  |=========                                                             |  12%
  |                                                                            
  |=========                                                             |  13%
  |                                                                            
  |=========                                                             |  14%
  |                                                                            
  |==========                                                            |  14%
  |                                                                            
  |==========                                                            |  15%
  |                                                                            
  |===========                                                           |  15%
  |                                                                            
  |===========                                                           |  16%
  |                                                                            
  |============                                                          |  17%
  |                                                                            
  |============                                                          |  18%
  |                                                                            
  |=============                                                         |  18%
  |                                                                            
  |=============                                                         |  19%
  |                                                                            
  |==============                                                        |  19%
  |                                                                            
  |==============                                                        |  20%
  |                                                                            
  |==============                                                        |  21%
  |                                                                            
  |===============                                                       |  21%
  |                                                                            
  |===============                                                       |  22%
  |                                                                            
  |================                                                      |  22%
  |                                                                            
  |================                                                      |  23%
  |                                                                            
  |=================                                                     |  24%
  |                                                                            
  |=================                                                     |  25%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |==================                                                    |  26%
  |                                                                            
  |===================                                                   |  27%
  |                                                                            
  |===================                                                   |  28%
  |                                                                            
  |====================                                                  |  28%
  |                                                                            
  |====================                                                  |  29%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |=====================                                                 |  31%
  |                                                                            
  |======================                                                |  31%
  |                                                                            
  |=======================                                               |  33%
  |                                                                            
  |========================                                              |  34%
  |                                                                            
  |========================                                              |  35%
  |                                                                            
  |=========================                                             |  36%
  |                                                                            
  |==========================                                            |  36%
  |                                                                            
  |===========================                                           |  38%
  |                                                                            
  |===========================                                           |  39%
  |                                                                            
  |============================                                          |  40%
  |                                                                            
  |=====================================                                 |  53%
  |                                                                            
  |======================================                                |  54%
  |                                                                            
  |======================================                                |  55%
  |                                                                            
  |=======================================                               |  56%
  |                                                                            
  |========================================                              |  57%
  |                                                                            
  |=========================================                             |  58%
  |                                                                            
  |=========================================                             |  59%
  |                                                                            
  |==========================================                            |  60%
  |                                                                            
  |===========================================                           |  61%
  |                                                                            
  |===========================================                           |  62%
  |                                                                            
  |============================================                          |  62%
  |                                                                            
  |============================================                          |  63%
  |                                                                            
  |=============================================                         |  64%
  |                                                                            
  |=============================================                         |  65%
  |                                                                            
  |==============================================                        |  65%
  |                                                                            
  |==============================================                        |  66%
  |                                                                            
  |===============================================                       |  67%
  |                                                                            
  |===============================================                       |  68%
  |                                                                            
  |================================================                      |  68%
  |                                                                            
  |================================================                      |  69%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |=================================================                     |  71%
  |                                                                            
  |==================================================                    |  72%
  |                                                                            
  |===================================================                   |  73%
  |                                                                            
  |====================================================                  |  74%
  |                                                                            
  |====================================================                  |  75%
  |                                                                            
  |======================================================                |  77%
  |                                                                            
  |=======================================================               |  78%
  |                                                                            
  |=======================================================               |  79%
  |                                                                            
  |=========================================================             |  81%
  |                                                                            
  |=========================================================             |  82%
  |                                                                            
  |==========================================================            |  82%
  |                                                                            
  |==========================================================            |  83%
  |                                                                            
  |==========================================================            |  84%
  |                                                                            
  |===========================================================           |  84%
  |                                                                            
  |===========================================================           |  85%
  |                                                                            
  |=============================================================         |  87%
  |                                                                            
  |==============================================================        |  88%
  |                                                                            
  |==============================================================        |  89%
  |                                                                            
  |===================================================================   |  96%
  |                                                                            
  |====================================================================  |  97%
  |                                                                            
  |====================================================================  |  98%
  |                                                                            
  |===================================================================== |  98%
  |                                                                            
  |===================================================================== |  99%
  |                                                                            
  |======================================================================| 100%
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 24 features and 5 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -79.48765 ymin: 37.91172 xmax: -75.04894 ymax: 39.72304
Geodetic CRS:  NAD83
First 10 features:
   GEOID                             NAME   variable estimate moe
1  24047       Worcester County, Maryland population    52322  NA
2  24003    Anne Arundel County, Maryland population   584064  NA
3  24033 Prince George's County, Maryland population   957767  NA
4  24025         Harford County, Maryland population   259162  NA
5  24015           Cecil County, Maryland population   103370  NA
6  24011        Caroline County, Maryland population    33234  NA
7  24023         Garrett County, Maryland population    28955  NA
8  24029            Kent County, Maryland population    19335  NA
9  24041          Talbot County, Maryland population    37510  NA
10 24045        Wicomico County, Maryland population   103223  NA
                         geometry
1  MULTIPOLYGON (((-75.66061 3...
2  MULTIPOLYGON (((-76.83849 3...
3  MULTIPOLYGON (((-77.07995 3...
4  MULTIPOLYGON (((-76.0921 39...
5  MULTIPOLYGON (((-76.23326 3...
6  MULTIPOLYGON (((-76.01505 3...
7  MULTIPOLYGON (((-79.48765 3...
8  MULTIPOLYGON (((-76.27737 3...
9  MULTIPOLYGON (((-76.34647 3...
10 MULTIPOLYGON (((-75.92033 3...
\end{verbatim}

We now have a new column, geometry, that contains the ``MULTIPOLYGON''
data that will draw an outline of each county when we go to draw a map.

The next step will be to join our population data to our foreclosure
data on the county column.

But there's a problem. The column in our population data that has county
names is called ``NAME'', and it has the full name of the county spelled
out in title case -- first word capitalized and has ``County'' and
``Maryland'' in it. The foreclosure data just has the name of the
county. For example, the population data has ``Anne Arundel County,
Maryland'' and the foreclosure data has ``Anne Arundel County''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 24 features and 5 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -79.48765 ymin: 37.91172 xmax: -75.04894 ymax: 39.72304
Geodetic CRS:  NAD83
First 10 features:
   GEOID                             NAME   variable estimate moe
1  24047       Worcester County, Maryland population    52322  NA
2  24003    Anne Arundel County, Maryland population   584064  NA
3  24033 Prince George's County, Maryland population   957767  NA
4  24025         Harford County, Maryland population   259162  NA
5  24015           Cecil County, Maryland population   103370  NA
6  24011        Caroline County, Maryland population    33234  NA
7  24023         Garrett County, Maryland population    28955  NA
8  24029            Kent County, Maryland population    19335  NA
9  24041          Talbot County, Maryland population    37510  NA
10 24045        Wicomico County, Maryland population   103223  NA
                         geometry
1  MULTIPOLYGON (((-75.66061 3...
2  MULTIPOLYGON (((-76.83849 3...
3  MULTIPOLYGON (((-77.07995 3...
4  MULTIPOLYGON (((-76.0921 39...
5  MULTIPOLYGON (((-76.23326 3...
6  MULTIPOLYGON (((-76.01505 3...
7  MULTIPOLYGON (((-79.48765 3...
8  MULTIPOLYGON (((-76.27737 3...
9  MULTIPOLYGON (((-76.34647 3...
10 MULTIPOLYGON (((-75.92033 3...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_notices}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 25 x 4
   Date                   Type                          county           notices
   <chr>                  <chr>                         <chr>              <dbl>
 1 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Allegany County       79
 2 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Anne Arundel Co~     475
 3 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Baltimore City       754
 4 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Baltimore County     847
 5 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Calvert County       116
 6 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Caroline County       40
 7 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Carroll County       133
 8 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Cecil County         109
 9 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Charles County       336
10 03/01/2023 12:00:00 AM Notice of Intent to Foreclose Dorchester Coun~      41
# i 15 more rows
\end{verbatim}

If they're going to join properly, we need to clean one of them up to
make it match the other.

Let's clean the population table. We're going to rename the ``NAME''
column to ``County'', then remove ``, Maryland'' and ``County'' and make
the county titlecase. Next we'll remove any white spaces after that
first cleaning step that, if left in, would prevent a proper join. We're
also going to rename the column that contains the population information
from ``estimate'' to ``population'' and select only the county name and
the population columns, along with the geometry. That leaves us with
this tidy table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_county\_population }\OtherTok{\textless{}{-}}\NormalTok{ md\_county\_population }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{county =}\NormalTok{ NAME) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{county =} \FunctionTok{str\_to\_title}\NormalTok{(}\FunctionTok{str\_remove\_all}\NormalTok{(county,}\StringTok{", Maryland"}\NormalTok{))) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{county =} \FunctionTok{str\_trim}\NormalTok{(county,}\AttributeTok{side=}\StringTok{"both"}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{population =}\NormalTok{ estimate) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(county, population, geometry)}

\NormalTok{md\_county\_population}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 24 features and 2 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -79.48765 ymin: 37.91172 xmax: -75.04894 ymax: 39.72304
Geodetic CRS:  NAD83
First 10 features:
                   county population                       geometry
1        Worcester County      52322 MULTIPOLYGON (((-75.66061 3...
2     Anne Arundel County     584064 MULTIPOLYGON (((-76.83849 3...
3  Prince George's County     957767 MULTIPOLYGON (((-77.07995 3...
4          Harford County     259162 MULTIPOLYGON (((-76.0921 39...
5            Cecil County     103370 MULTIPOLYGON (((-76.23326 3...
6         Caroline County      33234 MULTIPOLYGON (((-76.01505 3...
7          Garrett County      28955 MULTIPOLYGON (((-79.48765 3...
8             Kent County      19335 MULTIPOLYGON (((-76.27737 3...
9           Talbot County      37510 MULTIPOLYGON (((-76.34647 3...
10        Wicomico County     103223 MULTIPOLYGON (((-75.92033 3...
\end{verbatim}

Now we can join them.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_pop\_with\_foreclosures }\OtherTok{\textless{}{-}}\NormalTok{ md\_county\_population }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(md\_county\_notices, }\FunctionTok{join\_by}\NormalTok{(county))}

\NormalTok{md\_pop\_with\_foreclosures}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 24 features and 5 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -79.48765 ymin: 37.91172 xmax: -75.04894 ymax: 39.72304
Geodetic CRS:  NAD83
First 10 features:
                   county population                   Date
1        Worcester County      52322 03/01/2023 12:00:00 AM
2     Anne Arundel County     584064 03/01/2023 12:00:00 AM
3  Prince George's County     957767 03/01/2023 12:00:00 AM
4          Harford County     259162 03/01/2023 12:00:00 AM
5            Cecil County     103370 03/01/2023 12:00:00 AM
6         Caroline County      33234 03/01/2023 12:00:00 AM
7          Garrett County      28955 03/01/2023 12:00:00 AM
8             Kent County      19335 03/01/2023 12:00:00 AM
9           Talbot County      37510 03/01/2023 12:00:00 AM
10        Wicomico County     103223 03/01/2023 12:00:00 AM
                            Type notices                       geometry
1  Notice of Intent to Foreclose      47 MULTIPOLYGON (((-75.66061 3...
2  Notice of Intent to Foreclose     475 MULTIPOLYGON (((-76.83849 3...
3  Notice of Intent to Foreclose    1389 MULTIPOLYGON (((-77.07995 3...
4  Notice of Intent to Foreclose     250 MULTIPOLYGON (((-76.0921 39...
5  Notice of Intent to Foreclose     109 MULTIPOLYGON (((-76.23326 3...
6  Notice of Intent to Foreclose      40 MULTIPOLYGON (((-76.01505 3...
7  Notice of Intent to Foreclose      13 MULTIPOLYGON (((-79.48765 3...
8  Notice of Intent to Foreclose      19 MULTIPOLYGON (((-76.27737 3...
9  Notice of Intent to Foreclose      22 MULTIPOLYGON (((-76.34647 3...
10 Notice of Intent to Foreclose      68 MULTIPOLYGON (((-75.92033 3...
\end{verbatim}

Our final step before visualization, let's calculate the number of
foreclosure notices per 1000 population and sort from highest to lowest
to see what trends we can identify just from the table.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_pop\_with\_foreclosures }\OtherTok{\textless{}{-}}\NormalTok{ md\_county\_population }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{left\_join}\NormalTok{(md\_county\_notices, }\FunctionTok{join\_by}\NormalTok{(county)) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{rate =}\NormalTok{ notices}\SpecialCharTok{/}\NormalTok{population}\SpecialCharTok{*}\DecValTok{1000}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{arrange}\NormalTok{(}\FunctionTok{desc}\NormalTok{(rate))}

\NormalTok{md\_pop\_with\_foreclosures}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Simple feature collection with 24 features and 6 fields
Geometry type: MULTIPOLYGON
Dimension:     XY
Bounding box:  xmin: -79.48765 ymin: 37.91172 xmax: -75.04894 ymax: 39.72304
Geodetic CRS:  NAD83
First 10 features:
                   county population                   Date
1          Charles County     165209 03/01/2023 12:00:00 AM
2  Prince George's County     957767 03/01/2023 12:00:00 AM
3          Baltimore City     592211 03/01/2023 12:00:00 AM
4       Dorchester County      32486 03/01/2023 12:00:00 AM
5          Calvert County      92515 03/01/2023 12:00:00 AM
6         Caroline County      33234 03/01/2023 12:00:00 AM
7         Allegany County      68684 03/01/2023 12:00:00 AM
8            Cecil County     103370 03/01/2023 12:00:00 AM
9       St. Mary's County     113209 03/01/2023 12:00:00 AM
10       Baltimore County     850702 03/01/2023 12:00:00 AM
                            Type notices      rate
1  Notice of Intent to Foreclose     336 2.0337875
2  Notice of Intent to Foreclose    1389 1.4502483
3  Notice of Intent to Foreclose     754 1.2731949
4  Notice of Intent to Foreclose      41 1.2620821
5  Notice of Intent to Foreclose     116 1.2538507
6  Notice of Intent to Foreclose      40 1.2035867
7  Notice of Intent to Foreclose      79 1.1501951
8  Notice of Intent to Foreclose     109 1.0544645
9  Notice of Intent to Foreclose     113 0.9981539
10 Notice of Intent to Foreclose     847 0.9956483
                         geometry
1  MULTIPOLYGON (((-77.27382 3...
2  MULTIPOLYGON (((-77.07995 3...
3  MULTIPOLYGON (((-76.71152 3...
4  MULTIPOLYGON (((-76.06544 3...
5  MULTIPOLYGON (((-76.70121 3...
6  MULTIPOLYGON (((-76.01505 3...
7  MULTIPOLYGON (((-79.06756 3...
8  MULTIPOLYGON (((-76.23326 3...
9  MULTIPOLYGON (((-76.74729 3...
10 MULTIPOLYGON (((-76.3257 39...
\end{verbatim}

Let's take a look at the result of this table. The variances in the
rates aren't huge, but there are some clear differences: Charles County
and Prince George's County have higher rates, followed by Baltimore City
and some more rural counties.

First, let's use the counties() function from tigris to pull down a
shapefile of all U.S. counties and grab the ones for Maryland.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{md\_counties }\OtherTok{\textless{}{-}} \FunctionTok{counties}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(STATEFP }\SpecialCharTok{==} \StringTok{"24"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Retrieving data for the year 2020
\end{verbatim}

\begin{verbatim}

  |                                                                            
  |                                                                      |   0%
  |                                                                            
  |=                                                                     |   1%
  |                                                                            
  |==                                                                    |   4%
  |                                                                            
  |====                                                                  |   6%
  |                                                                            
  |=====                                                                 |   7%
  |                                                                            
  |======                                                                |   9%
  |                                                                            
  |========                                                              |  11%
  |                                                                            
  |========                                                              |  12%
  |                                                                            
  |==========                                                            |  14%
  |                                                                            
  |==========                                                            |  15%
  |                                                                            
  |===========                                                           |  15%
  |                                                                            
  |===========                                                           |  16%
  |                                                                            
  |============                                                          |  16%
  |                                                                            
  |============                                                          |  17%
  |                                                                            
  |============                                                          |  18%
  |                                                                            
  |=============                                                         |  18%
  |                                                                            
  |=============                                                         |  19%
  |                                                                            
  |==============                                                        |  19%
  |                                                                            
  |==============                                                        |  20%
  |                                                                            
  |===============                                                       |  22%
  |                                                                            
  |=================                                                     |  24%
  |                                                                            
  |=================                                                     |  25%
  |                                                                            
  |==================                                                    |  25%
  |                                                                            
  |===================                                                   |  27%
  |                                                                            
  |====================                                                  |  29%
  |                                                                            
  |=====================                                                 |  30%
  |                                                                            
  |=======================                                               |  32%
  |                                                                            
  |========================                                              |  34%
  |                                                                            
  |==========================                                            |  36%
  |                                                                            
  |==========================                                            |  37%
  |                                                                            
  |==========================                                            |  38%
  |                                                                            
  |===========================                                           |  38%
  |                                                                            
  |===========================                                           |  39%
  |                                                                            
  |=============================                                         |  41%
  |                                                                            
  |==============================                                        |  43%
  |                                                                            
  |================================                                      |  45%
  |                                                                            
  |================================                                      |  46%
  |                                                                            
  |==================================                                    |  48%
  |                                                                            
  |==================================                                    |  49%
  |                                                                            
  |===================================                                   |  50%
  |                                                                            
  |===================================                                   |  51%
  |                                                                            
  |====================================                                  |  51%
  |                                                                            
  |=====================================                                 |  53%
  |                                                                            
  |======================================                                |  54%
  |                                                                            
  |======================================                                |  55%
  |                                                                            
  |=======================================                               |  55%
  |                                                                            
  |========================================                              |  57%
  |                                                                            
  |=========================================                             |  58%
  |                                                                            
  |=========================================                             |  59%
  |                                                                            
  |==========================================                            |  60%
  |                                                                            
  |===========================================                           |  61%
  |                                                                            
  |===========================================                           |  62%
  |                                                                            
  |============================================                          |  63%
  |                                                                            
  |=============================================                         |  64%
  |                                                                            
  |=============================================                         |  65%
  |                                                                            
  |==============================================                        |  65%
  |                                                                            
  |===============================================                       |  67%
  |                                                                            
  |===============================================                       |  68%
  |                                                                            
  |================================================                      |  68%
  |                                                                            
  |================================================                      |  69%
  |                                                                            
  |=================================================                     |  70%
  |                                                                            
  |==================================================                    |  72%
  |                                                                            
  |====================================================                  |  74%
  |                                                                            
  |=====================================================                 |  76%
  |                                                                            
  |======================================================                |  78%
  |                                                                            
  |=======================================================               |  78%
  |                                                                            
  |=======================================================               |  79%
  |                                                                            
  |========================================================              |  81%
  |                                                                            
  |==========================================================            |  83%
  |                                                                            
  |===========================================================           |  84%
  |                                                                            
  |===========================================================           |  85%
  |                                                                            
  |=============================================================         |  87%
  |                                                                            
  |==============================================================        |  89%
  |                                                                            
  |===============================================================       |  90%
  |                                                                            
  |=================================================================     |  92%
  |                                                                            
  |=================================================================     |  93%
  |                                                                            
  |=================================================================     |  94%
  |                                                                            
  |==================================================================    |  94%
  |                                                                            
  |==================================================================    |  95%
  |                                                                            
  |===================================================================   |  95%
  |                                                                            
  |===================================================================   |  96%
  |                                                                            
  |====================================================================  |  97%
  |                                                                            
  |====================================================================  |  98%
  |                                                                            
  |===================================================================== |  98%
  |                                                                            
  |===================================================================== |  99%
  |                                                                            
  |======================================================================| 100%
\end{verbatim}

Okay, now let's visualize. We're going to build a choropleth map, with
the color of each county -- the fill -- set according to the number of
notices per 1K population on a color gradient.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_centroids }\OtherTok{\textless{}{-}} \FunctionTok{st\_centroid}\NormalTok{(md\_counties)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning in st_centroid.sf(md_counties): st_centroid assumes attributes are
constant over geometries of x
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_centroids\_df }\OtherTok{\textless{}{-}} \FunctionTok{as.data.frame}\NormalTok{(}\FunctionTok{st\_coordinates}\NormalTok{(county\_centroids))}
\NormalTok{county\_centroids\_df}\SpecialCharTok{$}\NormalTok{NAME }\OtherTok{\textless{}{-}}\NormalTok{ county\_centroids}\SpecialCharTok{$}\NormalTok{NAME}

\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data=}\NormalTok{md\_pop\_with\_foreclosures, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{rate)) }\SpecialCharTok{+}
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ X, }\AttributeTok{y =}\NormalTok{ Y, }\AttributeTok{label =}\NormalTok{ NAME), }\AttributeTok{data =}\NormalTok{ county\_centroids\_df, }\AttributeTok{size =} \DecValTok{3}\NormalTok{, }\AttributeTok{check\_overlap =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographicanalysis_files/figure-pdf/unnamed-chunk-9-1.pdf}

}

\end{figure}

This map is okay, but the color scale makes it hard to draw fine-grained
differences. Let's try applying the magma color scale we learned in the
last chapter.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data=}\NormalTok{md\_pop\_with\_foreclosures, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{rate)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_b}\NormalTok{(}\AttributeTok{option=}\StringTok{"magma"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographicanalysis_files/figure-pdf/unnamed-chunk-10-1.pdf}

}

\end{figure}

The highest ranking counties stand out nicely in this version, but it's
still hard to make out fine-grained differences between other counties.

So let's change the color scale to a ``log'' scale, which will help us
see those differences a bit more clearly.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_sf}\NormalTok{(}\AttributeTok{data=}\NormalTok{md\_pop\_with\_foreclosures, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{fill=}\NormalTok{rate)) }\SpecialCharTok{+}
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{scale\_fill\_viridis\_b}\NormalTok{(}\AttributeTok{option=}\StringTok{"magma"}\NormalTok{,}\AttributeTok{trans =} \StringTok{"log"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{./geographicanalysis_files/figure-pdf/unnamed-chunk-11-1.pdf}

}

\end{figure}

\bookmarksetup{startatroot}

\hypertarget{an-intro-to-text-analysis}{%
\chapter{An intro to text analysis}\label{an-intro-to-text-analysis}}

Throughout this course, we've been focused on finding information in
structured data. We've learned a lot of techniques to do that, and we've
learned how the creative mixing and matching of those skills can find
new insights.

What happens when the insights are in unstructured data? Like a block of
text?

Turning unstructured text into data to analyze is a whole course in and
of itself -- and one worth taking if you've got the credit hours -- but
some simple stuff is in the grasp of basic data analysis.

To do this, we'll need a new library --
\href{https://cran.r-project.org/web/packages/tidytext/vignettes/tidytext.html}{tidytext},
which you can guess by the name plays very nicely with the tidyverse. So
install it with \texttt{install.packages("tidytext")} and we'll get
rolling.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(tidytext)}
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

Here's the question we're going to go after: what words or phrases
appear most in Maryland Sen.~Ben Cardin's press releases?

To answer this question, we'll use the text of Cardin's most recent 999
press releases.

Let's read in this data and examine it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\OtherTok{\textless{}{-}} \FunctionTok{read\_rds}\NormalTok{(}\StringTok{"data/cardin\_releases.rds"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

We can see what it looks like with head:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{head}\NormalTok{(releases)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 4
  date       title                                                   url   text 
  <date>     <chr>                                                   <chr> <chr>
1 2022-12-01 Cardin, Trone Lead Effort to Review Health Impacts of ~ http~ " – ~
2 2022-12-01 Cardin Votes to Avert Rail Strike, Will Continue to Ad~ http~ " – ~
3 2022-12-01 Cardin Introduces Legislation to Reauthorize and Impro~ http~ "– U~
4 2022-12-01 Cardin Marks World AIDS Day 2022                        http~ " – ~
5 2022-12-01 Wyden, Crapo and Bipartisan Senate Finance Committee M~ http~ " – ~
6 2022-12-01 Cardin, Wicker Lead Legislation to Designate Wagner Gr~ http~ " – ~
\end{verbatim}

What we want to do is to make the \texttt{text} column easier to
analyze. Let's say we want to find out the most commonly used words.
We'll want to remove URLs from the text of the releases since they
aren't actual words. Let's use mutate to make that happen:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\OtherTok{\textless{}{-}}\NormalTok{ releases }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{text =} \FunctionTok{gsub}\NormalTok{(}\StringTok{"http.*"}\NormalTok{,}\StringTok{""}\NormalTok{, text))}
\end{Highlighting}
\end{Shaded}

If you are trying to create a list of unique words, R will treat
differences in capitalization as unique and also will include
punctuation by default, even using its \texttt{unique} function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{a\_list\_of\_words }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\StringTok{"Dog"}\NormalTok{, }\StringTok{"dog"}\NormalTok{, }\StringTok{"dog"}\NormalTok{, }\StringTok{"cat"}\NormalTok{, }\StringTok{"cat"}\NormalTok{, }\StringTok{","}\NormalTok{)}
\FunctionTok{unique}\NormalTok{(a\_list\_of\_words)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "Dog" "dog" "cat" ","  
\end{verbatim}

Fortunately, this is a solved problem with tidytext, which has a
function called \texttt{unnest\_tokens} that will convert the text to
lowercase and remove all punctuation. The way that
\texttt{unnest\_tokens} works is that we tell it what we want to call
the field we're creating with this breaking apart, then we tell it what
we're breaking apart -- what field has all the text in it. For us,
that's the \texttt{text} column:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unique\_words }\OtherTok{\textless{}{-}}\NormalTok{ releases }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text)}
\FunctionTok{View}\NormalTok{(unique\_words)}
\end{Highlighting}
\end{Shaded}

Now we can look at the top words in this dataset. Let's limit ourselves
to making a plot of the top 25 words:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unique\_words }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{25}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{word =} \FunctionTok{reorder}\NormalTok{(word, n)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ word, }\AttributeTok{y =}\NormalTok{ n)) }\SpecialCharTok{+}
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{xlab}\NormalTok{(}\ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{coord\_flip}\NormalTok{() }\SpecialCharTok{+}
      \FunctionTok{labs}\NormalTok{(}\AttributeTok{x =} \StringTok{"Count"}\NormalTok{,}
      \AttributeTok{y =} \StringTok{"Unique words"}\NormalTok{,}
      \AttributeTok{title =} \StringTok{"Count of unique words found in Cardin releases"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Selecting by n
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./textanalysis_files/figure-pdf/unnamed-chunk-7-1.pdf}

}

\end{figure}

Well, that's a bit underwhelming - a lot of very common (and short)
words. This also is a solved problem in working with text data, and
words like ``a'' and ``the'' are known as ``stop words''. In most cases
you'll want to remove them from your analysis since they are so common.
Tidytext provides a dataframe of them that we'll load, and then we'll
add some of our own.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{data}\NormalTok{(}\StringTok{"stop\_words"}\NormalTok{)}

\NormalTok{stop\_words }\OtherTok{\textless{}{-}}\NormalTok{ stop\_words }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"ben"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"cardin"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"senator"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"senators"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"maryland"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{\textquotesingle{}federal\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{\textquotesingle{}u.s\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{\textquotesingle{}md\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{\textquotesingle{}senate\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"hollen"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"van"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{add\_row}\NormalTok{(}\AttributeTok{word =} \StringTok{"chris"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then we're going to use a function we haven't used yet called an
\texttt{anti\_join}, which filters out any matches. So we'll
\texttt{anti\_join} the stop words and get a list of words that aren't
stop words.

From there, we can get a simple word frequency by just grouping them
together and counting them. We can borrow the percent code from above to
get a percent of the words our top 10 words represent.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unique\_words }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tally}\NormalTok{(}\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =}\NormalTok{ (n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
Selecting by percent
\end{verbatim}

\begin{verbatim}
# A tibble: 10 x 3
   word            n percent
   <chr>       <int>   <dbl>
 1 health       1804   0.576
 2 support      1509   0.481
 3 act          1488   0.475
 4 funding      1457   0.465
 5 program      1297   0.414
 6 project      1255   0.400
 7 communities  1189   0.379
 8 community    1015   0.324
 9 public       1014   0.323
10 covid        1008   0.322
\end{verbatim}

Those seem like more relevant unique words. Now, here's where we can
start to do more interesting and meaningful analysis. Let's create two
dataframes of unique words based on time: one for all of 2021 and the
other for all of 2022:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unique\_words\_2021 }\OtherTok{\textless{}{-}}\NormalTok{ releases }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(date }\SpecialCharTok{\textless{}} \StringTok{\textquotesingle{}2022{-}01{-}01\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text)}

\NormalTok{unique\_words\_2022 }\OtherTok{\textless{}{-}}\NormalTok{ releases }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(date }\SpecialCharTok{\textgreater{}=} \StringTok{\textquotesingle{}2022{-}01{-}01\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(text) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(word, text)}
\end{Highlighting}
\end{Shaded}

Then we can create top 10 lists for both of them and compare:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unique\_words\_2021 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tally}\NormalTok{(}\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =}\NormalTok{ (n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
Selecting by percent
\end{verbatim}

\begin{verbatim}
# A tibble: 10 x 3
   word            n percent
   <chr>       <int>   <dbl>
 1 health       1177   0.608
 2 act          1025   0.530
 3 funding       962   0.497
 4 support       902   0.466
 5 covid         807   0.417
 6 19            806   0.417
 7 communities   767   0.396
 8 program       736   0.380
 9 american      708   0.366
10 pandemic      708   0.366
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unique\_words\_2022 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{anti\_join}\NormalTok{(stop\_words) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tally}\NormalTok{(}\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =}\NormalTok{ (n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
Selecting by percent
\end{verbatim}

\begin{verbatim}
# A tibble: 10 x 3
   word            n percent
   <chr>       <int>   <dbl>
 1 project       834   0.695
 2 health        627   0.523
 3 support       607   0.506
 4 program       561   0.468
 5 funding       495   0.413
 6 act           463   0.386
 7 community     453   0.378
 8 communities   422   0.352
 9 funds         421   0.351
10 baltimore     416   0.347
\end{verbatim}

In the 2021 top 10 list, ``covid'' and ``pandemic'' appear, which makes
sense, while the 2022 doesn't reference the same topic aside from the
more generic ``health''.

\hypertarget{going-beyond-a-single-word}{%
\section{Going beyond a single word}\label{going-beyond-a-single-word}}

The next step in text analysis is using \texttt{ngrams}. An
\texttt{ngram} is any combination of words that you specify. Two word
ngrams are called bigrams (bi-grams). Three would be trigrams. And so
forth.

The code to make ngrams is similar to what we did above, but involves
some more twists.

So this block is is going to do the following:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Use the releases data we created above, and filter for pre-2022
  releases.
\item
  Unnest the tokens again, but instead we're going to create a field
  called bigram, break apart summary, but we're going to specify the
  tokens in this case are ngrams of 2.
\item
  We're going to make things easier to read and split bigrams into word1
  and word2.
\item
  We're going to filter out stopwords again, but this time we're going
  to do it in both word1 and word2 using a slightly different filtering
  method.
\item
  Because of some weirdness in calculating the percentage, we're going
  to put bigram back together again, now that the stop words are gone.
\item
  We'll then group by, count and create a percent just like we did
  above.
\item
  We'll then use top\_n to give us the top 10 bigrams.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(date }\SpecialCharTok{\textless{}} \StringTok{\textquotesingle{}2022{-}01{-}01\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(bigram, text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word1 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word2 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{bigram =} \FunctionTok{paste}\NormalTok{(word1, word2, }\AttributeTok{sep=}\StringTok{" "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(bigram) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tally}\NormalTok{(}\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =}\NormalTok{ (n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Selecting by percent
\end{verbatim}

\begin{verbatim}
# A tibble: 10 x 3
   bigram              n percent
   <chr>           <int>   <dbl>
 1 covid 19          786   0.856
 2 human rights      319   0.347
 3 19 pandemic       288   0.314
 4 health care       224   0.244
 5 chesapeake bay    190   0.207
 6 american rescue   166   0.181
 7 rescue plan       166   0.181
 8 public health     157   0.171
 9 john sarbanes     133   0.145
10 kweisi mfume      133   0.145
\end{verbatim}

And we already have a different, more nuanced result. Health was among
the top single words, and we can see that ``health care'', ``human
rights'' and ``chesapeake bay'' are among the top 2-word phrases. What
about after 2021?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(date }\SpecialCharTok{\textgreater{}=} \StringTok{\textquotesingle{}2022{-}01{-}01\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{unnest\_tokens}\NormalTok{(bigram, text, }\AttributeTok{token =} \StringTok{"ngrams"}\NormalTok{, }\AttributeTok{n =} \DecValTok{2}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{separate}\NormalTok{(bigram, }\FunctionTok{c}\NormalTok{(}\StringTok{"word1"}\NormalTok{, }\StringTok{"word2"}\NormalTok{), }\AttributeTok{sep =} \StringTok{" "}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word1 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\NormalTok{word2 }\SpecialCharTok{\%in\%}\NormalTok{ stop\_words}\SpecialCharTok{$}\NormalTok{word) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{bigram =} \FunctionTok{paste}\NormalTok{(word1, word2, }\AttributeTok{sep=}\StringTok{" "}\NormalTok{)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(bigram) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{tally}\NormalTok{(}\AttributeTok{sort=}\ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{percent =}\NormalTok{ (n}\SpecialCharTok{/}\FunctionTok{sum}\NormalTok{(n))}\SpecialCharTok{*}\DecValTok{100}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{top\_n}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Selecting by percent
\end{verbatim}

\begin{verbatim}
# A tibble: 10 x 3
   bigram                 n percent
   <chr>              <int>   <dbl>
 1 project location     262   0.447
 2 covid 19             167   0.285
 3 health care          140   0.239
 4 chesapeake bay       139   0.237
 5 human rights         119   0.203
 6 amount included      113   0.193
 7 baltimore city       100   0.171
 8 prince george’s       98   0.167
 9 mental health         94   0.160
10 location baltimore    85   0.145
\end{verbatim}

While ``covid 19'' is still a top phrase, it's not \emph{the} leading
phrase any longer. ``Mental health'' makes an appearance, too. You'll
notice that the percentages are very small; that's not irrelevant but in
some cases it's the differences in patterns that's more important.

There are some potential challenges to doing an analysis. For one, there
are variations of words that could probably be standardized - maybe
using OpenRefine - that would give us cleaner results. There might be
some words among our list of stop words that actually are meaningful in
this context.

\hypertarget{sentiment-analysis}{%
\section{Sentiment Analysis}\label{sentiment-analysis}}

Another popular use of text analysis is to measure the sentiment of a
word - whether it expresses a positive or negative idea - and tidytext
has built-in tools to make that possible. We use word counts like we've
already calculated and bring in a dataframe of words (called a lexicon)
along with their sentiments using a function called
\texttt{get\_sentiments}. The most common dataframe is called ``bing''
which has nothing to do with the Microsoft search engine. Let's load it:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bing }\OtherTok{\textless{}{-}} \FunctionTok{get\_sentiments}\NormalTok{(}\StringTok{"bing"}\NormalTok{)}

\NormalTok{bing\_word\_counts\_2021 }\OtherTok{\textless{}{-}}\NormalTok{ unique\_words\_2021 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(bing) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, sentiment, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bing\_word\_counts\_2022 }\OtherTok{\textless{}{-}}\NormalTok{ unique\_words\_2022 }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{inner\_join}\NormalTok{(bing) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{count}\NormalTok{(word, sentiment, }\AttributeTok{sort =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Joining with `by = join_by(word)`
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{View}\NormalTok{(bing\_word\_counts\_2021)}
\FunctionTok{View}\NormalTok{(bing\_word\_counts\_2022)}
\end{Highlighting}
\end{Shaded}

Gauging the sentiment of a word can be heavily dependent on the context,
and as with other types of text analysis sometimes larger patterns are
more meaningful than individual results. But the potential with text
analysis is vast: knowing what words and phrases that public officials
employ can be a way to evaluate their priorities, cohesiveness and
tactics for persuading voters and their colleagues. And those words and
phrases are data.

\bookmarksetup{startatroot}

\hypertarget{scraping-data-with-rvest}{%
\chapter{Scraping data with Rvest}\label{scraping-data-with-rvest}}

Sometimes, governments put data online on a page or in a searchable
database. And when you ask them for a copy of the data underneath the
website, they say no.

Why? Because they have a website. That's it. That's their reason. They
say they don't have to give you the data because they've already given
you the data, never mind that they haven't given to you in a form you
can actually load into R with ease.

Lucky for us, there's a way for us to write code to get data even when
an agency hasn't made it easy: webscraping.

One of the most powerful tools you can learn as a data journalist is how
to scrape data from the web. Scraping is the process of programming a
computer to act like a human that opens a web browser, goes to a
website, ingests the HTML from that website into R and turns it into
data.

The degree of difficulty here goes from ``Easy'' to ``So Hard You Want
To Throw Your Laptop Out A Window.'' And the curve between the two can
be steep. You can learn how to scrape ``Easy'' in a day. The hard ones
take a little more time, but it's often well worth the effort because it
lets you get stories you couldn't get without it.

In this chapter, we'll show you an easy one. And in the next chapter,
we'll so you a moderately harder one.

Let's start easy.

We're going to use a library called \texttt{rvest}, which you can
install it the same way we've done all installs: go to the console and
\texttt{install.packages("rvest")}.

Like so many R package names, rvest is a bad pun. You're supposed to
read it to sound like ``harvest'', as in ``harvesting'' information from
a website the same way you'd harvest crops in a field.

We'll load these packages first:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(rvest)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(lubridate)}
\FunctionTok{library}\NormalTok{(janitor)}
\end{Highlighting}
\end{Shaded}

For this example, we're going to work on loading a simple table of press
releases from the Maryland State Courts.

Let's suppose we can't find a table like that for download, but we do
see a version on the website at this URL:
\url{https://www.courts.state.md.us/media/news}.

\includegraphics[width=7.35in,height=\textheight]{./images/rvest1.png}

We could get this table into R with the following manual steps:
highlighting the text, copying it into Excel, saving it as a csv, and
reading it into R. Or, we could write a few lines of webscraping code to
have R do that for us!

In this simple example, it's probably faster to do it manually than have
R do it for us. But during the time when ballots are being counted, this
table is likely to change, and we don't want to keep doing manual
repetitive tasks.

Why would we ever write code to grab a single table? There's several
reasons:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Our methods are transparent. If a colleague wants to run our code from
  scratch to factcheck our work, they don't need to repeat the manual
  steps, which are harder to document than writing code.
\item
  Let's suppose we wanted to grab the same table every day, to monitor
  for changes. Writing a script once, and pressing a single button every
  day is going to be much more efficient than doing this manually every
  day.
\item
  If we're doing it manually, we're more likely to make a mistake, like
  maybe failing to copy every row from the whole table.
\item
  It's good practice to prepare us to do more complex scraping jobs. As
  we'll see in the next chapter, if we ever want to grab the same table
  from hundreds of pages, writing code is much faster and easier than
  going to a hundred different pages ourselves and downloading data.
\end{enumerate}

So, to scrape, the first thing we need to do is start with the URL.
Let's store it as an object called ag\_url.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{courts\_url }\OtherTok{\textless{}{-}} \StringTok{"https://www.courts.state.md.us/media/news"}
\end{Highlighting}
\end{Shaded}

When we go to the web page, we can see a nicely-designed page that
contains our information.

But what we really care about, for our purposes, is the html code that
creates that page.

In our web browser, if we right-click anywhere on the page and select
``view source'' from the popup menu, we can see the source code. Or you
can just copy this into Google Chrome:
view-source:https://www.courts.state.md.us/media/news.

Here's a picture of what some of the source code looks like.

\includegraphics[width=7.08in,height=\textheight]{./images/rvest2.png}

We'll use those HTML tags -- things like
\texttt{\textless{}table\textgreater{}} and
\texttt{\textless{}tr\textgreater{}} -- to grab the info we need.

Okay, step 1.

Let's write a bit of code to tell R to go to the URL for the page and
ingest all of that HTML code. In the code below, we're starting with our
URL and using the read\_html() function from rvest to ingest all of the
page html, storing it as an object called results.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in the html}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ courts\_url }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{read\_html}\NormalTok{()}

\CommentTok{\# display the html below}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{html_document}
<html lang="en" dir="ltr" prefix="content: http://purl.org/rss/1.0/modules/content/  dc: http://purl.org/dc/terms/  foaf: http://xmlns.com/foaf/0.1/  og: http://ogp.me/ns#  rdfs: http://www.w3.org/2000/01/rdf-schema#  schema: http://schema.org/  sioc: http://rdfs.org/sioc/ns#  sioct: http://rdfs.org/sioc/types#  skos: http://www.w3.org/2004/02/skos/core#  xsd: http://www.w3.org/2001/XMLSchema# ">
[1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
[2] <body class="path-media no-sidebars">\n        <a href="#main-content" cl ...
\end{verbatim}

If you're running this code in R Studio, in our environment window at
right, you'll see results as a ``list of 2''.

This is not a dataframe, it's a different type of data structure a
``nested list.''

If we click on the name ``results'' in our environment window, we can
see that it's pulled in the html and shown us the general page
structure. Nested within the \texttt{\textless{}html\textgreater{}} tag
is the \texttt{\textless{}head\textgreater{}} and
\texttt{\textless{}body\textgreater{}}, the two fundamental sections of
most web pages. We're going to pull information out of the
\texttt{\textless{}body\textgreater{}} tag in a bit.

\includegraphics[width=3.25in,height=\textheight]{./images/rvest3.png}

Now, our task is to just pull out the section of the html that contains
the information we need.

But which part do we need from that mess of html code? To figure that
out, we can go back to the page in a web browser like chrome, and use
built in developer tools to ``inspect'' the html code underlying the
page.

On the page, find the data we want to grab and right click on the word
``Date'' in the column header of the table. That will bring up a
dropdown menu. Select ``Inspect'', which will pop up a window called the
``element inspector'' that shows us where different elements on the page
are located, what html tags created those elements, and other info.

\includegraphics[width=3.13in,height=\textheight]{./images/rvest4.png}

The table that we want is actually contained inside an html
\texttt{\textless{}table\textgreater{}}. It has a
\texttt{\textless{}tbody\textgreater{}} that contains one row
\texttt{\textless{}tr\textgreater{}} per release.

Because it's inside of a table, and not some other kind of element (like
a \texttt{\textless{}div\textgreater{}}), rvest has a special function
for easily extracting and converting html tables, called html\_table().
This function extracts all the html tables on the page, but this page
only has one so we're good.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in the html and extract all the tables}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ courts\_url }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{read\_html}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{html\_table}\NormalTok{()}

\CommentTok{\# show the dataframe}

\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[[1]]
# A tibble: 19 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 July 5, 2023      Media Advisory: Jury selection, trial to begin for State o~
 2 July 5, 2023      Posthumous bar admission ceremony for Edward Garrison Drap~
 3 June 15, 2023     Judge Yolanda L. Curtin named administrative judge for the~
 4 May 30, 2023      Media Advisory: Circuit Court for Anne Arundel County host~
 5 May 22, 2023      Seven Baltimore City Public Schools students awarded $20,0~
 6 May 15, 2023      Media Advisory: Eighth annual Courting Art Baltimore conte~
 7 May 10, 2023      Judge Dennis M. Robinson, Jr. named Baltimore County admin~
 8 April 25, 2023    Chief Justice Matthew J. Fader appoints Judy Rupp state co~
 9 April 12, 2023    Retired Chief Judge Mary Ellen Barbera receives 2023 Speak~
10 April 11, 2023    Media Advisory: District Court in Anne Arundel County host~
11 April 11, 2023    Media Advisory: Judge Brenda A. Sexton named administrativ~
12 March 23, 2023    Media Advisory: Supreme Court of Maryland hosts Mock Trial~
13 March 16, 2023    Retired Judge Charlotte Cooksey enshrined in  Maryland Wom~
14 March 10, 2023    Media Advisory: District Court in Baltimore City relocates~
15 February 27, 2023 Media Advisory: Baltimore City judge to preside over Balti~
16 February 15, 2023 Media Advisory: Mental health awareness added to Schools i~
17 January 17, 2023  Judge Donna M. Schaeffer named Anne Arundel County adminis~
18 January 13, 2023  Media Advisory: Appellate Court of Maryland holds oral arg~
19 January 12, 2023  Statement on recent judicial appointments                  

[[2]]
# A tibble: 49 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 20, 2022 Media Advisory: Supreme Court of Maryland to hold open mee~
 2 December 14, 2022 Voter-approved constitutional change renames high courts t~
 3 December 7, 2022  Student winners of the Maryland Judiciary's 17th Annual Co~
 4 December 7, 2022  Retired Baltimore Ravens Wide Receiver Jermaine Lewis cele~
 5 December 6, 2022  Media Advisory: Judge DaNeeka Varner Cotton named administ~
 6 December 2, 2022  Media Advisory: Winners to be announced for Maryland Judic~
 7 November 23, 2022 Maryland Judiciary reminds the public to stay vigilant of ~
 8 November 22, 2022 Media Advisory: Baltimore City District Court Re-Entry Pro~
 9 November 17, 2022 New La Plata Court Help Center opens in Charles County     
10 November 16, 2022 Media Advisory: Circuit courts in Maryland host in-person ~
# i 39 more rows

[[3]]
# A tibble: 46 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 27, 2021 Maryland Judiciary reverts operations to Phase III in resp~
 2 December 22, 2021 Small fire at the Circuit Court for Talbot County causes t~
 3 December 15, 2021 Media Advisory: District Court in Anne Arundel County will~
 4 December 10, 2021 New Rockville District Court Help Center is already making~
 5 December 9, 2021  Chief Judge Joseph M. Getty congratulates student winners ~
 6 December 8, 2021  Media Advisory: Grand opening celebration of the District ~
 7 December 8, 2021  Media Advisory: Winners announced for Maryland Judiciary’s~
 8 November 17, 2021 Media Advisory: For a second year, circuit courts in Maryl~
 9 November 15, 2021 Media Advisory: Baltimore City Veterans Treatment Court ce~
10 November 1, 2021  Maryland lawyers donate 1.03 million hours of legal servic~
# i 36 more rows

[[4]]
# A tibble: 67 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 22, 2020 The Maryland Judiciary extends Phase II operations through~
 2 December 21, 2020 Maryland courts, court offices, and administrative offices~
 3 November 24, 2020 Maryland Judiciary restricts operations to Phase II in res~
 4 November 18, 2020 Media Advisory: Circuit courts in Maryland prepare for vir~
 5 November 16, 2020 Media Advisory: Circuit Court for Montgomery County to hon~
 6 November 13, 2020 Judge Ruth A. Jakubowski named circuit and county administ~
 7 November 13, 2020 Judge Sherri D. Koch named District Administrative Judge f~
 8 November 12, 2020 Maryland Judiciary restricts operations to Phase III in re~
 9 November 5, 2020  Somerset County Circuit Court holds its first Adult Recove~
10 November 5, 2020  Baltimore County Family Recovery Support Program celebrate~
# i 57 more rows

[[5]]
# A tibble: 53 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 20, 2019 Maryland courts, court offices, and administrative offices~
 2 December 11, 2019 Circuit Court for Washington County reduces trial jury ser~
 3 December 6, 2019  Maryland Judiciary hosts young winners of 14th Annual Conf~
 4 December 5, 2019  Media Advisory: District Court to hold Schools in the Cour~
 5 December 4, 2019  Media Advisory: Baltimore City District Court Adult Drug T~
 6 December 2, 2019  Media Advisory: Anne Arundel County District Court to hono~
 7 December 2, 2019  Media Advisory: Winners announced for Maryland Judiciary’s~
 8 November 18, 2019 Towson District Courthouse scheduled to move to new Catons~
 9 November 18, 2019 Media Advisory: Circuit courts in Maryland celebrate adopt~
10 November 13, 2019 Media Advisory: Maryland’s highest court to welcome studen~
# i 43 more rows

[[6]]
# A tibble: 82 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 26, 2018 New court rules refine court processes regarding guardians~
 2 December 20, 2018 Courts, court offices, and court administrative offices wi~
 3 December 18, 2018 New Maryland Rules shift pro bono and IOLTA reporting for ~
 4 December 14, 2018 Scheduled website maintenance                              
 5 December 12, 2018 Media Advisory: Anne Arundel County District Court to hono~
 6 December 11, 2018 Maryland lawyers donate 1.16 million hours of legal servic~
 7 December 10, 2018 Media Advisory: Maryland Judiciary hosts participants from~
 8 December 4, 2018  Suzanne C. Johnson named clerk of the Maryland Court of Ap~
 9 December 3, 2018  Media Advisory: Courts, court offices, and administrative ~
10 November 28, 2018 Media Advisory: District Court to hold Schools in the Cour~
# i 72 more rows

[[7]]
# A tibble: 77 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 21, 2017 Judiciary Honors Students for Promoting Peace Through Artw~
 2 December 21, 2017 New court rules regarding guardianships of vulnerable Mary~
 3 December 15, 2017 Baltimore City District Court Honors Re-Entry Project Grad~
 4 December 14, 2017 Court of Appeals Hosts Youth Conference                    
 5 December 8, 2017  Phase II of construction on Robert C. Murphy Courts of App~
 6 December 5, 2017  Judicial Data Center Outage - Thursday, December 7, 2017   
 7 December 5, 2017  Court of Appeals Hosts United States Naval Academy Midship~
 8 November 30, 2017 Pretrial Pilot Program to Launch in Baltimore County       
 9 November 29, 2017 Maryland Judiciary Hosts Oratorical Contest for Youths fro~
10 November 28, 2017 Media Advisory: Maryland Judiciary Hosts Students Who Help~
# i 67 more rows

[[8]]
# A tibble: 59 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 16, 2016 Judges Mentor Students in Civics                           
 2 December 15, 2016 Judiciary Honors Young Artists for Promoting Peace         
 3 December 12, 2016 Maryland Judiciary Expands MDEC and Electronic Filing to L~
 4 December 9, 2016  Media Advisory:  Maryland’s Chief Judge Congratulates Youn~
 5 December 8, 2016  Marilyn Bentley Named Clerk of Baltimore City Circuit Court
 6 December 8, 2016  Judges to Mentor Washington County Students on Civics      
 7 December 8, 2016  Judges Mentor Students on Civics                           
 8 December 1, 2016  New Email Scam Targets Attorneys                           
 9 November 23, 2016 Media Advisory: Judges to Mentor Montgomery County Student~
10 November 21, 2016 Judge Brett W. Wilson Named First Judicial Circuit Adminis~
# i 49 more rows

[[9]]
# A tibble: 64 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 28, 2015 Media Advisory:Pre-Trial Proceedings Begin for State of Ma~
 2 December 22, 2015 Media Advisory: New Trial Date Scheduled for State of Mary~
 3 December 21, 2015 Chief Judge Honors Young Artists for Peace                 
 4 December 15, 2015 New Rules Concerning Briefs Filed in Maryland’s Appellate ~
 5 December 9, 2015  New Mobile App Improves Access to Justice                  
 6 December 1, 2015  Media Advisory:Jury Selection Continues and Trial Begins f~
 7 November 25, 2015 Media Advisory: Judges Bring Civics to Carroll County Stud~
 8 November 23, 2015 Jury Selection for State of Maryland v. William Porter on ~
 9 November 20, 2015 Petition for Expungement of Records (CC-DC-CR-072) - Revis~
10 November 19, 2015 Media Advisory:Pre-Trial Motions Hearing for State of Mary~
# i 54 more rows

[[10]]
# A tibble: 50 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 23, 2014 Judge Charles J. Peters to Head Baltimore Circuit Court’s ~
 2 December 9, 2014  Baltimore City District Court Celebrates Newest Drug Treat~
 3 December 5, 2014  Media Advisory:District Court of  Maryland’s Towson Courth~
 4 November 25, 2014 Judiciary Warns  Public to Beware of “Court” Scams         
 5 November 24, 2014 Judges Help Bring Civics to Charles County High School Stu~
 6 November 14, 2014 Young Artists Honored for Peace-making Efforts             
 7 November 12, 2014 Media Advisory: Maryland Judiciary Hosts Youths from DJS F~
 8 November 10, 2014 Media Advisory: Judiciary Holds Reception to Honor Young A~
 9 November 7, 2014  Media Advisory: Anne Arundel County Circuit Court Address ~
10 November 7, 2014  Media Advisory: Circuit Courts in Maryland Celebrate “Happ~
# i 40 more rows

[[11]]
# A tibble: 26 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 December 30, 2013 Media Advisory: Judiciary Warns of False “Court Case” Emai~
 2 December 27, 2013 Judge Althea M. Handy Heads Civil Docket for Baltimore Cit~
 3 December 17, 2013 Media Advisory: Judiciary Helps Bring Civics to Baltimore ~
 4 December 12, 2013 Harford County Courts Visited by Maryland’s Chief Judge    
 5 December 11, 2013 Cecil County Courts Welcome Maryland’s Chief  Judge        
 6 December 4, 2013  Maryland’s New Chief Judge Visits Charles County Courts    
 7 December 4, 2013  Maryland’s New Chief Judge Visits Prince George’s County D~
 8 December 2, 2013  State Task Force Begins Study of Civil Right to Counsel fo~
 9 November 18, 2013 Media Advisory: Maryland Judiciary Hosts Youths from DJS F~
10 November 18, 2013 Media Advisory: Circuit Courts in Maryland Celebrate Natio~
# i 16 more rows
\end{verbatim}

In the environment window at right, look at results Note that it's now a
``list of 11''.

This gets a little complicated, but what you're seeing here is a nested
list that contains one data frame -- also called tibbles -- one for each
table that exists on the web page we scraped. There are tables for each
year on the page, so we have 11 years' worth of press releases.

Let's say we are interested in 2023, so all we need to do now is to
store that first dataframe as an object. We can do that with this code,
which says ``keep only the first dataframe from our nested list.''

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in all html from table, store all tables on page as nested list of dataframes.}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ courts\_url }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{read\_html}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{html\_table}\NormalTok{()}

\CommentTok{\# Just keep the first dataframe in our list}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results[[}\DecValTok{1}\NormalTok{]]}

\CommentTok{\# show the dataframe}

\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 19 x 2
   Date              `News Release/News Item`                                   
   <chr>             <chr>                                                      
 1 July 5, 2023      Media Advisory: Jury selection, trial to begin for State o~
 2 July 5, 2023      Posthumous bar admission ceremony for Edward Garrison Drap~
 3 June 15, 2023     Judge Yolanda L. Curtin named administrative judge for the~
 4 May 30, 2023      Media Advisory: Circuit Court for Anne Arundel County host~
 5 May 22, 2023      Seven Baltimore City Public Schools students awarded $20,0~
 6 May 15, 2023      Media Advisory: Eighth annual Courting Art Baltimore conte~
 7 May 10, 2023      Judge Dennis M. Robinson, Jr. named Baltimore County admin~
 8 April 25, 2023    Chief Justice Matthew J. Fader appoints Judy Rupp state co~
 9 April 12, 2023    Retired Chief Judge Mary Ellen Barbera receives 2023 Speak~
10 April 11, 2023    Media Advisory: District Court in Anne Arundel County host~
11 April 11, 2023    Media Advisory: Judge Brenda A. Sexton named administrativ~
12 March 23, 2023    Media Advisory: Supreme Court of Maryland hosts Mock Trial~
13 March 16, 2023    Retired Judge Charlotte Cooksey enshrined in  Maryland Wom~
14 March 10, 2023    Media Advisory: District Court in Baltimore City relocates~
15 February 27, 2023 Media Advisory: Baltimore City judge to preside over Balti~
16 February 15, 2023 Media Advisory: Mental health awareness added to Schools i~
17 January 17, 2023  Judge Donna M. Schaeffer named Anne Arundel County adminis~
18 January 13, 2023  Media Advisory: Appellate Court of Maryland holds oral arg~
19 January 12, 2023  Statement on recent judicial appointments                  
\end{verbatim}

We now have a proper dataframe, albeit with some lengthy column headers.

From here, we can do a little cleaning. First we'll use clean\_names()
to lower the column names. Then use rename() to replace the title column
with a simpler version and properly format the date.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Read in all html from table, get the HTML table.}
\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ courts\_url }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{read\_html}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{html\_table}\NormalTok{()}

\CommentTok{\# Standardize column headers, remove last row}

\NormalTok{results }\OtherTok{\textless{}{-}}\NormalTok{ results[[}\DecValTok{1}\NormalTok{]] }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{clean\_names}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\AttributeTok{title =}\NormalTok{ news\_release\_news\_item) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{date =} \FunctionTok{mdy}\NormalTok{(date))}

\CommentTok{\# show the dataframe}
\NormalTok{results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 19 x 2
   date       title                                                             
   <date>     <chr>                                                             
 1 2023-07-05 Media Advisory: Jury selection, trial to begin for State of Maryl~
 2 2023-07-05 Posthumous bar admission ceremony for Edward Garrison Draper      
 3 2023-06-15 Judge Yolanda L. Curtin named administrative judge for the third ~
 4 2023-05-30 Media Advisory: Circuit Court for Anne Arundel County hosts recep~
 5 2023-05-22 Seven Baltimore City Public Schools students awarded $20,000 in s~
 6 2023-05-15 Media Advisory: Eighth annual Courting Art Baltimore contest to a~
 7 2023-05-10 Judge Dennis M. Robinson, Jr. named Baltimore County administrati~
 8 2023-04-25 Chief Justice Matthew J. Fader appoints Judy Rupp state court adm~
 9 2023-04-12 Retired Chief Judge Mary Ellen Barbera receives 2023 Speaker’s Me~
10 2023-04-11 Media Advisory: District Court in Anne Arundel County hosts Schoo~
11 2023-04-11 Media Advisory: Judge Brenda A. Sexton named administrative judge~
12 2023-03-23 Media Advisory: Supreme Court of Maryland hosts Mock Trial State ~
13 2023-03-16 Retired Judge Charlotte Cooksey enshrined in  Maryland Women’s Ha~
14 2023-03-10 Media Advisory: District Court in Baltimore City relocates commis~
15 2023-02-27 Media Advisory: Baltimore City judge to preside over Baltimore Ci~
16 2023-02-15 Media Advisory: Mental health awareness added to Schools in the C~
17 2023-01-17 Judge Donna M. Schaeffer named Anne Arundel County administrative~
18 2023-01-13 Media Advisory: Appellate Court of Maryland holds oral argument f~
19 2023-01-12 Statement on recent judicial appointments                         
\end{verbatim}

And there we go. We now have a nice tidy dataframe of Maryland state
court press releases. We could combine all 11 dataframes if we wanted to
have the whole set.

What about HTML that doesn't have a table? Well, that's where things get
a bit more complicated, and rely on your ability to read HTML and
identify particular elements of it.

Let's consider the list of press releases from the little-known but
fascinating Office of the State Prosecutor:
https://osp.maryland.gov/press-releases/. Let's save that URL as a
variable as we did above:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{osp\_url }\OtherTok{\textless{}{-}} \StringTok{"https://osp.maryland.gov/press{-}releases/"}
\end{Highlighting}
\end{Shaded}

The next step is similar to before, too, except this time we're not
going to call \texttt{html\_table()} because there are no tables here
that we want. In order to find what we want, we'll need to right-click
on the first release date and choose ``Inspect''.

One reason that we use HTML elements like ul and li is to help organize
the code and make it easier to maintain. But it also helps with
scraping, because we can zero in on the elements we want. Let's start by
reading it in:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{osp\_results }\OtherTok{\textless{}{-}}\NormalTok{ osp\_url }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{read\_html}\NormalTok{()}

\CommentTok{\# show the result}
\NormalTok{osp\_results}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{html_document}
<html lang="en-US">
[1] <head>\n<meta http-equiv="Content-Type" content="text/html; charset=UTF-8 ...
[2] <body><div class="container">\n        <div class="skipNav">\n            ...
\end{verbatim}

Once again we get a list of two as our result. In this case, we don't
want to isolate the first item in our list. Instead, we want to try to
locate all of the \texttt{li} tags inside this HTML. Since the
\texttt{li} tag is an HTML element, we use the function
\texttt{html\_elements()} to get them. If we only wanted a single
element, we'd use \texttt{html\_element()}:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{osp\_results }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{\textquotesingle{}li\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
{xml_nodeset (122)}
 [1] <li>\n<a href="https://www.facebook.com/OSPMD/" class="sm-facebook" titl ...
 [2] <li>\n<a href="https://twitter.com/#!/@OSP_MD" class="sm-twitter" title= ...
 [3] <li>\n<a href="https://www.instagram.com/osp_md/" class="sm-instagram2"  ...
 [4] <li><a href="https://www.linkedin.com/company/office-of-the-maryland-sta ...
 [5] <li id="menu-item-565" class="menu-item menu-item-type-custom menu-item- ...
 [6] <li id="menu-item-561" class="menu-item menu-item-type-custom menu-item- ...
 [7] <li id="menu-item-1954" class="menu-item menu-item-type-post_type menu-i ...
 [8] <li id="menu-item-150" class="menu-item menu-item-type-custom menu-item- ...
 [9] <li id="menu-item-1919" class="menu-item menu-item-type-post_type menu-i ...
[10] <li id="menu-item-1382" class="menu-item menu-item-type-post_type menu-i ...
[11] <li id="menu-item-564" class="menu-item menu-item-type-post_type menu-it ...
[12] <li id="menu-item-1363" class="menu-item menu-item-type-custom menu-item ...
[13] <li id="menu-item-566" class="menu-item menu-item-type-custom menu-item- ...
[14] <li id="menu-item-1378" class="menu-item menu-item-type-custom menu-item ...
[15] <li id="menu-item-1379" class="menu-item menu-item-type-custom menu-item ...
[16] <li id="menu-item-1380" class="menu-item menu-item-type-custom menu-item ...
[17] <li class="listing-item"><a class="title" href="https://osp.maryland.gov ...
[18] <li class="listing-item"><a class="title" href="https://osp.maryland.gov ...
[19] <li class="listing-item"><a class="title" href="https://osp.maryland.gov ...
[20] <li class="listing-item"><a class="title" href="https://osp.maryland.gov ...
...
\end{verbatim}

Good news and bad news here: we have all of the

tags, but we have lots of them, and it's hard to see if we have the
right ones. Let's see what we're dealing with and just look at the text
\emph{inside} the

tags. To do that, we add \texttt{html\_text()} to the end of the last
code block:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{osp\_results }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{\textquotesingle{}li\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  [1] "Facebook "                                                                                                                                                                                                                                           
  [2] "Twitter "                                                                                                                                                                                                                                            
  [3] "Instagram  "                                                                                                                                                                                                                                         
  [4] "LinkedIn"                                                                                                                                                                                                                                            
  [5] "Home"                                                                                                                                                                                                                                                
  [6] "Leadership"                                                                                                                                                                                                                                          
  [7] "FILE A COMPLAINT"                                                                                                                                                                                                                                    
  [8] "Press Releases"                                                                                                                                                                                                                                      
  [9] "Social Media"                                                                                                                                                                                                                                        
 [10] "Careers"                                                                                                                                                                                                                                             
 [11] "Contact Us"                                                                                                                                                                                                                                          
 [12] "OSP Annual Reports"                                                                                                                                                                                                                                  
 [13] "Maryland Judiciary Case Search"                                                                                                                                                                                                                      
 [14] "State Board of Elections"                                                                                                                                                                                                                            
 [15] "State Ethics Commission"                                                                                                                                                                                                                             
 [16] "Attorney Grievance Commission"                                                                                                                                                                                                                       
 [17] "July 31, 2023: Treasurer for Political Committees “Friends of Cathy Bevins” and the “Baltimore County Victory Slate” Sentenced to Six Months Incarceration"                                                                                          
 [18] "May 25, 2023: Treasurer for Political Committees “Friends of Cathy Bevins” and the “Baltimore County Victory Slate” Pleads Guilty to Felony Theft Scheme and Perjury"                                                                                
 [19] "April 18, 2023: Former Baltimore City State’s Attorney Sentenced to Two Years in Federal Prison for Unlawfully Obtaining Phone Records"                                                                                                              
 [20] "April 13, 2023: Calvert County Sheriff’s Deputy Pleads Guilty To Misconduct In Office"                                                                                                                                                               
 [21] "February 16, 2023: Treasurer to Political Committees “Friends of Cathy Bevins” and the “Baltimore County Victory Slate” Charged with Multiple Counts Of Felony Theft Scheme, Embezzlement, and Perjury"                                              
 [22] "January 13, 2023: Former Maryland State Delegate Richard K. Impallaria Pleads Guilty to Misconduct in Office for Misusing State Funds"                                                                                                               
 [23] "January 9, 2023: Calvert County Sheriff’s Deputy Charged with Misconduct in Office for Engaging in Sexual Relations with a Person Requesting Assistance"                                                                                             
 [24] "January 4, 2023: Former Annapolis Police Department Officer Pleads Guilty to Misconduct in Office"                                                                                                                                                   
 [25] "December 9, 2022: Former Baltimore City Assistant State’s Attorney Pleads Guilty to Federal Charges For Unlawfully Obtaining Phone Records"                                                                                                          
 [26] "September 30, 2022: Former Baltimore City Assistant State’s Attorney Facing Federal Charges for Unlawfully Obtaining Phone Records"                                                                                                                  
 [27] "September 29, 2022: Treasurer of Political Committee “Team 30 Slate” Pleads Guilty to Theft"                                                                                                                                                         
 [28] "July 27, 2022: Delegate Richard Impallaria charged with Felony Theft and related crimes for schemes related to misuse of state funds"                                                                                                                
 [29] "May 4, 2022: Treasurer of Political Committee “Team 30 Slate” Charged with Theft of Campaign Funds"                                                                                                                                                  
 [30] "April 18, 2022: Andrew Bradshaw, Former Mayor of Cambridge, MD, Pleads Guilty to Five Counts of Distributing Revenge Porn"                                                                                                                           
 [31] "November 30, 2021: Former Baltimore City Homicide Prosecutor Indicted on 88 Charges, including Stalking, Extortion, and Misconduct in Office"                                                                                                        
 [32] "November 23, 2021:  Fairmount Heights Police Officer Charged with Kidnapping, Perjury and Misconduct in Office"                                                                                                                                      
 [33] "November 15, 2021: Mayor of Cambridge Charged with violating Maryland’s Revenge Porn Statute"                                                                                                                                                        
 [34] "October 26, 2021: Non-Profit CEO Sentenced to Two Years in Federal Prison after Pleading Guilty to Wire Fraud in Connection with the Misuse of Federal Funds Intended For The Treatment of Survivors of Domestic Violence and Sexual Assault"        
 [35] "October 5, 2021: Former Executive Director Of Maryland Environmental Service Facing Federal And State Charges For Allegedly Fraudulently Obtaining More Than $276,731 From His Employer, And State Felony Violations Of The Maryland Wiretap Statute"
 [36] "September 23, 2021: Two Additional Baltimore Correctional Employees Plead Guilty, Making a Total of Eight Correctional Employees Convicted of Charges Related to Theft of over $400,000 Dollars of State Funds."                                     
 [37] "August 20, 2021: Ivan Gonzalez, Former Baltimore Police Officer and Candidate for Mayor of Baltimore City, Pleads Guilty to Perjury"                                                                                                                 
 [38] "August 11, 2021: Baltimore Correctional Officer Pleads Guilty to Charges of Felony Theft and Misconduct in Office"                                                                                                                                   
 [39] "August 9, 2021: Edward M. Estes, former Mayor of the City of Glenarden, Pleads Guilty to Misconduct in Office"                                                                                                                                       
 [40] "July 30, 2021:  Lora Walters, former Deputy Director of the Cecil County Board of Elections, Pleads Guilty to Misconduct in Office"                                                                                                                  
 [41] "July 29, 2021: Baltimore Correctional Officers Plead Guilty to Charges of Felony Theft and Misconduct in Office"                                                                                                                                     
 [42] "July 26, 2021: Baltimore City Employee Pleads Guilty to Perjury"                                                                                                                                                                                     
 [43] "July 9, 2021: Correctional Officer at Metropolitan Transition Center in Baltimore Pleads Guilty to Charges of Felony Theft and Misconduct in Office"                                                                                                 
 [44] "June 10, 2021:  Five Correctional Officers charged with Felony Theft and Misconduct in Office"                                                                                                                                                       
 [45] "June 4, 2021:  Lora Walters, former Deputy Director of the Cecil County Board of Elections, charged with Misconduct in Office, Perjury, and other offenses."                                                                                         
 [46] "March 29, 2021: Ivan Gonzalez, Baltimore Police Officer and Candidate for Mayor of Baltimore City, charged with Perjury and other crimes related to his 2020 Mayoral Campaign"                                                                       
 [47] "March 11, 2021: New Bank Fraud and Aggravated Identity Theft Charges Added to Non-Profit CEO’S Previous Federal Indictment Charging Her with Wire Fraud, Bank Fraud, and Aggravated Identity Theft"                                                  
 [48] "January 15, 2021: Correctional Officers and State Employee Charged with Theft and Bribery"                                                                                                                                                           
 [49] "January 13, 2021: Baltimore City Employee Charged with Perjury"                                                                                                                                                                                      
 [50] "November 17, 2020: Edward M. Estes, Mayor of Glen Arden, Maryland, Charged with Violating the Maryland Wiretap Statute and Misconduct in Office"                                                                                                     
 [51] "June 19, 2020: Former Baltimore Mayor Catherine Pugh Pleads Guilty To Perjury"                                                                                                                                                                       
 [52] "March 17, 2020: Level II – Flexible Operations of the Pandemic Flu and Other Infectious Diseases Attendance and Leave Policy"                                                                                                                        
 [53] "February 13, 2020: Devan Martin,  OSP Case #20-5785"                                                                                                                                                                                                 
 [54] "January 17, 2020: Greensboro Police Chief Pleads Guilty to Misconduct in Office"                                                                                                                                                                     
 [55] "January 6, 2020: Charles Blomquist and Sarah David Named Deputy State Prosecutors"                                                                                                                                                                   
 [56] "December 18, 2019: Ex-Baltimore Mayor Catherine Pugh Charged with Perjury"                                                                                                                                                                           
 [57] "December 10, 2019: City of District Heights Mayor Sentenced for Misconduct in Office"                                                                                                                                                                
 [58] "December 5, 2019: Former Baltimore City School Business Manager Pleads Guilty to Theft of School Funds"                                                                                                                                              
 [59] "November 25, 2019: Charlton T. Howard III Sworn in as State Prosecutor"                                                                                                                                                                              
 [60] "November 20, 2019: City of District Heights Mayor Convicted of Misconduct in Office"                                                                                                                                                                 
 [61] "November 19, 2019: Maryland State Police Sergeant Charged with Misconduct in Office"                                                                                                                                                                 
 [62] "November 14, 2019: Greensboro Police Chief Charged with Misconduct in Office"                                                                                                                                                                        
 [63] "November 7, 2019: Former Baltimore City School Business Manager Charged with Theft of School Funds"                                                                                                                                                  
 [64] "October 30, 2019: Candidate for Sheriff in Baltimore City Pleads Guilty to Campaign Violations"                                                                                                                                                      
 [65] "Meeting Minutes for the 09/24/19 Meeting of the State Prosecutor Selection and Disabilities Commission. "                                                                                                                                            
 [66] "Meeting Minutes for the 09/17/19 Meeting of the State Prosecutor Selection and Disabilities Commission"                                                                                                                                              
 [67] "Notice of Meeting of State Prosecutor Selection and Disabilities Commission"                                                                                                                                                                         
 [68] "Meeting Minutes for the 08/21/19 Meeting of the State Prosecutor Selection and Disabilities Commission"                                                                                                                                              
 [69] "Notice of Meeting of State Prosecutor Selection and Disabilities Commission"                                                                                                                                                                         
 [70] "State Prosecutor Selection and Disabilities Commission Receives 18 Total Applications for State Prosecutor Vacancy; 2 Withdraw"                                                                                                                      
 [71] "September 9, 2019: Candidate for Sheriff of Baltimore City Charged with Campaign Violations"                                                                                                                                                         
 [72] "Notice of Meeting of State Prosecutor Selection and Disabilities Commission"                                                                                                                                                                         
 [73] "State Prosecutor Selection and Disabilities Commission Receives 18 Applications for State Prosecutor Vacancy"                                                                                                                                        
 [74] "August 15, 2019: Notice of Meeting of State Prosecutor Selection and Disabilities  Commission"                                                                                                                                                       
 [75] "July 9, 2019: Former Pocomoke City Police Chief Sentenced for Misconduct in Office Conviction"                                                                                                                                                       
 [76] "July 9, 2019: Kelvin Sewell, Sentencing Memorandum"                                                                                                                                                                                                  
 [77] "June 7, 2019: City of District Heights Mayor Charged with Misconduct in Office"                                                                                                                                                                      
 [78] "June 7, 2019: Criminal Information, Eddie Martin"                                                                                                                                                                                                    
 [79] "April 3, 2019: Circuit Court for Anne Arundel County Issues Memorandum Opinion in State of MD v. John Leopold"                                                                                                                                       
 [80] "April 2, 2019: Baltimore Area Tax Preparer Sentenced For Felony Theft And Willfully Filing A False Tax Return"                                                                                                                                       
 [81] "February 1, 2019:Baltimore Area Tax Preparer And Former Maryland Department Of Human Services Employee Pleads Guilty To Felony Theft Scheme And Willfully Filing False Income Tax Returns."                                                          
 [82] "February 1, 2019: Statement of Facts, Floyd Briscoe Wright"                                                                                                                                                                                          
 [83] "January 22, 2019: State Prosecutor Announces Retirement"                                                                                                                                                                                             
 [84] "January 14, 2019: Eighteen of Eighteen Defendants Plead Guilty to their roles in a Corruption and Bribery Conspiracy"                                                                                                                                
 [85] "November 16, 2018: Former Baltimore County Attorney Pleads Guilty to Felony Theft from Clients"                                                                                                                                                      
 [86] "November 16, 2018: Statement of Facts for Roger Harris"                                                                                                                                                                                              
 [87] "October 1, 2018: Baltimore Area Tax Preparer Indicted for Felony Theft, Willfully Filing False Taxes and Other Related Charges"                                                                                                                      
 [88] "October 1, 2018: Floyd Wright Indictment"                                                                                                                                                                                                            
 [89] "September 14, 2018: Former Correctional Officer Jailed in Corruption and Bribery Conspiracy"                                                                                                                                                         
 [90] "July 12, 2018: Nine Defendants Plead Guilty to Corruption and Bribery Conspiracy"                                                                                                                                                                    
 [91] "May 15, 2018: Kelly Madigan Named Deputy State Prosecutor"                                                                                                                                                                                           
 [92] "April 20, 2018: Shaun Dallas Dance Sentenced"                                                                                                                                                                                                        
 [93] "April 19, 2018: Former Attorney Charged with Felony Theft from Clients"                                                                                                                                                                              
 [94] "April 6, 2018: Former Register of Wills for Allegany County Pleads Guilty to Misconduct In Office"                                                                                                                                                   
 [95] "April 6, 2018: Rebecca Drew, Statement of Facts in Support of Guilty Plea"                                                                                                                                                                           
 [96] "March 8, 2018: Shaun Dallas Dance, Statement of Facts in Support of Guilty Plea"                                                                                                                                                                     
 [97] "March 1, 2018: Deputy State Prosecutor Retires After 33 Years"                                                                                                                                                                                       
 [98] "February 2, 2018: Former Baltimore City School Principal Incarcerated and Ordered to Pay Restitution of $58,689.34 for Thefts from School System"                                                                                                    
 [99] "December 5, 2017: Statement of Facts for Leslie Lewis, Indictment #117075031"                                                                                                                                                                        
[100] "December 5, 2017: Statement of Facts for Leslie Lewis, Indictment #117075030"                                                                                                                                                                        
[101] "February 2, 2018: Erica Holland, Former Account Clerk for the Town of Snow Hill, Incarcerated for Theft Scheme"                                                                                                                                      
[102] "January 23, 2018: Shaun Dallas Dance, Former Superintendent of the Baltimore County Public Schools, Charged With Multiple Counts of Perjury"                                                                                                         
[103] "January 23, 2018: Indictment of Shaun Dallas Dance"                                                                                                                                                                                                  
[104] "January 11, 2018: Correctional Officers Among 18 Indicted in Prison Corruption Investigation"                                                                                                                                                        
[105] "December 22, 2017: Daniel E. Gosnell, Lieutenant with the Aberdeen Police Department, Pleads Guilty to Misconduct in Office and Possession of a Controlled Dangerous Substance"                                                                      
[106] "December 22, 2017: Daniel E. Gosnell, Statement of Facts in Support of Guilty Plea"                                                                                                                                                                  
[107] "December 5, 2017: Leslie Lewis, Former Baltimore Community High School Principal, Pleads Guilty To Two Counts Of Felony Theft"                                                                                                                       
[108] "November 8, 2017: Daniel E. Gosnell, Lieutenant With The Aberdeen Police Department, Charged With Misconduct In Office And Possession Of A Controlled Dangerous Substance"                                                                           
[109] "November 8, 2017: Daniel E. Gosnell, Criminal Information"                                                                                                                                                                                           
[110] "November 7, 2017: Former Employee of Historic Inns of Annapolis Pleads Guilty to Felony Theft of State Delegate’s Campaign Funds and Identity Theft"                                                                                                 
[111] "November 7, 2017: Nicole Smith, Statement of Facts in Support of Guilty Plea"                                                                                                                                                                        
[112] "September 26, 2017: Erica Holland, Former Account Clerk for Town of Snow Hill, Charged With Felony Theft"                                                                                                                                            
[113] "September 26, 2017: Indictment of Erica Holland"                                                                                                                                                                                                     
[114] "July 20, 2017: Rebecca Drew, Register Of Wills For Allegany County, Charged With Misconduct In Office, Two Counts Of Felony Theft And Misappropriation"                                                                                              
[115] "July 20, 2017: Indictment of Rebecca Drew"                                                                                                                                                                                                           
[116] "July 12, 2017: Town Mayor Sentenced to Nine (9) years for Felony Theft, Misconduct in Office and Forgery"                                                                                                                                            
[117] "Contact Us"                                                                                                                                                                                                                                          
[118] "Report Fraud"                                                                                                                                                                                                                                        
[119] "Facebook "                                                                                                                                                                                                                                           
[120] "Twitter "                                                                                                                                                                                                                                            
[121] "Instagram  "                                                                                                                                                                                                                                         
[122] "LinkedIn"                                                                                                                                                                                                                                            
\end{verbatim}

Ok, so now we can see the press releases. But they are surrounded by a
bunch of things we don't want, and this \emph{still} isn't a dataframe.
Let's solve the second problem first with the \texttt{as\_tibble()}
function:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\OtherTok{\textless{}{-}}\NormalTok{ osp\_results }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{\textquotesingle{}li\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Now we're getting closer. The press releases have a specific HTML class
tied to the

tag - in this case ``listing-item''. We can isolate those:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\OtherTok{\textless{}{-}}\NormalTok{ osp\_results }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{\textquotesingle{}li.listing{-}item\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

Ok, this is \emph{better}, but it's still not great. We have at least
two issues:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The column is called \texttt{value}
\item
  The column has two kinds of information in it: a date and the title.
\end{enumerate}

Let's solve both of those using the \texttt{separate()} function, which
does what you think it does and gives us a chance to rename the columns:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\OtherTok{\textless{}{-}}\NormalTok{ osp\_results }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{\textquotesingle{}li.listing{-}item\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{separate}\NormalTok{(value, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}date\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}title\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{":"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Expected 2 pieces. Missing pieces filled with `NA` in 8 rows [49, 50, 51, 52,
53, 54, 56, 57].
\end{verbatim}

That worked, but it also exposed a couple of things, as that Warning
message indicates: not all of the rows had a colon in them and the
\texttt{title} column has a leading space. Let's fix both of those -
we'll drop the rows where the title is NA and convert the dates into
actual dates.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{releases }\OtherTok{\textless{}{-}}\NormalTok{ osp\_results }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_elements}\NormalTok{(}\StringTok{\textquotesingle{}li.listing{-}item\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{html\_text}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{as\_tibble}\NormalTok{() }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{separate}\NormalTok{(value, }\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}date\textquotesingle{}}\NormalTok{, }\StringTok{\textquotesingle{}title\textquotesingle{}}\NormalTok{), }\AttributeTok{sep=}\StringTok{":"}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{drop\_na}\NormalTok{(title) }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{title =} \FunctionTok{str\_squish}\NormalTok{(title), }\AttributeTok{date =} \FunctionTok{mdy}\NormalTok{(date))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Warning: Expected 2 pieces. Missing pieces filled with `NA` in 8 rows [49, 50, 51, 52,
53, 54, 56, 57].
\end{verbatim}

And now you have the data.

\bookmarksetup{startatroot}

\hypertarget{basic-stats-linear-regression-and-the-t-test}{%
\chapter{Basic Stats: Linear Regression and The
T-Test}\label{basic-stats-linear-regression-and-the-t-test}}

Many stories that rely on data use basic descriptive statistics:
journalists often are calculating rates, creating rankings and computing
averages. That's often in service of finding patterns that could yield
interesting stories, but there are useful ways to go beyond patterns and
try to establish whether there is a relationship between two things and,
if so, how strong that relationship is.

Here's an example: an elected official claims that more money spent on
education leads to greater economic growth and people moving into the
state. How can we test whether those ideas - education spending,
economic growth and population growth - are related? How do we estimate
how strong any relationship between them is? To do that, we'll use two
slightly more advanced statistical analysis methods to look for
patterns: linear regression, to examine relationships, and a t.test, to
confirm the statistical validity of an average between two groups. So,
let's do that here.

We'll be using state-by-state data on education spending, economic
activity and population growth from 2021, plus the presidential winner
from the 2020 election.

First, let's load libraries. We're going to load janitor, the tidyverse
and a new package, \href{https://corrr.tidymodels.org/}{corrr}, which
will help us do linear regression a bit easier than base R.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(janitor)}
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(corrr)}
\end{Highlighting}
\end{Shaded}

Now let's load the data we'll be using. It has five fields:

\begin{itemize}
\tightlist
\item
  state
\item
  per\_pupil\_2021: education spending per pupil in 2021.
\item
  gdp\_growth\_2021: the percentage growth in Gross Domestic Product in
  2021.
\item
  pop\_change\_2021: the percentage change in population in 2021.
\item
  vote\_2020: the winner of the state's popular vote in the 2020
  presidential election.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_data }\OtherTok{\textless{}{-}} \FunctionTok{read\_csv}\NormalTok{(}\StringTok{"data/regression\_data.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Rows: 50 Columns: 5
-- Column specification --------------------------------------------------------
Delimiter: ","
chr (2): state, vote_2020
dbl (3): per_pupil_2021, gdp_growth_2021, pop_change_2021

i Use `spec()` to retrieve the full column specification for this data.
i Specify the column types or set `show_col_types = FALSE` to quiet this message.
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_data}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 50 x 5
   state       per_pupil_2021 gdp_growth_2021 pop_change_2021 vote_2020
   <chr>                <dbl>           <dbl>           <dbl> <chr>    
 1 Alabama              10683             5.1            0.37 Trump    
 2 Alaska               19540             0.3            0.17 Trump    
 3 Arizona               9611             6.3            1.18 Biden    
 4 Arkansas             11266             5.2            0.46 Trump    
 5 California           14985             7.8           -0.91 Biden    
 6 Colorado             12255             5.8            0.46 Biden    
 7 Connecticut          22769             4.8            0.72 Biden    
 8 Delaware             17448             3.4            1.28 Biden    
 9 Florida              10401             8.4            1.1  Trump    
10 Georgia              12145             7              0.54 Biden    
# i 40 more rows
\end{verbatim}

\bookmarksetup{startatroot}

\hypertarget{linear-regression}{%
\chapter{Linear Regression}\label{linear-regression}}

Let's start with this question: did states that had the greatest
population growth and see GDP growth (our economic activity measure)? We
can answer it by examining the relationship or correlation between two
variables, pop\_change\_2021 and gdp\_growth\_2021. How much do they
move in tandem?

Let's start by plotting them to get a sense of the pattern.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{pop\_change\_2021,}\AttributeTok{y=}\NormalTok{gdp\_growth\_2021)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{pop\_change\_2021,}\AttributeTok{y=}\NormalTok{gdp\_growth\_2021), }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./basicstats_files/figure-pdf/unnamed-chunk-3-1.pdf}

}

\end{figure}

It's a bit messy, but we can see something of a pattern here in the blob
of dots. Generally, the dots are moving from the lower left (states with
lower population growth) to upper right (states with higher population
growth). The blue ``line of best fit'' shows the general direction of
the relationship. It appears that those two things might be at least
somewhat related.

Let's test another variable, the per-pupil education spending amount per
state.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{ggplot}\NormalTok{() }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{per\_pupil\_2021,}\AttributeTok{y=}\NormalTok{gdp\_growth\_2021)) }\SpecialCharTok{+}
  \FunctionTok{geom\_smooth}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{per\_pupil\_2021,}\AttributeTok{y=}\NormalTok{gdp\_growth\_2021), }\AttributeTok{method=}\StringTok{"lm"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
`geom_smooth()` using formula = 'y ~ x'
\end{verbatim}

\begin{figure}[H]

{\centering \includegraphics{./basicstats_files/figure-pdf/unnamed-chunk-4-1.pdf}

}

\end{figure}

This one is messier. There may be a slight downward slope in this blob
of dots, but it's not quite as apparent. It seems less certain that
there's a relationship between these two variables.

We can be a bit more precise by calculating a statistic called the
correlation coefficient, also called ``r''. r is a value between 1 and
-1. An r of 1 indicates a strong positive correlation.

An increase in air temperature and air conditioning use at home is
strongly-positively correlated: the hotter it gets, the more we have to
use air conditioning. If we were to plot those two variables, we might
not get 1, but we'd get close to it.

An r of -1 indicates a strong negative correlation. An increase in
temperature and home heating use is strongly negatively correlated: the
hotter it gets, the less heat we use indoors. We might not hit -1, but
we'd probably get close to it.

A correlation of 0 indicates no relationship.

All r values will fall somewhere on this scale, and how to interpret
them isn't always straightforward. They're best used to give general
guidance when exploring patterns.

We can calculate r with a function from the corrr package called
``correlate()''. First, we remove the non-numeric values from our
state\_data (state and vote\_2020), then we correlate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{state, }\SpecialCharTok{{-}}\NormalTok{vote\_2020) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{correlate}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
Correlation computed with
* Method: 'pearson'
* Missing treated using: 'pairwise.complete.obs'
\end{verbatim}

\begin{verbatim}
# A tibble: 3 x 4
  term            per_pupil_2021 gdp_growth_2021 pop_change_2021
  <chr>                    <dbl>           <dbl>           <dbl>
1 per_pupil_2021         NA              -0.0599          -0.411
2 gdp_growth_2021        -0.0599         NA                0.211
3 pop_change_2021        -0.411           0.211           NA    
\end{verbatim}

The table this function produces generally confirms our interpretation
of the two graphs above. The relationship between a state's
gdp\_growth\_2021 and per-pupil education spending is slightly negative
at -0.06 (on a scale of -1 to 1), and the relationship isn't
particularly strong. That's why the second graphic above was messier
than the first.

The relationship between population growth and GDP growth seems slightly
positive at 0.21 (on a scale of -1 to 1), but there are a lot of dots
falling outside that line of best fit in our scatterplot.

The relationship between a state's per-pupil education spending and
population growth is a bit stronger (in a negative direction), if
moderate, -.41 (on a scale of -1 to 1). Is this finding statistically
valid? We can get a general sense of that by calculating the p-value of
this correlation, a test of statistical significance. For that, we can
use the cor.test function.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(state\_data}\SpecialCharTok{$}\NormalTok{per\_pupil\_2021, state\_data}\SpecialCharTok{$}\NormalTok{pop\_change\_2021)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  state_data$per_pupil_2021 and state_data$pop_change_2021
t = -3.1256, df = 48, p-value = 0.00301
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.6187516 -0.1500577
sample estimates:
       cor 
-0.4112291 
\end{verbatim}

This output is quite a bit uglier, but for our purposes there are two
key pieces of information from this chunk of unfamiliar words. First, it
shows the correlation calculated above: r -0.4112291. Two, it shows the
p-value, which is 0.00301. That's pretty low, as far as p-values go,
which indicates that there's a very slim chance that our finding is a
statistical aberration; that is, that it happened solely by chance.

Now let's test another one, the relationship between the
per\_pupil\_2021 and gdp\_growth\_2021.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor.test}\NormalTok{(state\_data}\SpecialCharTok{$}\NormalTok{per\_pupil\_2021, state\_data}\SpecialCharTok{$}\NormalTok{gdp\_growth\_2021)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's product-moment correlation

data:  state_data$per_pupil_2021 and state_data$gdp_growth_2021
t = -0.41572, df = 48, p-value = 0.6795
alternative hypothesis: true correlation is not equal to 0
95 percent confidence interval:
 -0.3326975  0.2221550
sample estimates:
        cor 
-0.05989652 
\end{verbatim}

Again, it shows our r value of -0.06, which was weaker. And the p-value
here is a much larger 0.6795. That indicates a much greater chance of
our finding being a statistical aberration, high enough that I wouldn't
rely on its validity.

p \textless{} .05 is accepted in many scientific disciplines -- and by
many data journalists -- as the cutoff for statistical significance. But
there's heated debate about that level, and some academics question
whether p-values should be relied on so heavily.

And to be clear, a low p-value does not prove that we've found what we
set out to find. There's nothing on this graph or in the regression
model output that proves that education spending alone has a direct
impact on GDP growth or population change. It's entirely possible that
there's some other variable -- or variables -- not considered here that
explain this pattern.

All we know is that we've been able to explore the relationship between
multiple variables, and the results can help us develop better questions
or provide some context to the elected official's claims. One of the
best uses of techniques like regression and significance testing is to
see just how much stock we should put into a claim or conclusion.

\bookmarksetup{startatroot}

\hypertarget{t-tests}{%
\chapter{T-tests}\label{t-tests}}

Let's suppose we want to ask a related set of questions: did states won
by Donald Trump have higher than average GDP growth than states won by
Joe Biden? Or did states won by Biden have greater education spending
than those won by Trump?

We can do this because, in our data, we have a column with two possible
categorical values, Biden or Trump, for each state.

We could just calculate the averages like we're used to doing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{state\_data }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{group\_by}\NormalTok{(vote\_2020) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{summarise}\NormalTok{(}
    \AttributeTok{mean\_gdp\_growth =} \FunctionTok{mean}\NormalTok{(gdp\_growth\_2021),}
    \AttributeTok{mean\_per\_pupil\_education =} \FunctionTok{mean}\NormalTok{(per\_pupil\_2021),}
    \AttributeTok{mean\_pop\_growth =} \FunctionTok{mean}\NormalTok{(pop\_change\_2021)}
\NormalTok{  )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 2 x 4
  vote_2020 mean_gdp_growth mean_per_pupil_education mean_pop_growth
  <chr>               <dbl>                    <dbl>           <dbl>
1 Biden                5.87                   16579.           0.177
2 Trump                4.30                   12379.           0.550
\end{verbatim}

Examining this, it appears that in all three categories there's a
difference.

The average GDP growth was higher in Biden states than Trump states
(5.8\% to 4.3\%). The average amount of per-pupil education spending in
Biden states was larger -- \$16,578 -- than Trump states -- \$12,378.
And states that backed Trump saw higher population growth in 2021.
Should we report these as meaningful findings?

A t-test can help us answer that question. It can tell us where there's
a statistically significant difference between the means of two groups.
Have we found a real difference, or have we chanced upon a statistical
aberration? Let's see by calculating it for the population growth.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(pop\_change\_2021 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ vote\_2020, }\AttributeTok{data =}\NormalTok{ state\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  pop_change_2021 by vote_2020
t = -1.828, df = 45.476, p-value = 0.07411
alternative hypothesis: true difference in means between group Biden and group Trump is not equal to 0
95 percent confidence interval:
 -0.78342389  0.03782389
sample estimates:
mean in group Biden mean in group Trump 
             0.1768              0.5496 
\end{verbatim}

We see our two means, for Trump and Biden, the same as we calculated
above. The t-value is approximately -1.8, the p-value here is 0.07411,
both of which should give us pause that that we've found something
meaningful.
\href{https://conjointly.com/kb/statistical-student-t-test/}{More on
t-tests here}

Let's try per-pupil education spending.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(per\_pupil\_2021 }\SpecialCharTok{\textasciitilde{}}\NormalTok{ vote\_2020, }\AttributeTok{data =}\NormalTok{ state\_data)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  per_pupil_2021 by vote_2020
t = 4.2494, df = 39.029, p-value = 0.0001289
alternative hypothesis: true difference in means between group Biden and group Trump is not equal to 0
95 percent confidence interval:
 2200.814 6199.026
sample estimates:
mean in group Biden mean in group Trump 
           16578.80            12378.88 
\end{verbatim}

This is more promising. T is much stronger -- about 4.2 -- and the
p-value is a very low 0.0001. Both of these should give us assurance
that we've found something statistically meaningful. Again, this doesn't
prove that spending more on education means that voters there will back
Democrats for president, or that voting for Donald Trump led to
population growth. It just suggests there's a pattern worth following up
on.

\bookmarksetup{startatroot}

\hypertarget{writing-with-numbers}{%
\chapter{Writing with numbers}\label{writing-with-numbers}}

The number one sin of all early career data journalist is to get really,
really, really attached to the analysis you've done and include every
number you find.

Don't do that.

Numbers tell you what. Numbers rarely tell you why. What question has
driven most people since they were three years old? Why. The very first
thing to do is realize that is the purpose of reporting. You've done the
analysis to determine the what. Now go do the reporting to do the why.
Or as an old editor of mine used to say ``Now go do that reporting shit
you do.''

The trick to writing a numbers story is to frame your story around
people. Sometimes, your lead can be a number, if that number is
compelling. Often, your lead is a person, a person who is one of the
numbers you are writing about.

Tell their story. Briefly. Then, let us hear from them. Let them speak
about what it is you are writing about.

Then come the numbers.

\hypertarget{how-to-write-about-numbers-without-overwhelming-with-numbers.}{%
\section{How to write about numbers without overwhelming with
numbers.}\label{how-to-write-about-numbers-without-overwhelming-with-numbers.}}

Writing complex stories is often a battle against that complexity. You
don't want to overwhelm. You want to simplify where you can. The first
place you can do that is only use exact numbers where an exact number is
called for.

Where you can, do the following:

\begin{itemize}
\tightlist
\item
  Using ratios instead of percents
\item
  Often, it's better to put it in counts of 10. 6 of 10, 4 of 10. It's
  easy to translate that from a percentage to a ratio.
\item
  But be careful when your number is 45 percent. Is that 4 in 10 or 5 in
  10?
\item
  If a ratio doesn't make sense, round. There's 287,401 people in
  Lincoln, according to the Census Bureau. It's easier, and no less
  accurate, to say there's more than 287,000 people in Lincoln.
\end{itemize}

\textbf{A critical question your writing should answer: As compared to
what?}

How does this compare to the average? The state? The nation? The top?
The bottom?

One of the most damning numbers in the series of stories Craig Pittman
and I wrote that became the book
\href{https://www.amazon.com/Paving-Paradise-Floridas-Vanishing-Wetlands-ebook/dp/B004HZXZCE}{Paving
Paradise} was comparing approvals and denials.

We were looking at the US Army Corps of Engineers and their permitting
program. We were able to get a dataset of just a few years of permits
that was relatively clean. From that, we were able to count the number
of times the corps had said yes to a developer to wipe out wetlands the
law protected and how many times they said no.

They said yes 12,000 times. They said no once.

That one time? Someone wanted to build an eco-lodge in the Everglades.
Literally. Almost every acre of the property was wetlands. So in order
to build it, the developer would have to fill in the very thing they
were going to try to bring people into. The corps said no.

\hypertarget{when-exact-numbers-matter}{%
\section{When exact numbers matter}\label{when-exact-numbers-matter}}

Sometimes ratios and rounding are not appropriate.

This is being written in the days of the coronavirus. Case counts are
where an exact number is called for. You don't say that there are more
than 70 cases in Lancaster County on the day this was written. You
specify. It's 75.

You don't say almost 30 deaths. It's 28.

Where this also comes into play is any time there are deaths: Do not
round bodies.

\hypertarget{an-example}{%
\section{An example}\label{an-example}}

\href{https://www.azcentral.com/in-depth/news/local/arizona-wildfires/2019/07/22/wildfire-risks-more-than-500-spots-have-greater-hazard-than-paradise/1434502001/}{Read
this story from USA Today and the Arizona Republic}. Notice first that
the top sets up a conflict: People say one thing, and that thing is not
true.

\begin{quote}
No one could have anticipated such a catastrophe, people said. The
fire's speed was unprecedented, the ferocity unimaginable, the
devastation unpredictable.
\end{quote}

\begin{quote}
Those declarations were simply untrue. Though the toll may be impossible
to predict, worst-case fires are a historic and inevitable fact.
\end{quote}

The first voice you hear? An expert who studies wildfires.

\begin{quote}
Phillip Levin, a researcher at the University of Washington and lead
scientist for the Nature Conservancy in Washington, puts it this way:
``Fire is natural. But the disaster happens because people didn't know
to leave, or couldn't leave. It didn't have to happen.''
\end{quote}

Then notice how they take what is a complex analysis using geographic
information systems, raster analysis, the merging of multiple different
datasets together and show that it's quite simple -- the averaging
together of pixels on a 1-5 scale.

Then, the compare what they found to a truly massive fire: The Paradise
fire that burned 19,000 structures.

\begin{quote}
Across the West, 526 small communities --- more than 10 percent of all
places --- rank higher.
\end{quote}

And that is how it's done. Simplify, round, ratios: simple metrics,
powerful results.



\end{document}
